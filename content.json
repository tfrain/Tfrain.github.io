{"pages":[{"title":"about","text":"Support meBuy Programmers Career a Coffee. ko-fi.com/programmerscareer - Ko-fi ❤️ Where creators get support from fans through donations, memberships, shop sales and more! The original ‘Buy Me a Coffee’ Page. Info Backend: Golang Writing: Programmer’s Career – Medium Github、X and so on: WesleyWei","link":"/about/"},{"title":"archives","text":"","link":"/archives/"},{"title":"categories","text":"","link":"/categories/"},{"title":"tags","text":"","link":"/tags/"}],"posts":[{"title":"Syntactic Sugars that You Should Know in Golang","text":"Exploring Syntactic Sugars in Go: A Comprehensive Guide Walking with a friend in the dark is better than walking alone in the light. — Helen Keller 中文文章: https://programmerscareer.com/zh-cn/golang-syntactic-sugar/Author: Wesley Wei – Twitter Wesley Wei – MediumNote: If you choose to repost or use this article, please cite the original source. Introduction to Syntax Sugars in GoBefore I get into the specifics of syntactic sugar, let me explain to you what syntactic sugar is. In computer science, “syntactic sugar” refers to a programming syntax that is internally transformed into a base syntax in order to make a program easier to read or express. In other words, this syntax does not introduce new features, but rather provides a more convenient way of programming. Golang, being a modern language, contains a lot of syntactic sugar to ease the programmer’s burden and make the code more readable. For example, many high-level programming languages have the ++ operator, which is syntactic sugar used to increment the value of a variable by 1. Thus, instead of i = i + 1, we can type i++, which is shorter and faster to type, and which expresses the same incremental operation. Next we’ll go through the common syntactic sugars in Golang, detailing each one. This will help you understand and use Golang better and write more compact and readable code. Of course, the goal is not only to learn these features, but also to understand when and why to use them. A responsible Go developer knows not only how to utilize these syntactic sugars, but also when it is appropriate to use them. Variadic ParametersBasic IntroductionGo allows a function to take any number of values as arguments, and Go provides the … operator to be used only at the end of a function’s parameter list. When using this operator, you should be aware of the following points: A function can have at most one variadic parameter; The type of a variadic parameter is always a slice type; The last parameter of a function can be a variadic parameter. Declaration and CallingThe declaration of a variadic function is similar to that of a regular function, except that the last parameter must be a variadic parameter. In the function body, a variadic parameter is treated as a slice. 12345678func SumData(values …int64) (sum int64) { // Type of values is []int64. sum = 0 for _, v := range values { sum += v } return} When calling a variadic function, you can use two styles to pass arguments to a variadic parameter of type []T: 1. Pass a slice as an argument. This slice must be assignable to a value of type []T (or can be implicitly converted to type []T). Following this argument, there must be three dots ‘…’.2. Pass zero or more arguments that can be implicitly converted to type T (or can be assigned to a value of type T). These arguments will be added to an anonymous slice of type []T that is created at runtime, and then this slice will be passed as an argument to the function call. Note that you cannot mix these two styles of argument passing in the same variadic function call. Note that you cannot mix these two styles of argument passing in the same variadic function call. 1234567891011121314func main() { a0 := SumData() a1 := SumData(3) a3 := SumData(3, 4, 8) // The top three lines are equivalent to the bottom three lines. b0 := SumData([]int64{})… b1 := SumData([]int64{2})… b3 := SumData([]int64{2, 3, 5})… fmt.Println(a0, a1, a3) fmt.Println(b0, b1, b3)}// print// 0 3 15// 0 3 15 The Print、Println and Printf functions in the fmt standard library package are all variadic functions. Their declarations are roughly as follows: 123func Print(a …interface{}) (n int, err error) func Printf(format string, a …interface{}) (n int, err error) func Println(a …interface{}) (n int, err error) Ignoring Unnecessary InformationWe want to initialize the init function in a package but do not want to use any of the methods in the package. In this case, we can use the _ operator to rename the import of an unused package: 1import _ \"github.com/tfrain\" Sometimes we don’t necessarily use the return values of a function, and we have to come up with a creative name for it. Is there a way to handle unnecessary return values? Of course, we can use the _ operator to assign the unwanted values to a blank identifier, which allows us to ignore them: 1_, ok := test(a, b int) Sometimes we want to exclude certain fields from serialization in JSON. The - operator can help us with this. Go structures provide a labeling feature, and in the structure tag, we can use the - operator to perform special handling on fields that we don’t want to serialize: 12345type Person struct { Name string `json:\"-\"` Age string `json:\"age\"` Email string `json:\"email,omitempty\"`} When we use json.Marshal to serialize a structure, it does not ignore empty values by default. Instead, it outputs the zero value of the field’s type (the zero value of a string type is “”, and the zero value of an object type is nil). If we want to ignore empty fields during serialization, we can add the omitempty attribute to the structure tag: 123456type Person struct { Name string `json:\"name\"` Age string `json:\"age\"` Email string `json:\"email,omitempty\"` Active bool `json:\"active,omitempty\"`} Declaration StatementsShort Variable DeclarationIn some other programming languages, it’s not common to declare variables every time they’re used. In Go, you can declare and initialize local variables using the syntax name := expression instead of using the var statement for declaration. This can reduce the number of steps required for declaration: 123var a int = 10same asa := 10 When using short variable declaration, there are two things to note: Short variable declaration can only be used inside functions, not for initializing global variables. Short variable declaration introduces a new variable, so you can’t declare the same variable again in the same scope. When declaring multiple variables using short variable declaration, if one variable is new, you can use short variable declaration, but if all variables are already declared, you can’t declare them again. Declaring Variables with Unspecified LengthIn Go, arrays usually have a fixed length, and you have to specify the length when declaring the array. However, you can also omit the length and use the … operator to declare arrays. In this case, you just need to fill in the element values, and the compiler will handle the length: 1a := […]int{1, 3, 5} // same as a := [3]int{1, 3, 5} When declaring a large array, you can use the … operator to set specific values for some indices: 1a := […]int{1: 20, 999: 10} // array length is 1000, index 1 has value 20, index 999 has value 10, other indices have value 0 Checking logicChecking if a Key Exists in a Map in GoGo provides the syntax value, ok := m[key] to check if a key exists in a map. This syntax is commonly used to only check the ok value. If the key exists, it returns the value associated with the key; otherwise, it returns an empty value: 12345678910import \"fmt\"func main() { dict := map[string]int{\"tfrain\": 1} if value, ok := dict[\"tfrain\"]; ok { fmt.Println(value) } else { fmt.Println(\"Key:tfrain not exist\") }} Type AssertionsWe often use interface in Go, where there are two types: interfaces with methods and empty interfaces. Since Go 1.18 does not have generics, we can use empty interfaces as a pseudo-generic type. When we use empty interfaces as input parameters or output values, we need to use type assertions to obtain the type we need. In Go, the syntax for type assertions is as follows: 1value, ok := x.(T) Here, x is an interface type, and T is a specific type. This syntax requires distinguishing the type of x. If x is an empty interface type: The type assertion for empty interface types is essentially a comparison of _type and the type to be matched in eface. If the comparison is successful, the value is assembled in memory and returned. If the comparison fails, the register is cleared, and the default value is returned. If x is a non-empty interface type: The type assertion for non-empty interface types is essentially a comparison of *itab in iface. If the comparison is successful, the value is assembled in memory and returned. If the comparison fails, the register is cleared, and the default value is returned.","link":"/golang-syntactic-sugar/"},{"title":"hello-world","text":"Start at the beginningI have also written a blog before, but I have not started writing for more than 2 years, and I am ready to pick it up again. I have changed to a new blog system, the most important part may be to support bilingual. blogging at the same time, but also to exercise my English.","link":"/hello-world/"},{"title":"Kafka interviews: How does Kafka send messages reliably?","text":"Have you ever been asked a similar question in an interview? Or you will meet in the future, let’s explore and master it together Thank you for reading this article. More Interview Questions here:https://programmerscareer.com/software-interview-set/ Topic: 1.1 How Does Kafka Work?Apache Kafka is an open-source stream-processing software platform developed by LinkedIn and later donated to the Apache Software Foundation. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. It is written in Scala and Java. The Kafka ecosystem consists of producers, brokers, consumers, topics, and Zookeeper: Producers: Producers are components that push raw messages to Kafka brokers. Producers can send messages to a specific topic or allow Kafka to do the routing and load balancing. Brokers:&nbsp;Kafka brokers are the heart of the system;&nbsp;they receive messages from producers, assign offsets to them, and commit the messages to storage on disk. Consumers: Consumers read from brokers. However, unlike traditional messaging systems, consumers pull messages from brokers. Topics and Partitions: Topics are feeds of messages in specific categories. Kafka topics are divided into a number of partitions, which contain records in an unchangeable sequence. Partitions allow you to parallelize topics by splitting the data across multiple nodes. Zookeeper: Zookeeper manages and coordinates the Kafka cluster. ZooKeeper service is used to maintain naming and configuration data and to provide flexible and robust synchronization within distributed systems. A simple analogy for the working of Kafka is a messaging system. Think of it like a postman (producer) delivering mail letters (messages) to specific mailboxes (topics). The mail agency (Kafka) sorts out and maintains these letters, and finally, the residents (consumers) pick up their mail from the mailboxes. Topic:&nbsp;1.2 Kafka ProducersKafka Producers are responsible for publishing or writing data (known as records) to one or more Kafka topics.&nbsp;Producers play a crucial role in feeding Kafka brokers with data. Here’s how Kafka Producers work: Creating a Producer:&nbsp;The producers are created with certain properties like the broker’s address, the key serializer, and the value serializer. Brokers use these properties to identify the correct topic and understand how to parse the messages. Writing Data:&nbsp;After creating a producer, it can start sending records to the specified topic. A record in Kafka contains a&nbsp;key&nbsp;and a&nbsp;value. These are byte arrays. The key, which is optional, is used in determining the specific partition of the topic where the message will be written. If a key isn’t provided, Kafka uses a round-robin method to write to a partition. Partitioning:&nbsp;The producers publish data to different partitions in a round-robin way or a semantically meaningful manner. When the key is null, data is sent round-robin. If a key is present, all messages for that key will go to the same partition (if the number of partitions does not change). Serialization:&nbsp;Kafka messages are byte arrays. Therefore, whatever data format you have, you must convert it to bytes when it’s sent to Kafka. We call this process serialization. Therefore, whenever a producer is sending a record, it needs to convert the objects into bytes. Acknowledgements and Retries:&nbsp;In a distributed system, machines fail. Kafka provides mechanism of acknowledgements and retries. Kafka can be set to acknowledge a write once it has been written to the leader (ack=1), all followers (ack=all), or none at all (ack=0). From the producer configurations, it’s clear that we can adjust the reliability and durability guarantees by choosing the number of acknowledgements (acks) and specifying whether the producer should retry sending messages. A deep understanding of Kafka Producers is&nbsp;key to leveraging effectively the power of Kafka&nbsp;and to implementing reliable and efficient event-driven systems. Topic: 1.3 Reliable Message Delivery — BasicsReliable message delivery is a critical aspect of any message-oriented middleware. Kafka provides strong durability guarantees and fault-tolerance to ensure reliable message delivery. The basics of reliable message delivery in Kafka revolve around these main concepts: Producers and Message Acknowledgments:&nbsp;As we’ve learned before, Producers send messages to Kafka brokers. These messages can be acknowledged in different ways, controlled by the&nbsp;acks&nbsp;property of the producers. This level of acknowledgement impacts the durability of the messages. An ack of ‘1’ means that the message is considered written successfully once written to the leader’s log. An ack of ‘all’ means that the message is considered written successfully once all in-sync replicas have applied it to their log. Replication and In-sync replicas:&nbsp;Replication is a key feature in ensuring message durability. Each partition in Kafka has multiple replicas, one of which is elected as the leader. All writes and reads go through the leader. The rest are followers, and their main job is to replicate the leader. Only when a message is written to all in-sync replicas, it is considered committed and hence, successfully written. Consumer Offsets and Delivery Semantics:&nbsp;Consumers read message from Kafka topics at their own pace and keep track of what messages they have read by storing the offset of messages. Kafka provides three delivery semantics: at most once, at least once, and exactly once. By storing and managing offsets properly, Kafka ensures that messages are delivered at least once. Committed and Uncommitted Messages:&nbsp;Messages in Kafka are considered committed once they’re written successfully to the log on all in-sync replicas. Consumers can only consume committed messages. This ensures that consumers always get complete, correct data. Messages can be written to the log but not viewable to consumers until they’re committed, protecting against data inconsistencies and partial data in case of a failure. Topic:&nbsp;1.4 Kafka Brokers and Topic ReplicationKafka Brokers, as we’ve previously learned, are the heart of the system, handling receiving messages from producers, assigning offsets to them, and committing the messages to storage on disk. Now, let’s dig deeper. A Kafka cluster is composed of multiple brokers. Each broker can handle data and requests from many clients because topics are partitioned and replicas are distributed across multiple broker instances. Topic Replication: Replication is a crucial feature in Kafka for reliability and fault-tolerance. Each topic can have multiple replicas allowing it to be stored on multiple brokers. This means that even if a broker goes down, the topic data is still available from the other broker holding another replica. Replicas of a Kafka topic partition are distributed to different brokers in the cluster. Having replicas makes Kafka fault-tolerant. Leader and Follower:&nbsp;For a partition, one replica will serve as the Leader, and the rest will be Followers. The Leader handles all read and write requests for the partition, while Followers passively replicate the leader. If the Leader fails, one of the Followers will automatically become the new Leader. In-Sync Replica(ISR): If a follower remains too far behind the leader (configurable by a parameter), the leader will remove the follower from the list of ISR (in-sync replica). Only members in the ISR list can be elected as the leader. Replication and Reliability: The role played by each broker in a partition’s replication is vital for the delivery semantics Kafka provides. Reading and writing to a broker ensures durability of records, and a broker’s failure doesn’t affect data integrity. Topic: 1.5 Ins and Outs of Kafka ConsumersKafka Consumers are applications that read and process data from Kafka topics. The role and functionality of Kafka consumers is vital in maintaining the reliability and fault tolerance of Kafka as a distributed system. Here are some key aspects of Kafka Consumers: Consumer Groups:&nbsp;Multiple consumers can form a ‘Consumer Group’. As a part of the group, they share the load of consuming messages, each consumer reading from one or more partitions of a topic. This provides both load balancing and fault tolerance features to Kafka. Consuming Messages:&nbsp;Consumers read the messages from a topic and process it. They maintain the offset of the next message they expect to read. Offsets and Consumer Position:&nbsp;Each consumer group maintains its offset or position — a record of which messages have been consumed. If a consumer has processed a message successfully, the offset will be advanced. So, even if a consumer crashes, it can pick up where it left off, increasing the system’s fault tolerance and resiliency. Rebalancing: When a consumer stops or a new consumer joins a Kafka consumer group, a rebalancing protocol is initiated. The protocol ensures that consumers leave gracefully if they are planning to stop, while new consumers join smoothly without affecting the message consumption process within the group. Delivery Semantics:&nbsp;based on how the consumers manage the offsets and commits, Kafka provides three semantics for message delivery — at most once, at least once, and exactly once. It’s important to design consumer applications such that they’re capable of handling these semantics accurately and consistently. In a Kafka data flow, Consumers play a significant role in driving real-time processing systems. Getting a solid grasp of Kafka Consumers is key to leveraging Kafka’s full potential for building robust and scalable data processing systems. Topic: 1.6 How does Kafka Send Messages Reliably?Kafka’s primary responsibility is to reliably transfer records from producers (which write the data) to consumers (which read the data). Here’s a breakdown of how Kafka ensures reliable message delivery: Replication and Redundancy: Kafka ensures message durability through its topic replication feature. A Kafka topic is divided into partitions, and each partition can be replicated across multiple nodes called brokers. This means the same message can live in multiple places, providing a high level of redundancy. Leader and Follower: For every partition of Kafka, the broker can play two types of roles: leader and follower. All read and writes are served by the leader, and followers passively replicate the leader. If a leader fails, a follower can take its place and serve data to consumers,&nbsp;therefore providing business continuity. Acknowledgments (ACKs): ACKs play a significant role in reliability. When a producer sends a message, it can choose to receive an acknowledgment after the message is written to the leader’s log (acks=1), or after it’s written to all in-sync replicas (acks=all). This choice contributes to a trade-off between performance and resilience. In-sync Replicas (ISRs): Kafka enforces that only replicas which are in-sync can be elected as leader. An ISR is a replica that has fully caught up with the partition leader, and hasn’t lagged behind the leader’s log for more than a specified time. Ensuring the leader is always from ISR gives Kafka a strong consistency as it guarantees any message that was written to the leader and acknowledged, will not be lost as long as the number of failures is within the replication factor. Consumer Offsets: Kafka consumers maintain their offset (the position from where they have read). Even if a consumer fails, it can resume reading messages from the offset it has kept track of, thereby minimizing data loss. In summary, Kafka guarantees the reliable delivery of messages by dividing data across multiple nodes for redundancy, ensuring data persistence through acknowledgments, maintaining ISR list for consistency, and utilizing&nbsp;offsets for effective consumption. Topic: 1.7 Best Practices for Reliable Messaging in KafkaThe reliability of a Kafka cluster greatly depends on how well it is managed and the practices in place regarding messaging. Here are some best practices for reliable messaging in Kafka: Monitor Your Cluster: Be sure to keep an eye on your Kafka cluster. This includes tracking things like the number of uncommitted messages, data rate in and out of each broker, topic and partition count, and under-replicated partitions. Monitoring will help you identify potential issues before they become serious. Set Appropriate Retention Periods: Keep in mind that increasing the retention period increases storage and heap use. Balance needs accordingly to avoid resource constraints. Sensible Partitioning of Topics: Choose the number of partitions with thought. While more partitions allow greater parallelism, they also imply more open server connections and greater Zookeeper overhead. Reasonable Replication Factors: Higher replication factor boosts redundancy and thus reliability, but it also increases storage requirements. Choose a replication factor that matches the level of fault tolerance needed. Proper Acknowledgement Policies: Use the correct acknowledgment policy (‘acks’) based on your application requirements. For critical data, consider using ‘acks=all’ to ensure data is replicated to all in-sync replicas before confirmation. Effective Use of In-Sync Replicas (ISRs): Configure your ISR settings to ensure you have the right balance between latency and durability guarantees. Make sure min.insync.replicas is set as per your needs, so you don’t lose data during failovers. Consumer Offset Management: Make sure consumers commit their offsets regularly. This avoids rebroadcasting massive amounts of data if a failure occurs. But don’t commit too frequently, since each commit is a call to Zookeeper. To sum up, achieving reliable messaging with Kafka is crucial, but it requires a balance between operational requirements, resource usage, and application-specific needs. Topic: 1.8 Kafka’s Message Delivery SemanticsIn Kafka, message delivery semantics govern how messages are delivered from the producer to the consumer. Kafka provides three types of delivery semantics: 1. At Most Once: In this case, the messages are delivered to the consumer at most once. This means that messages may be lost, but they are never redelivered or duplicated. This approach is fastest because it involves the least coordination between the producer and Kafka. However, it is not as reliable as other methods because any failure between the time Kafka sends the message and the consumer reads it will result in the loss of that message. 2. At Least Once: Messages are delivered at least once to the consumer. But, in certain situations, messages may be redelivered, resulting in duplicates. This method is more reliable than ‘at most once’ because it ensures messages are not lost. However, it has the risk of duplicate messages due to potential redelivery. For idempotent processing, this can be perfectly fine. 3. Exactly Once: This&nbsp;ensures that each message is delivered exactly once — no losses, no duplicates. However, it’s the slowest and most resource-intensive option because of the transactions needed to keep track of the progress. This is typically used in critical systems where message loss or duplication can lead to significant issues. These delivery semantics determine how resilient and reliable your Kafka-based system will be. The choice between speed, consistency, and reliability is yours to make depending on the use case of your application. Topic: 1.9 Review and AssessmentsWe’ve covered a great deal in our Kafka curriculum, let’s do a brief round-up of those lessons: How Kafka Works: We learned how various components of Kafka interact with each other to provide a robust, scalable, and fault-tolerant messaging system. Kafka Producers: We delved into how Kafka Producers send messages and explored their crucial configurations. Reliable Message Delivery Basics: We understood the fundamental concepts involved in ensuring message durability and reliability in Kafka. Kafka Brokers &amp; Topic Replication: We dived into the functioning of Kafka Brokers and learned how Topic replication furthers reliability. Kafka Consumers: We navigated the complexities of Kafka Consumers, Consumer groups, and ascertained their role in maintaining reliability. How Kafka Sends Messages Reliably: Unraveled Kafka’s internal mechanisms for ensuring reliable message delivery. Best Practices for Reliable Messaging in Kafka: We have discussed practical ways to optimize Kafka’s message delivery for reliability. Kafka’s Message Delivery Semantics: Finally, we looked at the three types of delivery semantics, their significance, and use cases. Now, it’s time to assess your understanding and application of this knowledge. We can proceed with some practice problems and analysis of real-world scenarios where Kafka is extensively used. This will help reinforce what you’ve learned and enable you to better incorporate Kafka into your systems. Example Problem: List and explain the three different delivery semantics in Kafka? Solution: At Most Once: Messages are delivered at most once, meaning they could be lost but will not be redelivered resulting in duplicates. This method is the fastest, but is less reliable as messages could be lost. At least Once: Messages are delivered at least once, which means that messages are assured to be delivered, but there’s a possibility of duplicates due to potential redelivery. This method is more reliable, but the duplication could potentially be an issue. Exactly Once: In this case, messages are delivered exactly once, meaning there are no losses or duplicates. This method is the most reliable, but also the slowest because of the overhead of keeping track of the delivery state of each message. Are you ready for the test questions? Let’s proceed. Question 1What is the role of a Kafka Producer in a Kafka cluster? Question 2Explain the concept of Topic replication in Kafka. Why is it important? Question 3What are In-Sync Replicas (ISRs) in Kafka? Question 4Mention some of the best practices for reliable messaging in Kafka. Question 5How does Kafka ensure reliable message delivery? Answer 1A Kafka producer’s role in a Kafka cluster is to publish data or messages to one or more Kafka topics. The messages sent by producers are appended to the committed log at the end and are assigned a unique offset number. Answer 2Topic replication is a feature in Kafka to ensure that messages in the cluster remain available during the unavailability of a broker (due to a failure or being taken down for maintenance). Each topic can be replicated across a configurable number of Kafka brokers to ensure redundancy. This helps in ensuring no message loss and high data availability. Answer 3In-Sync Replicas (ISRs) are the set of replicas that are up-to-date with the leader replica. Any replica that has not sent a fetch request to the leader for some configurable time is removed from the ISR set. If a follower fails to fetch from the leader, it will be out of ISR and won’t be considered for producing data to clients. Answer 4Some of the best practices for reliable messaging in Kafka include choosing the right message delivery semantics for your use case, following the principle of least privilege with permissions, using compaction for long-term topics with key-value data, monitoring and setting alerts for critical metrics, keeping your Kafka cluster and client libraries up-to-date, etc. Answer 5Kafka ensures reliable message delivery through several mechanisms like replication, in-sync replica sets, acknowledgements, and configurable delivery semantics. The producers wait for acknowledgements that a message has been written to the full set of in-sync replicas. If a message fails to be written, the producer will automatically retry. The consumers maintain an offset to track their progress through each topic. 中文文章: https://programmerscareer.com/zh-cn/kafka-interview1/Author: Wesley Wei – Twitter Wesley Wei – MediumNote: If you choose to repost or use this article, please cite the original source.","link":"/kafka-interview1/"},{"title":"MySQL interviews: Briefly describe the primary and secondary synchronization mechanism of MySQL","text":"let’s structure curriculum on primary/secondary synchronization mechanism in MySQL and understanding what happens if the synchronization fails Thank you for reading this article. More Interview Questions here:https://programmerscareer.com/software-interview-set/ Topic: 1.1 Introduction to Database SynchronizationDatabase synchronization is a critical concept in the world of databases. The complexity of maintaining accurate, consistent data across multiple platforms, databases, or systems has always been a challenge. This is where database synchronization shines. When we talk about database synchronization, we’re referring to the process of ensuring that the data in two or more databases is consistent. This usually means that the data in all databases should be the same, reflecting all updates in any of the databases. For example, in a banking system, a customer’s account balance should be the same whether it’s checked online, at an ATM, or at a branch. If the customer makes a withdrawal at a branch, this should be immediately reflected in the balance that’s seen online and at ATMs. This real-time accuracy is achieved through synchronization of the various databases involved. Some of the key benefits of synchronization include: Data Consistency: Database synchronization ensures that data remains consistent across all platforms. This is critical in many sectors, like finance and healthcare, where data accuracy is paramount. Efficiency: By ensuring that changes in one database are reflected in all others, synchronization aids in making systems more efficient and data more reliable. Redundancy is reduced, and users always have access to the latest data, no matter where they’re accessing it from. Scalability: As a system grows in size, so does its data. Database synchronization allows for easy scaling of databases as data input increases. Multiple servers can be synced to handle more data, improving the system’s overall performance. Backup: Synchronization can serve as a form of data backup. If one server goes down, the data is not lost because it’s mirrored in a different server. This increases data reliability and system durability. In the next part of the curriculum, we’ll take a closer look at the primary and secondary architecture models in database systems, including MySQL. Stay tuned! Topic: 1.2 Understanding Primary/Secondary ArchitecturesIn many database systems, and particularly in MySQL, a popular structure is the&nbsp;Primary/Secondary Architecture, also known as the Master/Slave architecture. Before we delve into the architecture specifics, let’s briefly discuss what each component represents. Primary Database (Master Database): This is the original or main database. Any changes or updates made here are also reflected in the secondary database(s). The primary database is typically in read-write mode and is often where most of the application operations take place. Secondary Database (Slave Database): These are the replicas(s) of the primary database. The secondary database often exists to enhance reliability, data recovery, and load balancing. They replicate the data in the primary database, and while some applications allow two-way synchronization (updates on either database are reflected in the other), many secondary databases are read-only. In a MySQL environment, the primary database logs the updates performed on it in a binary log (binlog). This log serves as a written history of all changes and can be used to replicate these changes to the secondary database. Pretty cool, right? When an event or a transaction is executed on the primary server, nothing happens immediately on the secondary server. Instead, the event is first written to the binary log on the primary server. The secondary server has a component named I/O Thread which connects to the primary server and copies the binary log events to its relay log almost instantly. Another component named the SQL thread reads the events from the relay log and applies them to the secondary server. This way, the same events are executed in the same sequence on the secondary server and thus, the data on both servers is consistent. This model provides benefits such as backup provision, analytics performance, read scaling, and high availability. However, it requires careful management to ensure data consistency and avoid conflicts. In our following lesson modules, we’ll dive deeper into other specifics of this synchronization mechanism and how to handle potential issues efficiently. Topic: 1.3 Synchronization Mechanisms in MySQLMySQL has a rich set of mechanisms to ensure data is kept consistent across different databases. Here are the key elements involved in MySQL synchronization: 1. Binary Logging:&nbsp;The binary log records all changes made to the MySQL data. This includes data changes such as table creation operations or changes to table data, as well as how long each statement took that caused a change. This plays a key role in synchronizing the data. 2. Replication:&nbsp;Replication is one of the most popular features used in MySQL. It allows data from one MySQL database server (the primary server) to be replicated to one or more MySQL database servers (the secondary servers). Replication is asynchronous by default, which brings a great level of flexibility. But you can also optionally setup semi-synchronous replication. 3. Global Transaction Identifiers (GTIDs):&nbsp;GTIDs make tracking transactions much easier. When a transaction occurs, it is given a GTID which is unique across all servers. The primary benefit of GTIDs is to enable much simpler automated failover and increased reliability. 4. Group Replication:&nbsp;Group Replication enhances MySQL replication. It provides built-in detection of servers that crash or become unreachable and can reconfigure the group, primary elections and automatic distributed recovery from other group members so business operations don’t have to be halted. 5. InnoDB ReplicaSet:&nbsp;For smaller scale setups that do not require highly available systems, a lighter method for failover management called InnoDB ReplicaSet can be deployed. It provides easy to use command-line tools to set up and administer smaller scale replicasets. 6. Semisync Replication:&nbsp;Semisync replication provides an option for a commit to return successfully to a client only if the data to be replicated was sent to another replica. Semisync replication can be used to prevent data loss due to a lost or crashed primary by blocking transactions until a replica acknowledges that it has written the events to its replica log. Thus, we can say SemiSynchronous Replication is a compromise between the high durability of synchronous replication and the low latency of asynchronous replication. MySQL achieves data consistency with these synchronization mechanisms. These mechanisms ensure the replicas receive updates made on the primary, resulting in data harmony across prospective data-crunching pipelines. Up next, we’ll delve into the consequences of synchronization failure and how to detect and mitigate these occurrences. Topic: 1.4 Consequences of Synchronization FailureIn any system where synchronization is vital, such as in a Primary/Secondary setup in MySQL, failure of this synchronization can lead to various issues. Here are some potential consequences of synchronization failure: 1. Data Inconsistency: This is one of the most immediate and visible impacts of a synchronization failure. In a banking application, for example, you might end up with different account balance values in different databases, which could lead to major financial implications. 2. Service Interruptions: If servers are not properly synchronized, services relying on the database could face performance issues or even complete failure. This can disrupt the availability of applications and can lead to a poor user experience. 3. Data Corruption: In worst-case scenarios, synchronization failure could even lead to data corruption. This happens if, for instance, two users simultaneously modify the same data but those modifications are not synchronized properly. Understanding the symptoms of synchronization failure is as important as understanding its consequences. Symptoms can include an increase in the number of errors or exceptions in your logs, a sudden drop in performance, or inconsistencies in your data when comparing between the primary and secondary databases. Mitigation strategies usually start with detecting the failure through regular checks of the database health or configuring alerts for specific error codes related to replication failure. Once detected, quick response is required to diagnose the cause of the issue and taking corrective actions. The nature of those corrective actions will depend on the specific issue and the configuration of the database and could range from a simple database restart to a more drastic full data resync or even failover to a different server. Now, we know that preventing problems is better than fixing them. This takes us to our next topic, which is about best practices to prevent synchronization failures from happening in the first place. Topic: 1.5 Preventing Synchronization FailuresPreventing synchronization failures in MySQL databases involves careful planning, monitoring, and application of best practices to ensure consistency of your data. Here are some vital steps to achieve this: 1. Regular Monitoring:&nbsp;Regularly monitor your database health and performance. This includes monitoring the status of your replication, checking the status and error logs, and setting up alerts for various replication events. 2. Use Reliable Networks:&nbsp;Network failures can cause major synchronization issues. Therefore, ensure that your primary and secondary servers are connected via a reliable network. Consider using redundant network paths for increased availability. 3. Thorough Error Handling in Applications:&nbsp;Your application should also be well-equipped to handle errors, including those from the database. Thorough error handling can prevent instances of synchronization failures due to application errors. 4. Use GTIDs:&nbsp;As we discussed earlier, Global Transaction Identifiers (GTIDs) can be very handy in preventing synchronization failures as they provide a consistent way to track each replication event across all servers. 5. Regular Backups:&nbsp;Regularly back up your database. Backups are your last line of defence in case of catastrophic failures. Also, validate your backups by restoring them in a separate environment to make sure they’re good. 6. Test Failover Scenarios:&nbsp;Regularly test your failover scenarios under controlled conditions to understand what the potential issues can be during actual failover scenarios. This helps in minimizing the RTO (Recovery Time Objective) when an actual outage happens. 7. Use Semisynchronous Replication:&nbsp;As we’ve discussed in the previous lessons, semisynchronous replication can also help prevent “commit succeed inconsistencies”. In this approach, the primary server waits until at least one secondary server has received and logged the changes to its relay log. 8. Keep Binlogs Until All Replicas Have processed Them:&nbsp;This can prevent issues where a primary crashes and a backup primary is then promoted which is at an earlier point in the replication stream. By applying these strategies, you can drastically reduce the chances of encountering synchronization failures in your MySQL environment. We’ll proceed to real-world scenarios in our next topic to bridge the gap between theory and practice. Topic:&nbsp;1.6 Synchronization Failures Case StudyTo better understand how synchronization failures occur in real world scenarios, let’s use a hypothetical case reflective of problems that may be faced in practice: Let’s assume we have a tech startup with a mobile app that has a rapidly growing user base. The company uses a primary-secondary MySQL setup to manage its user data. One day, they released a new feature that led to a surge in database writes due to increased user operations. Although this was a happy problem given the app’s success, it led to an unexpected issue: the secondary server began lagging behind the primary. As user operations increased, delays in the secondary server’s processing of the binary logs from the primary led to this lag. This is termed as replication lag. This is a common issue in synchronized MySQL setups. In this scenario, the failure wasn’t a sudden crash but a growing lag, which is often harder to detect immediately. Users began to notice inconsistencies in their app experience. For example, a user might delete a post but still see it in their feed because read operations directed at the delayed secondary server still found the post there. The company eventually detected the issue through their monitoring systems noticing an increasing replication lag and took immediate action. Their response involved: Scaling their database setup: They added more secondary servers and optimized their distribution of read operations among these servers to handle the load better. Buffering writes: They implemented a queue system for non-critical write operations, thus reducing immediate load on the database. Optimizing their app operations: They found that many concurrent read and write operations were not necessary and reworked their app logic to reduce these. Through this situation, the company learned the hard way about the importance of actively monitoring the health of their database setup, anticipating scaling requirements, and optimizing app operations to reduce unnecessary database loads. The issues faced in this scenario and the steps taken to rectify them are typical to many real-world applications. From this case study, we see the importance of the preventive and mitigative measures we talked about in the previous lessons. In our next session, we will revisit and review the key concepts we have learned in our lessons, reinforce them with some practical assignments, and evaluate your understanding with some assessments. Topic: 1.7 Review and AssessmentsReviewLet’s recap the key concepts we’ve covered throughout this course: Database Synchronization: We started by understanding the need for database synchronization, its benefits, and potential challenges. This concept is crucial for maintaining data consistency across multiple database instances. Primary/Secondary Architectures: We explored the commonly used primary/secondary architecture in MySQL. This setup allows data to be replicated from a primary server to one or more secondary servers. Synchronization Mechanisms in MySQL: We dived into the mechanisms used by MySQL to achieve synchronization, including binary logging and replication. Consequences of Synchronization Failure: We discussed the potential impacts of synchronization failure, from data inconsistency to service interruptions and even data corruption. Preventing Synchronization Failures: We learned about various strategies and best practices to prevent these failures, like regular monitoring, GTIDs, and reliable network connections. Synchronization Failures Case Study: We studied a real-world scenario to understand how such failures can occur and the steps to handle them. AssessmentsNow, let’s test your understanding with some questions: Why is synchronization necessary in a database system? And specifically, in a primary/secondary set up? What are the key mechanisms MySQL uses to achieve synchronization? What could be the potential consequences of a synchronization failure? Describe some strategies to prevent synchronization failures in MySQL databases. In the case study we discussed, how did the company detect the issue? And what were their responses to handle the situation? Please, take your time to answer these questions. Your understanding of these concepts is more important than speed. 1. Why is synchronization necessary in a database system? And specifically, in a primary/secondary set up? Synchronization is essential in a database system to ensure data consistency across multiple database instances. In a primary/secondary setup, it allows data to be replicated from a primary server to one or more secondary servers to enhance performance and deliver reliable, redundant data storage. 2. What are the key mechanisms MySQL uses to achieve synchronization? MySQL achieves synchronization through binary logging and related replication techniques. The primary server produces a binary log of all data changes. Secondary servers fetch this log and apply the changes, thus achieving synchronization with the primary server. 3. What could be the potential consequences of a synchronization failure? The consequences could include data inconsistency across servers, service interruptions, and in severe cases, data corruption. This could lead to a unreliable system, and negatively impact users’ experience. 4. Describe some strategies to prevent synchronization failures in MySQL databases. Preventive strategies include regular monitoring of database health and performance, using reliable network connections, thorough error handling in applications, employing Global Transaction Identifiers (GTIDs), conducting regular backups, testing failover scenarios, semisynchronous replication, and making sure to keep binary logs until all replicas have processed them. 5. In the case study we discussed, how did the company detect the issue? And what were their responses to handle the situation? The company detected the issue through their monitoring systems noticing an increasing replication lag. Their response involved scaling their database setup by adding more secondary servers, implementing a queue system for buffering writes and reducing immediate load on the database, and optimizing their app operations to reduce unnecessary database loads. 中文文章: https://programmerscareer.com/zh-cn/mysql-interview1/Author: Wesley Wei – Twitter Wesley Wei – MediumNote: If you choose to repost or use this article, please cite the original source.","link":"/mysql-interview1/"},{"title":"MySQL interviews: Briefly describe gap locks in MySQL","text":"Have you ever been asked a similar question in an interview? Or you will meet in the future, let’s explore and master it together Thank you for reading this article. More Interview Questions here:https://programmerscareer.com/software-interview-set/ Topic: Deep Dive into MySQLMySQL is a widely used, open-source Relational Database Management System (RDBMS). It uses a relational database and SQL (Structured Query Language) to manage its data. The “My” in MySQL is named after co-founder Michael Widenius’s daughter, My. MySQL database is a stable, reliable and powerful solution with advanced features like: Robust Transactional Support Replication &amp; Failover Cluster Support Workflow Control &amp; Scheduled Job Query Caching Advanced Replication Technology That makes MySQL an excellent choice for applications that demand complete data protection and real-time analytics like financial, banking, e-commerce, CRM, ERP applications and many more. Next, let’s start understanding databases from a theoretical perspective. For the sake of simplicity, let’s imagine a database as a big, digital filing cabinet full of file folders. The file folders represent tables. Within each table is the real data, represented as records. Each record includes information about a single entity. For example, if you own a business, you might have a database with a table for Customers, another for Order, and so on. Each row in the Customers table would represent a single customer, and each row in the Orders table would represent a single order. Topic: Exploring MySQL TransactionsTransactions are a fundamental concept of all database systems. A transaction in MySQL is a group of SQL statements that are executed as if they were one single unit. A transaction follows the ACID model which stands for Atomicity, Consistency, Isolation, and Durability. This model ensures the reliability of database transactions. For example, if you’re transferring money from one bank account to another, it involves several operations such as debiting money from one account and crediting that money to another. Here, transactions make sure these operations (credit and debit) happen entirely or not at all. If one operation fails, the whole transaction fails, ensuring data integrity. Our next step is to learn about locking mechanisms in MySQL, which are closely related to transactions. In the context of databases, a lock is a flag associated with a record. This flag can regulate whether the record can be read from or written to. It is locking that allows many users to access the database at the same time without conflict. When a record or a table is locked, it means some transaction is accessing the data and should not be interrupted. Topic: Introduction to Locking in MySQLIn the realm of databases, “Locking” is an essential feature that ensures consistency and order in concurrent data access. In MySQL, the InnoDB storage engine supports several types of locks at different levels to make sure transactions do not interfere with each other in an undesired manner. Locking is particularly crucial when there are several transactions trying to access and manipulate the same piece of data. When we say a transaction “locks” a piece of data, it prevents other transactions from making conflicting changes to that data until the lock is released. There are two primary types of locks: Shared Locks (S): This is a read-only lock. More than one shared lock can be held for a particular piece of data as long as there’s no exclusive lock. Exclusive Locks (X): An exclusive lock is a write lock. When a transaction holds an exclusive lock on data, no other transaction can read or write the data until the lock is released. Locking in MySQL can take place at three levels: Row-level locks: These locks are placed on rows of data. This is the finest level of locking granularity and allows the highest degree of concurrency. Page-level locks: These locks are placed on blocks of rows called “pages”. Page-level locks are less fine-granular than row-level locks and offer a medium degree of concurrency. Table-level locks: These locks are placed on an entire table. This is the coarsest level of locking, and it offers the lowest degree of concurrency. You typically want to avoid table-level locks in high transaction environments because they can become a bottleneck. Once we understand these basic locking concepts, we can dig deeper into some advanced types of locks in MySQL, including gap locks. Topic: Row Locks and Table Locks in MySQLTo ensure data integrity while allowing for maximum concurrency, MySQL employs two types of locks: row-level locks and table-level locks. Each of these has its own place and purpose. Row-Level Locks Row-level locking is more granular and is used when a specific row of the table is being updated. This means that only the rows involved in an operation are locked and not the entire table. This allows for a higher degree of concurrency, where multiple transactions can access different rows from the same table simultaneously. InnoDB supports row-level locking. It sets locks automatically during read and write operations but it doesn’t lock the entire table. Example: If you’re updating a specific record in an Employee Table, such a locking mechanism would only block transactions trying to modify that particular Employee record. However, tasks that involve other Employee records can proceed unhindered. Table-Level Locks Table-level locking is less granular. It locks the entire table during a particular database operation. Most often MySQL applies such locks during write operations. Whilst this locking method allows for simple management and less memory use, the level of concurrency is low when compared to row-level locks. Thus, table-level locks can be inefficient for high concurrency use cases, where many transactions need to access the same table simultaneously. By understanding the inner workings of these two types of locks, you are one step closer to mastering database manipulation with MySQL. Having this knowledge will also help when we delve into more complex topics like gap locks. Topic: Discussing Gap Locks in MySQLGap locking is a crucial MySQL mechanism used to prevent phantom rows. A phantom row is a row that matches a query’s WHERE clause; however, it is not initially seen or changed by the transaction. Let’s imagine a situation where we have a transaction that selects rows in a specific range with the intention of later updating those rows. During this operation, another transaction inserts a new row into that range, creating what we refer to as a “phantom” row. Without gap locks, the first transaction won’t be aware of the new row added by the second one and may lead to data inconsistency. This is where gap locks prove beneficial! A gap lock is a lock on a gap between index records. More explicitly, it’s a lock on the range of index records. Gap locks in MySQL prevent other transactions from inserting new rows into the gap locked by a transaction providing repeatable reads. Suppose you have an index on a column and you run the following statement in a REPEATABLE READ isolation level: 1SELECT * FROM table_name WHERE index_column &gt; 100 FOR UPDATE; MySQL will put a next-key lock on all index records where&nbsp;index_column&nbsp;is greater than 100 and a gap lock on the gap following those index records. Remember, though, that gap locks are a double-edged sword! While they can ensure consistency, they might also introduce lock waits or even deadlocks if not managed properly. Topic: Example Scenarios for Gap LocksTo understand gap locks better, let’s go with an example scenario. Assume we have a table&nbsp;orders&nbsp;and it has a bunch of rows. Scenario 1:For instance, let’s consider the following SQL statement, 1SELECT * FROM orders WHERE id &gt; 3 FOR UPDATE; With this query in a transaction, MySQL will put an exclusive next-key lock on all records where id &gt; 3. That implies that no other transaction can insert any new records with the id value &gt; 3 into the&nbsp;orders&nbsp;table till the first transaction is completed. Scenario 2:Now let’s consider another SQL statement, 1INSERT INTO orders (id, item) VALUES (102, 'New_Item'); If we try to execute this statement while the earlier transaction (with the SELECT … FOR UPDATE statement) is still active, it will have to wait till the first transaction is completed. This is because of the gap lock applied by the first transaction, which doesn’t allow any new records with id &gt; 3. These example scenarios illustrate how gap locks control the concurrent transactions, ensuring consistent data state and eliminating phantom reads under certain transaction isolation levels like REPEATABLE READ or SERIALIZABLE. With this understanding, we can now move forward to more intricate details about traversing the Locking Labyrinth in MySQL. Topic: Traversing the Locking LabyrinthLocks in MySQL form an intricate labyrinth where each lock plays an essential role, but it could cause confusion, delay, or even deadlocks if not handled properly. Here’s a simplified view on how the main types of locks interact with each other: Shared Locks and Exclusive Locks: A shared lock allows other transactions to read (shared lock) the locked object but not to write (exclusive lock) it. An exclusive lock prevents other transactions from reading/writing the locked object. Additional shared locks can be applied to an object that’s already been shared-locked, but requests for exclusive locks will wait. Table Locks and Row Locks: Table locks are straightforward but offer less specificity, leading to higher chances of transaction delay. Row locks provide higher concurrency as they only lock specific rows in a table. Gap Locks and Next-Key Locks: Gap locks prohibit insertion into a specific range of index records. They team up with row locks (or next-key locks) to prevent phantom reads in REPEATABLE READ or SERIALIZABLE isolation levels. Intent Locks: Intent locks indicate the kind of lock a transaction intends to acquire (shared or exclusive) before it actually obtains it. They are a notification mechanism, not a control mechanism. Auto-Increment Lock: Auto-increment locks are used to maintain the sequence of auto-increment values. They avoid conflicts when multiple transactions attempt to insert into an auto-increment column simultaneously. Navigating this labyrinth successfully requires a clear understanding of each lock type and how transactions connect and affect each other. Topic: Review and AssessmentsOver the past lessons, we’ve covered a lot of ground on MySQL and its locking mechanisms. Let’s briefly revisit these concepts to ensure a solid understanding: Deep Dive into MySQL: We started by getting to know MySQL’s interface, commands, and how it differs from other SQL implementations, setting a strong foundation for the learning process that followed. Exploring MySQL Transactions: We delved into the core concept of transactions in MySQL, discussing its consistency and isolation levels, which ensures data accuracy and concurrency. Introduction to Locking in MySQL: We introduced the concept of locking in MySQL, which is crucial in maintaining data integrity and concurrency control. Row Locks and Table Locks: We explored row-level locks and table-level locks and their significance in MySQL in managing concurrent transactions. Discussing Gap Locks in MySQL: We took a deep dive into gap locks, including what they are, how they work, and their importance in preventing phantom reads. Example Scenarios for Gap Locks: We walked through common scenarios where gap locks are beneficial to understand their practical implementation. Traversing the Locking Labyrinth: We discussed the interaction and influence among the various types of locks in MySQL, a complex but interesting topic. Example Problem:Consider a scenario where you have a high traffic database and you constantly find yourself running into deadlocks. Your task is to identify a plausible solution to minimize these occurrences. Solution: Potential solutions could be reducing transaction time, ensuring transactions access tables in the same order, or even increasing the innodb_lock_wait_timeout value. Also, making sure that the most precise locks are used can help reduce the chances of encountering deadlocks. Simple Problem:Consider a transaction that reads and writes several records in a table. What type of lock (row-level, table-level, or gap lock) would you use to ensure minimal blocking in a database with high traffic, with the condition that phantom reads should be avoided? Advanced Problem:In a ticket booking system, there can be concurrent transactions trying to book the same seat at the same time. How would you handle this situation using MySQL’s locking mechanisms to ensure a fair system? Expert Problem:In the context of MySQL, how might you deal with a deadlock scenario in a banking application where two transactions attempt to transfer money between two accounts concurrently? Simple Problem Solution:For this scenario, using a row-level lock mechanism would be the most efficient. It will provide the necessary locking to ensure data integrity while avoiding unnecessary blocking of unrelated rows in high traffic situations. Furthermore, including the “FOR UPDATE” clause in the SELECT statement could avoid phantom reads. Advanced Problem Solution:In a ticket booking system, to ensure a fair system, we can use the SELECT FOR UPDATE command. This will place exclusive nex-key locks on all index records the search encounters, thus preventing other transactions from inserting a new row in the gap covered by the record locks. It will also select the seat’s current status, and if it’s available, the transaction will update it as booked, ensuring that the seat can’t be double-booked. Expert Problem Solution:In a banking application where two transactions are concurrently attempting to transfer money, we may run into a deadlock scenario. To handle this situation, we could use a fixed order in accessing the accounts. For instance, transactions could access the account with the lower ID first. This will prevent a deadlock as both transactions won’t wait for each other indefinitely, eliminating the circular wait condition for deadlock. 中文文章: https://programmerscareer.com/zh-cn/mysql-interview10/Author: Wesley Wei – Twitter Wesley Wei – MediumNote: If you choose to repost or use this article, please cite the original source.","link":"/mysql-interview10/"},{"title":"MySQL interviews: What is the difference between a unique index and a normal index? What are the advantages and disadvantages of using indexes?","text":"Let’s dive into the review and assessment of understanding “Unique Index” and “Normal Index” in MySQL. Thank you for reading this article. More Interview Questions here:https://programmerscareer.com/software-interview-set/ Topic: Understanding Indexes in MySQLIn any relational database management system like MySQL, efficient access to data is one of the key aspects, especially when dealing with large amounts of data. The efficiency we’re concerned about is the speed and ease with which our system can locate and retrieve the data we need. This is where&nbsp;Indexes&nbsp;come into play. You can think of an index as if it were an index at the back of a book. Suppose you wanted to look up a specific topic in a book. You have two choices: You could scan through each page until you find the topic. You could go to the index, find the page number of the topic, and then flip directly to that page. The second method is faster, isn’t it? In the database world, scanning through all the data is called a&nbsp;full table scan. If you have millions of rows, this can take a very long time. But if you have an index, MySQL can use it to locate the data much more quickly — just like you used the book’s index to find your topic. An index creates an entry for each value and thus it will be much quicker to retrieve data. Keep in mind, however, that while indexes speed up querying, they can slow down the speed of writing operations (INSERT, UPDATE, DELETE) because each time we write, we need to update the index as well. Therefore, we need to maintain a balance and only use indexing on columns that will be frequently searched against. Topic: Normal Indexes in MySQLNow that we have a good understanding of what an index is and the role it plays in MySQL, let’s delve into one specific type of index, which is a&nbsp;Normal Index. A Normal Index, also known as a Non-unique Index, allows you to speed up the query process by creating an index on one or more columns of your database table. Unlike a Unique Index, a Normal Index does not impose any constraints on the values you can have in the column. In other words, a Normal Index allows duplicated values in the column(s) it is indexed on. To illustrate, let’s say we have a&nbsp;Students&nbsp;table that has columns&nbsp;ID, Name, Age, and Address. While querying data, we often use the&nbsp;WHERE&nbsp;clause to filter data. For example: 1`SELECT * FROM Students WHERE Age = 20` Without an index, MySQL would have to go through every row in the&nbsp;Students&nbsp;table to find the ones where&nbsp;Age&nbsp;is equal to 20. This can be time-consuming and inefficient. If we create a Normal Index on the&nbsp;Age&nbsp;column, MySQL can use this index to quickly locate the relevant rows. Creating a Normal Index in MySQL is pretty straightforward, you use the&nbsp;CREATE INDEX&nbsp;command, followed by the name you want to give to the index, and the table and column you want to create it on. Here’s an example of how you’d create an index on the&nbsp;Age&nbsp;column in the&nbsp;Students&nbsp;table: 1CREATE INDEX age_index ON Students (Age); Remember, while Normal Indexes can undeniably speed up read operations, they also take up storage space and can slow down write operations (INSERT, UPDATE, DELETE) as they need to be updated each time a write operation occurs. Therefore, they should be used thoughtfully and strategically. Topic: Unique Indexes in MySQLNow that we have a solid grasp on Normal Indexes, it’s time to discuss&nbsp;Unique Indexes&nbsp;in MySQL. A Unique Index is a type of index that enforces a constraint that all values in the index must be different. This means, a Unique Index doesn’t allow duplicate values in the column (or combination of columns) it is indexed on, making it useful when you want to prevent duplication in certain fields. For example, consider a Users table in a database where every user is supposed to have a unique email address. In such a scenario, a Unique Index on the email column would ensure that two users cannot have the same email. The syntax to create a Unique Index is just slightly different than a Normal Index: 1CREATE UNIQUE INDEX index_name ON table_name (column_name); You can replace&nbsp;index_name&nbsp;with the name you want to give to the index,&nbsp;table_name&nbsp;with the name of the table in which you want to create the index, and&nbsp;column_name&nbsp;should be replaced by the name of the column on which you want to create the index. For example, to create a unique index on the&nbsp;Email&nbsp;column in the&nbsp;Users&nbsp;table, you would write: 1CREATE UNIQUE INDEX email_index ON Users (Email); Every time a new email is inserted or an existing one is updated in the Users table, MySQL will check the unique index, and if it finds another row with the same email value, it won’t allow the change to be made. Keep in mind, a Unique Index not only helps in ensuring data integrity but, just like a Normal Index, it can also help in improving the performance of data retrieval operations. Topic: The Differences Between Normal Indexes and Unique IndexesAs we’ve discussed in the previous sessions, indexes are integral to efficient data operations in databases. We’ve also looked at two specific types of indexes:&nbsp;Normal Indexes&nbsp;(or non-unique indexes) and&nbsp;Unique Indexes. Both of these index types fulfill different roles and it’s important to understand the differences. Uniqueness: The foremost difference lies in the name itself — Normal Indexes in MySQL allow duplication in the column or set of columns. In contrast, a Unique Index will prevent the insertion of a new row with a duplicate index column value. Usage: Normal indexes are mainly used to increase the speed of operations in MySQL. Unique indexes, however, serve a dual purpose. They can both increase operational efficiencies and maintain data integrity by rejecting duplicate values. Constraints: When you insert a row in a table that has a Unique Index, MySQL first checks whether inserting the new data will violate the uniqueness constraint. If it does, MySQL rejects the change and issues an error. In contrast, with Normal Indexes, MySQL does not perform such checks. It’s crucial to know when to use which type of index. When you need to speed up queries on a large dataset, a Normal Index would do the job. But if you need to ensure data consistency in a column where each value must be unique, you would use a Unique Index, even though it will consume more resources to enforce the uniqueness constraint. Topic: Optimizing SQL Queries with IndexesEnhancing the performance of our database by organizing its data is one of the primary objectives of using indexes in MySQL. When used properly, indexes can significantly speed up data retrieval operations. Here are some pointers on how to optimize your SQL queries using indexes: Index the Search Fields: It seems pretty straightforward, but it’s worth repeating. If you are frequently searching a particular field in the table, consider indexing it. This could vastly improve your database performance. Consider Index Size: The smaller the index (in terms of data size), the faster it is. Therefore, indexed columns with smaller data types will usually be faster than those with larger data types. For example, an INT is faster than a VARCHAR, and a VARCHAR is faster than a TEXT. Limit Indexes on Write-Heavy Tables: Indexes can slow down write operations (like INSERT, UPDATE, and DELETE statements) because every time you modify the data, the index also needs to be updated. If a table is frequently updated, consider minimizing the number of indexes. Composite Indexes: They consist of more than one column and can speed data retrieval when you’re filtering on multiple columns in your WHERE clause. The trick is they work on the left-most prefix basis. That means the order of columns in the index matters. Use Explain Plan: MySQL’s EXPLAIN statement can show you how the MySQL optimizer would execute your query, helping you understand whether the database is able to utilize the index or not, and allowing you to optimize your queries further. That concludes our lesson on how to optimize your SQL queries using indexes. Topic: Common Pitfalls with IndexesIndexes in MySQL are powerful tools that can significantly speed up your queries. However, there are a few common pitfalls that you should be aware of when working with them. Too Many Indexes: Having numerous indexes can be counterproductive. Every index that you add increases the amount of time that MySQL spends updating and managing these indexes. This can slow down write operations. Hence, it’s important to have only necessary indexes. Not Understanding Cardinality: Cardinality is the number of unique values in the index. If the cardinality is low (meaning there are many repeated values), the index may not be very effective. You should pay attention to the cardinality of your indexes and consider if an alternative column might serve as a better index. Indexing the Wrong Column: Indexing should be done based on the columns that you will be searching or sorting on frequently. Indexing the wrong column can lead to inefficient queries. Ignoring the Query Execution Plan: The query execution plan provided by MySQL’s&nbsp;EXPLAIN&nbsp;statement can offer valuable insights into how your query will be executed and which indexes are used. Ignoring this information can lead to inefficient indexes or missed opportunities for optimization. Using Large String Indexes: Indexes on large VARCHAR or TEXT columns can consume a lot of memory and slow down your queries. This is where indexing a prefix of a column (INDEX(column(10))) can be useful. Remember, the secret to effective indexing lies in understanding your data and how your application queries it. An optimal number of well-chosen indexes can make your database perform dramatically better. Topic: Review and AssessmentsWe’ve learned a lot about MySQL indexes, including what they are, their types (normal and unique indexes), how they are used in query optimization, as well as some common misconceptions and pitfalls when implementing them. Now it’s time for a quick assessment. This will reinforce your learning and help highlight any areas we might need to revisit. Example Problem:To see how well you’ve grasped the topic, let’s go through an example problem. We have a&nbsp;students&nbsp;table in our MySQL database with the following structure: 1| id (INT) | name (VARCHAR) | class (VARCHAR) | age (INT) | You need to frequently run a query to find students in a specific&nbsp;class. How can you optimize this query? Solution:To optimize this query, we could add an index on the&nbsp;class&nbsp;column. As we’re frequently searching this field, having an index could significantly increase the performance of our query. Here’s the SQL statement to do this: 1CREATE INDEX idx_students_class ON students (class); Now, let’s test your understanding: Simple Problem (Difficulty: 3/10): What SQL statement would you use to add an index on the&nbsp;age&nbsp;column in the&nbsp;students&nbsp;table? Complex Familiar Problem (Difficulty: 6/10): What factors should you consider before deciding to add an index? Complex Unfamiliar Problem (Difficulty: 9/10): The&nbsp;students&nbsp;table also has a&nbsp;registration_date&nbsp;column (date type) and you’re running queries to find students who registered in a particular year. What type of index could you use to optimize this query and how would you create it? Simple Problem: To add an index on the&nbsp;age&nbsp;column in the&nbsp;students&nbsp;table, you would use the statement: 1CREATE INDEX idx_students_age ON students(age); Complex Familiar Problem: Before adding an index, consider: The column’s cardinality: High cardinality columns (columns with many unique values) are best-suited for indexing. Your application’s read-write ratio: If the application performs many more reads than writes, indexing is beneficial. But if your application performs many write operations (insert, update, delete), the cost of maintaining the index could outweigh the benefits. The column’s data type: Indexing on smaller data type columns is faster. Complex Unfamiliar Problem: In the case of running queries to find students who registered in a particular year, you could create an index on the&nbsp;YEAR(registration_date). MySQL allows creating an index on a function or expression, known as a Functional Index. To create a functional index in MySQL 8.0+, you could use the following statement: 1CREATE INDEX idx_students_registration_year ON students((YEAR(registration_date)); This way, MySQL would directly map the year to the row in the index, thus speeding up your queries. Please note that creating an index on a function or expression is supported in MySQL 8.0 and later. If you’re using an earlier version of MySQL, you would need to add a separate column for the year and then index that column. 中文文章: https://programmerscareer.com/zh-cn/mysql-interview11/Author: Wesley Wei – Twitter Wesley Wei – MediumNote: If you choose to repost or use this article, please cite the original source.","link":"/mysql-interview11/"},{"title":"MySQL Interviews: Briefly describe the occurrence scenarios of dirty reading and phantom reading. How does InnoDB solve phantom reading?","text":"Have you ever been asked a similar question in an interview? Or you will meet in the future, let’s explore and master it together Thank you for reading this article. More Interview Questions here:https://programmerscareer.com/software-interview-set/ Topic: 1.1 Introduction to Problems in Database TransactionsAs we dive into the complexities of database transactions, it’s essential to acknowledge that these operations are not always straightforward. Their primary purpose is to execute a series of operations so that the database morphs from one consistent state to another. However, with simultaneous transactions, we encounter a plethora of issues that need to be resolved for maintaining consistency and integrity. One such challenge arises due to the concurrency of transactions. To maintain the&nbsp;ACID&nbsp;properties (Atomicity, Consistency, Isolation, and Durability) of transactions, the database systems must handle the concurrent execution of transactions properly. The failure to ensure proper management can lead to several problems: Dirty Reading: This problem occurs when one transaction reads changes made by another transaction that hasn’t been committed yet. If the latter transaction is rolled back for some reason, the former would have read an invalid value. Non-Repeatable Read: This occurs when a single transaction reads the same row multiple times and gets different data each time as other transactions are updating this row simultaneously. Phantom Read: This scenario is a variation of non-repeatable read where a transaction performs two identical queries, but the second result set includes additional rows that weren’t present in the first result set, added by a different transaction. These transaction control problems disrupt the smooth functionality of the database transactions and affect data integrity. In the following lessons, we’ll do a deep dive into the scenarios involving dirty reads and phantom reads and understand the solutions, including those offered by InnoDB engine in MySQL. Topic: 1.2 Understanding Dirty ReadingThe term “dirty read” in the context of a database involves one transaction reading uncommitted or “dirty” data from another transaction. Picture this: Transaction 1 modifies a certain row but hasn’t committed it yet. Now, before Transaction 1 is either committed or rolled back, Transaction 2 comes along and reads the uncommitted change. This phenomenon is known as a dirty read. Why is this a problem? Well, suppose Transaction 1 is eventually rolled back. In that case, the change is undone, but Transaction 2 has already read the uncommitted data, leading to inconsistencies and potentially invalid results in the database. Here’s a simple example for clarity: Step 1: Transaction 1 modifies a row in the&nbsp;orders&nbsp;table, updating the&nbsp;order_status&nbsp;from ‘Pending’ to ‘Shipped’. Step 2: Before Transaction 1 commits, Transaction 2 reads the&nbsp;order_status&nbsp;for the same row and finds it as ‘Shipped’. Step 3: Transaction 1 encounters an error and executes a ROLLBACK operation, changing&nbsp;order_status&nbsp;back to ‘Pending’. Step 4: Transaction 2, however, proceeds with the ‘Shipped’ status, hence reading data that never should have existed. Dirty read can lead to significant errors, particularly in data analysis or reporting processes where accuracy is paramount. Topic: 1.3 Understanding Phantom ReadingLike dirty reading, phantom reading is another concurrency problem that arises in database transactions. Phantom reading typically occurs when a transaction re-queries data it has already queried, but finds new rows that were not there in the initial read. These “phantom” rows are the result of another transaction that inserted or updated some rows after our original transaction began and before it ended. To visualise this, let’s consider a simple example: Step 1: Transaction 1 retrieves all rows from the&nbsp;orders&nbsp;table where&nbsp;order_status&nbsp;is ‘Pending’. Step 2: Meanwhile, Transaction 2 inserts a new row in the&nbsp;orders&nbsp;table with&nbsp;order_status&nbsp;as ‘Pending’ and commits. Step 3: Now, Transaction 1 re-runs the same retrieval query. This time, it finds the row inserted by Transaction 2 — this is a phantom row. The problem of phantom reads persists mostly in lower isolation levels such as “Read Committed” but not in higher isolation levels like “Serializable”. This is due to the use of exclusive range locks that prohibit the insertion of new rows in the read range for “Serializable” isolation level. However, these higher levels of isolation come with their own problems such as lower concurrency and higher contention. Therefore, the selection of transaction isolation level often involves a trade-off between performance and consistency. But don’t worry, technologies like InnoDB provide ways to handle these situations. Topic: 1.4 The Role of InnoDB in Handling Phantom ReadsThe InnoDB storage engine plays a crucial role in handling transaction problems, including phantom reads, in MySQL. It does so by using&nbsp;multi-version concurrency control (MVCC), which allows multiple transactions to access the same row without affecting each other’s work. Each transaction sees a snapshot of the database at the start of its work, keeping concurrent transactions isolated from each other. This contributes to maintaining the ‘I’ (Isolation) part of the ACID properties in MySQL InnoDB. Moreover, you can also set different isolation levels in MySQL to customize the balance between read consistency, concurrency, and performance. These isolation levels are READ UNCOMMITTED, READ COMMITTED, REPEATABLE READ, and SERIALIZABLE. The Repeatable Read level is the default level in InnoDB, which guarantees that all reads within the same transaction will see a snapshot of the database as it was at the start of the transaction. This feature effectively prevents phantom reads. However, in some business scenarios, the Serializable level, providing the highest data consistency but at the cost of concurrency and performance, might be required. In the later stage of this curriculum, we will discuss in detail how InnoDB implements the ACID properties and the customization of these transaction properties as per user requirements. Topic: 1.5 Transaction Isolation LevelsTransaction Isolation Levels play a central role in how a database management system like MySQL manages concurrency and protects transactions from potential problems like dirty reading, non-repeatable read, and phantom reading. In MySQL, there are four preset isolation levels, each with a different level of protection versus performance trade-off: READ UNCOMMITTED: This is the lowest level of isolation, and it allows transactions to see uncommitted changes made by other transactions. This means a transaction can see “dirty” data that another transaction may later roll back, leading to dirty reads. READ COMMITTED: This level guarantees that any data read is committed at the moment it is read. Thus, it prevents dirty reads. However, if a transaction reads the same row twice, it could see different values if another transaction modifies that row between the reads, leading to non-repeatable read. REPEATABLE READ: This is the default isolation level in InnoDB. It prevents both dirty reads and non-repeatable reads by ensuring that all reads of the same row for a single transaction return the same result, unless the row was changed by that transaction itself. SERIALIZABLE: This is the highest level of isolation. It locks the rows that a transaction reads, preventing other transactions from accessing them (read or write) until the first transaction is finished. While this level prevents dirty reads, non-repeatable reads, and phantom reads, it significantly reduces concurrency. Understanding these isolation levels is key to managing concurrent transactions effectively. In the coming topics, we will discuss some techniques and practices for implementing concurrency control based on these isolation levels. Topic: 1.6 Strategies to Implement Concurrency ControlConcurrency control in databases aims to allow multiple transactions to access the database without conflicts or errors simultaneously. To implement concurrency control effectively, there are several strategies we can leverage: Lock-Based Protocols: This common method works by giving a transaction lock access to a data item when it needs. There are exclusive and shared locks. The former doesn’t permit another transaction to access the data; the latter does but only for reading purposes. Timestamp-Based Protocols: This approach involves assigning a timestamp to each transaction, ensuring that earlier transactions get priority in case of conflict. Validation-Based Protocols: Also known as optimistic concurrency control, this method allows transactions to execute without restriction and validates the transactions only at commit time. Multiversion Concurrency Control (MVCC): Primarily used in InnoDB, MVCC allows each user connected to the database to view the database from a consistent snapshot set at the start of their transaction. Granularity of Data: This decides the size of the data item for locking — from a single row to the entire database. Each of these strategies has its strengths and trade-offs. For example, lock-based protocols can create performance issues due to lock contention, while MVCC can provide high concurrency with reduced need for locking at the potential cost of increased storage. It’s important to choose a strategy that aligns with your application’s needs and considerations, such as performance, consistency, and complexity. Topic: 1.7 Review and AssessmentsYou’ve done a fantastic job exploring the crucial aspects of database transactions, understanding the concept of concurrency control, the problems it poses, and the various strategies employed to handle such issues. By now, we’ve journeyed through understanding the necessity of handling simultaneous transactions — concurrency control. We’ve identified potential challenges like dirty reading, non-repeatable read, and phantom read, which basically involve how transactions handle changes in data from other transactions. We took a deep dive into what dirty reading is, how it occurs, and its implications on database transactions. Similarly, we had an in-depth discussion into the occurrence scenarios of phantom readings and the issues it can cause. We investigated the role of the InnoDB storage engine in MySQL, specifically how it implements a ‘consistent read’ to manage phantom readings. We further dissected the concept of Transaction Isolation Levels, understanding how configuring different transaction isolation levels can in turn affect the occurrence of dirty and phantom reads. We also touched upon the various strategies employed to handle concurrency issues — for instance, transaction scheduling and using various types of lock-basedprotocols. As we wrap up this unit, let’s revisit some of these key concepts covered through an assessment to evaluate your understanding of dirty reading, phantom reading, and the methods used by InnoDB to resolve these issues. Alright, let’s move on to the three assessment problems. Try to solve each one, and I’ll provide the solutions afterward. Simple Problem (Difficulty: 3/10)Suppose you have two transactions happening simultaneously where transaction 1 reads a data object, and transaction 2 updates it later. Identify the kind of reading problem that could occur here and explain your reasoning. Complex Familiar Problem (Difficulty: 6/10)In the context of database transactions, how does the InnoDB storage engine in MySQL handle phantom readings? Describe how ‘consistent read’ contributes to managing these phantom reads. Complex Unfamiliar Problem (Difficulty: 9/10)You are the database manager of a banking system. Two transactions are happening concurrently, one where the customer ‘A’ transfers an amount to customer ‘B’, and the other where the bank calculates the total balance of customer ‘A’. Due to the concurrency of these separate transactions, the bank balance calculation occurs before the transfer transaction is complete. Explain the issues that could arise in this scenario. Simple Problem (Difficulty: 3/10)This problem is associated with a ‘dirty read’. A dirty read is a concept in Database Management where a transaction (Transaction 1) is allowed to read data from a row that has been modified by another transaction (Transaction 2) but not yet committed. So, if Transaction 2 rolls back the change, Transaction 1 will have read data that is considered not to have existed. Complex Familiar Problem (Difficulty: 6/10)The InnoDB storage engine in MySQL uses ‘consistent read’ for handling phantom readings. Consistent read is a non-locking read used by InnoDB select operations to provide a point-in-time snapshot of the data. It’s done by applying multi-version concurrency control (MVCC) where multiple versions of a record are stored. This way, users can view the database without lock-induced delays. Complex Unfamiliar Problem (Difficulty: 9/10)In the given scenario, a non-repeatable read may occur. Non-repeatable reads happen when a transaction reads the same row twice and gets different data each time. In this case, the bank balance calculating transaction may first calculate the balance before any transaction occurs then repeat the reading process after customer ‘A’ has transferred the amount to customer ‘B’. Hence the first and second read will result in different balances for customer ‘A’. 中文文章: https://programmerscareer.com/zh-cn/mysql-interview12/Author: Wesley Wei – Twitter Wesley Wei – MediumNote: If you choose to repost or use this article, please cite the original source.","link":"/mysql-interview12/"},{"title":"MySQL interviews: What is the difference between a clustered index and a non-clustered index?","text":"Have you ever been asked a similar question in an interview? Or you will meet in the future, let’s explore and master it together Thank you for reading this article. More Interview Questions here:https://programmerscareer.com/software-interview-set/ Topic: 1.1 Introduction to Clustered and Non-Clustered IndexesKnowing how your data is stored and retrieved can greatly impact the performance of your databases. More specifically, understanding&nbsp;how&nbsp;MySQL makes use of indexes is crucial to this understanding. This is where the concepts of&nbsp;Clustered&nbsp;and&nbsp;Non-Clustered Indexes&nbsp;come into play. Indexes, as you may have learned from your prerequisite studies, are a vital component of databases. They are essentially lookup tables that the database engine utilizes to speed up the data retrieval process, much like how an index in a book helps you quickly locate information without having to read each page. Now, let’s specifically dive into what clustered and non-clustered indexes are: Clustered Indexes: Just as it sounds, a clustered index dictates the&nbsp;physical order of data in a table. To understand this better, consider a clustered index as a dictionary. In a dictionary, words are not randomly scattered across but are stored alphabetically, so you can quickly jump to a section for a particular letter and find the word you’re looking for. In the context of a database, this “word” is your data row. MySQL, in fact, organizes rows of data based on the clustered index so that retrieval is faster. Please note that there can only be one clustered index per table. Non-Clustered Indexes: Non-clustered indexes, on the other hand, do not dictate the physical order of data. Yet, they carry a ‘pointer’ to the data. To understand better, if a clustered index is like a dictionary, then a non-clustered index can be considered like an index in a book. The index points you to the pages where information resides, but it doesn’t contain the information itself. This means that once the database engine looks through the non-clustered index, it must perform extra work to ‘go’ to the data row, compared to a clustered index where the data resides with the index. Hence, data retrieval via non-clustered indexes may be slower. However, you can have multiple non-clustered indexes, which can be beneficial for a variety of data retrieval scenarios. Topic: 1.2 Clustered Indexes ExplainedClustered indexes are all about the physical storage of data. When you create a clustered index on a table, the rows of the table are stored on disk in the same order as the index key. There can only be one clustered index per table, and if you don’t explicitly define a clustered index, SQL Server will automatically create one for you. This is known as a heap. The structure of a clustered index is also known as a B+ tree structure. At the root of the tree, which is the topmost level, you have the root node. This root node then branches out into multiple leaf nodes at the bottom-most level. These leaf nodes contain the actual data rows in the order of the index. A primary key constraint automatically creates a clustered index on that particular column. However, in some cases, you might want to manually create a clustered index on a non-primary-key column. This depends on your requirements. For example, if you have a table with employees’ data and constantly query data in the order of employees’ hire date, it might be beneficial to create a clustered index on the hire date column to speed up these queries. Furthermore, the update of records in a table with a clustered index may be slower than that in a heap. That’s because when a record is updated in a table with a clustered index, the database might need to physically move the entire row to maintain the sort order. Topic: 1.3 Non-Clustered Indexes ExplainedSo now we have a good understanding of what a clustered index is and how it sorts and stores data on disk. But sometimes, we don’t always want to retrieve data based on the (single) clustered index. That’s where the&nbsp;non-clustered index&nbsp;comes into play. A non-clustered index is markedly different from a clustered index. For starters, creating a non-clustered index does not rearrange the physical order of data in the table. Instead, it creates a distinct object within the database that houses a sorted list of pointers that point to the data in the table. Here’s a neat example to illustrate. Imagine a book — rather than looking through every page for a particular topic, you would generally turn to the book’s index, right? It guides you straight to the pages that contain your specified topic. That quick directing is the function a non-clustered index performs! The architecture of a non-clustered index is similar to a clustered one — a B-tree data structure with root nodes, intermediate level nodes, and leaf nodes. However, the leaf nodes of a non-clustered index consist only of the index columns and a pointer to the corresponding row in the data table. You can have multiple non-clustered indexes on a single table, with each catering to a specific query you want to speed up. In MySQL, a non-clustered index is essentially all secondary indexes you create, with each of them containing a copy of the primary key columns for the rows where the search key matches. Topic: 1.4 Differences Between Clustered and Non-Clustered IndexesWith a solid understanding of what clustered and non-clustered indexes are, let’s now clarify the key differences between the two: Order of Data: A clustered index determines the physical order of data in a table. On the other hand, a non-clustered index doesn’t alter the way the records are stored but creates a separate object within a database that points back to those original records. Number of Indexes: A table can have only one clustered index, but multiple non-clustered indexes. Remember, the more indexes, the more disk space required. Data Retrieval Speed: Clustered indexes can lead to faster data retrieval than non-clustered, but that’s not always the case. If a non-clustered index covers a query (meaning, the query’s data can all be served from the index’s leaf nodes), it can retrieve data faster despite having a few extra hoops to jump through. Update Performance: Clustered indexes can slow down updates, while non-clustered indexes often have little effect on performance. Storage Space: As non-clustered indexes are stored separately from the table data, they require additional storage. Each non-clustered index is a separate disk structure that stores a sorted array of column values, whereas a clustered index is the actual table data and forms the lowest level of an index. In the grand scheme of databases — the speed at which data can be retrieved, the efficiency of storage, the quickness of updates — all of these factors rely heavily on proper indexing. Being clear on when and why to use either clustered and non-clustered indexes puts you in control of optimizing your database performance. Topic: 1.5 Choosing the Right IndexGreat job! Now that we know what clustered and non-clustered indexes are and the key differences between them, let’s dive into choosing the right index. Choosing the right index for performance optimization in MySQL comes down to understanding the queries that will be executed against your database. It’s not just about whether to use clustered or non-clustered indexes, but also includes understanding columns and their cardinality. Here are a few key points to help you decide: Choose a Clustered Index for Wide Column Queries: Since clustered indexes are essentially the table data itself, they are excellent for wide column queries because of the reduced number of reads required. Choose a Non-Clustered Index for Specific Column Queries: Non-clustered indexes are useful when you need to retrieve a smaller subset of columns often. In such cases, creating a non-clustered index on these columns can be beneficial. High-Cardinality Columns: When a column has a high cardinality (each row is unique), using it as a clustered index can result in quicker look-ups. Low-Cardinality Columns: For low-cardinality columns (many rows share the same value), usage of non-clustered indexes is generally more efficient. Data Modification Operations: If your application entails frequent modifications (INSERT, UPDATE, DELETE operations), non-clustered indexes might be a better choice since they don’t impact the physical ordering of the data on disk. Space Considerations: Since non-clustered indexes are separate disk structures, they consume additional storage space. If storage space is a constraint, clustered indexes might be a better fit, albeit at the cost of speed, in some cases. Remember, the best strategy is always dictated by the specific workload at hand. It’s essential to continuously monitor and analyze performance and adjust your indexing strategy accordingly. Topic: 1.6 Examples and Use-casesWonderful! Well done so far. To solidify the understanding, let’s look at a few real-world examples and use-cases. Starting with a basic example, let’s say you directly manage an online bookstore. You have a&nbsp;Books&nbsp;table that contains the following columns:&nbsp;BookID,&nbsp;Title,&nbsp;Author,&nbsp;Genre,&nbsp;Price, and&nbsp;PublicationDate. Using a Clustered Index: Let’s say customers frequently search books by the&nbsp;BookID&nbsp;in your store. To enhance the speed of these common pull requests, you can use a clustered index on the&nbsp;BookID&nbsp;column. Since a clustered index determines the physical order of data in a table, row look-ups can be significantly faster. Using a Non-Clustered Index: On the flip side, if customers often look up books by&nbsp;Genre&nbsp;or&nbsp;Author, it could be beneficial to create a non-clustered index on these columns. As mentioned, non-clustered indexes are particularly useful when you need to retrieve a smaller subset of columns, which perfectly fits our case here. Use-case: Suppose your database has a&nbsp;Customers&nbsp;table storing millions of records, and you frequently need to retrieve customer information based on&nbsp;CustomerID. A clustered index on&nbsp;CustomerID&nbsp;can speed up these look-ups dramatically. However, if business needs require you to retrieve records based not just on&nbsp;CustomerID&nbsp;but also, let’s say,&nbsp;LastName&nbsp;and&nbsp;ZipCode, then non-clustered indexes on&nbsp;LastName&nbsp;and&nbsp;ZipCode&nbsp;can be more efficient. Keep in mind that these are just examples and the actual implementation may greatly vary depending on factors like data size, query complexity, and hardware capabilities. Understanding when to use clustered and non-clustered indexes — predicated on intelligent database design — is a crucial aspect of managing SQL databases. Topic: 1.7 Review and AssessmentsAmazing progress! Let’s review the main concepts we’ve covered and then move onto some assessments. Clustered Index: This type of index determines the physical order of data in a table. A table can have only one clustered index. Non-Clustered Index: This type of index is a separate disk structure referencing the table data, which helps speed up the queries that are not covered by clustered index. A table can have multiple non-clustered indexes. High vs. Low Cardinality: High-cardinality refers to columns with unique values on most, if not all, rows. Clustered indexes on high-cardinality columns can result in quicker look-ups. Low-cardinality refers to columns where many rows share the same value. Non-clustered indexes are generally more efficient for such columns. Choosing the Right Index: This depends on various factors including query types, cardinality, data modification needs, and space constraints. Now, let’s assess our understanding with a few questions: What is the key difference between a clustered and a non-clustered index? In which case would a non-clustered index be a more suitable choice over a clustered index? What do high-cardinality and low-cardinality mean and how do they affect the choice of index type? Question: What is the key difference between a clustered and a non-clustered index?Answer: The key difference between a clustered and a non-clustered index lies in the way they store and reference data. A clustered index determines the physical order of data in a table, essentially being the table itself, whereas a non-clustered index is a separate structure that points to the data located elsewhere in the database. Question: In which case would a non-clustered index be a more suitable choice over a clustered index?Answer: A non-clustered index would be more suitable when the database needs to support a lot of search queries on columns that are not part of the clustered index. Moreover, non-clustered indexes would also be preferable when the table undergoes frequent changes as changes do not result in the entire table needing to be reorganized, unlike with a clustered index. Question: What do high-cardinality and low-cardinality mean and how do they affect the choice of index type?Answer: Cardinality refers to the uniqueness of data values in a column. High cardinality means a column contains a large percentage of totally unique values, and low cardinality means a column contains many repeated values. High-cardinality columns are good candidates for a clustered index, whilst indexes on low-cardinality columns, where the column values are very repetitive, are less effective. 中文文章: https://programmerscareer.com/zh-cn/mysql-interview13/Author: Wesley Wei – Twitter Wesley Wei – MediumNote: If you choose to repost or use this article, please cite the original source.","link":"/mysql-interview13/"},{"title":"MySQL Interviews: What are database transactions and why does MySQL use InnoDB as the default option","text":"Have you ever been asked a similar question in an interview? Or you will meet in the future, let’s explore and master it together Thank you for reading this article. More Interview Questions here:https://programmerscareer.com/software-interview-set/ Topic 1.1 — Deep Dive into TransactionsIn simple terms, a transaction is a unit of work performed within a database management system (or similar system) against a database, and treated in a reliable way independent of other transactions. A transaction generally represents any change in the database. More technically, a transaction is a sequence of database operations that constitutes a logical unit of work. Consider the example of transferring funds from one bank account to another. This operation requires two steps: Withdrawal in one account. Deposit in another. Both steps must be complete for the transaction to be considered complete. If something were to happen in between these two operations, like a server failure, the transaction would be considered incomplete, and the database must be rolled back to its previous, consistent state. This way, transactions help maintain the&nbsp;Integrity&nbsp;of a database by ensuring that sets of related operations either all occur, or none occur. The concept of transactions ties in closely with the principles of ACID (Atomicity, Consistency, Isolation, Durability), providing a framework to ensure reliable processing of data. Topic 1.2 — MySQL and TransactionsMySQL is a DBMS that provides support for managing transactions. What does this mean? It means that MySQL allows you to group a certain set of actions together in such a way that either all actions are executed or none of them are. This is especially crucial in scenarios where the failure of an intermediate step might lead to database inconsistency. Consider again the example of transferring funds between two bank accounts. Let’s now say we’re working with a MySQL database that manages these transactions. Let’s go step by step: User A initiates a transaction by requesting to transfer $100 to User B. MySQL starts a transaction. MySQL debits $100 from User A’s account balance. MySQL attempts to credit $100 to User B’s account. Here we consider two scenarios: If the operation is successful, MySQL commits the transaction, and the database shows the updated balances for both users. If there’s a failure (maybe due to a server crash or an unforeseen issue), MySQL rolls back the transaction. The result? The database remains in its initial state, with no changes to User A or User B’s account balances. The database remains consistent. In this way, MySQL plays a critical role in ensuring secure and consistent transactions by adhering to the principles of ACID. Remember, MySQL’s power to handle transactions isn’t provided out of the box with every setup, but requires particular types of storage engines. We’ll explore this in more depth in the upcoming subjects. Topic 1.3 — Introduction to InnoDBNow that we understand the vital role that transactions play in maintaining database integrity and how MySQL handles transactions, it’s time to explore InnoDB. InnoDB is a storage engine for MySQL. MySQL users initially had to choose between two major types of storage engines — MyISAM and InnoDB. Each engine came with its own advantages and disadvantages. However, MyISAM, an earlier engine, did not support transactions. InnoDB, on the other hand, offers full transaction support, involving multiple statements with commit and rollback. InnoDB also provides row-level locking, which increases multi-user concurrency and performance, and features such as consistent reads, foreign key constraints, and ACID compliance, which we’ve touched on already. Since the MySQL 5.5 release, InnoDB has been the default MySQL storage engine, unless otherwise specified. This decision was made largely based on InnoDB’s robustness and full-featured design. So, InnoDB’s strong transaction support, along with its other features, makes it a powerful choice for extensive and complex database systems. Topic&nbsp;1.4 — Advantages of InnoDB.InnoDB’s main advantage is in its robust handling of transactions alongside its row-level locking capabilities. This makes it a powerful choice for extensive and complex database systems with a high number of concurrent users. Let’s break down some of these advantages: Reliability and Durability — ACID Compliance:&nbsp;Like I mentioned before, InnoDB’s transaction complies with the ACID model, thanks to its commit, rollback, and crash-recovery capabilities. It ensures that your transactions are reliable and your data is durable. Row-level locking:&nbsp;Unlike table-level locking(MyISAM uses it), row-level locking is more efficient as it allows higher concurrency and great performance in a multi-user environment. InnoDB Buffer Pool:&nbsp;The Buffer Pool is the memory space that InnoDB uses to cache data and indices of its tables. This feature reduces the I/O operations, which makes the system faster and more performant. Foreign Key Constraints:&nbsp;InnoDB supports the use of foreign key constraints for referential integrity. When the data in one table relies on data from another table, you can prevent erroneously removing critical data with foreign key constraints. Automatic Crash Recovery:&nbsp;InnoDB has the capability to recover from a crash automatically. Critical data being updated in a crash will not be lost, due to the engine’s automatic replay of its logs. These are only a few of many advantages InnoDB brings to your MySQL database. Depending on your specific needs, there may be more benefits relevant to your use case. Topic 1.5 — InnoDB vs MyISAMInnoDB and MyISAM are both storage engines for MySQL, but they have some significant differences. Understanding these differences is crucial when deciding which storage engine to use for your specific use-case. Let’s compare them across a few key parameters: Transactions:&nbsp;As we’ve discussed before, InnoDB supports transactions, whereas MyISAM does not. So, if you require transactional integrity, you should opt for InnoDB. Locking:&nbsp;InnoDB implements row-level locking while MyISAM implements table-level locking. Row-level locking allows higher concurrency and thus offers better performance for operations that require frequent, small data modifications. Foreign Key Constraints:&nbsp;Foreign key constraints, part of referential integrity, are supported in InnoDB but not in MyISAM. Full-Text Search:&nbsp;Full-text search is an operation that searches through a large collection of data and returns results that match one or more keywords. MyISAM has built-in full-text search support, which makes it a good option if this is your primary requirement. Data safety:&nbsp;InnoDB uses a transaction log to ensure data safety (ACID compliance), while MyISAM does not. Compression:&nbsp;InnoDB supports table compression, allowing your table data and associated indexes to be compressed to conserve disk space, which can also increase I/O efficiency and performance. Most importantly, remember that there’s no such thing as a universally “right” choice between InnoDB and MyISAM. The suitable engine depends on your specific situation and requirements. Topic 1.6 — Case studies: Real-world examples and scenarios where data integrity and transactions matterTransactions and data integrity are critical in many real-world applications. To illustrate their importance in practical scenarios, let’s consider a few case studies. Online Banking and Financial Services: In an online banking system, suppose a customer is transferring money from their savings account to their checking account. This process comprises two separate tasks ‒ reducing the balance of the savings account and increasing the balance of the checking account. Both tasks need to happen together. If there’s a system failure after the savings account was debited but before the checking account was credited, the customer would lose money. The ACID properties of a transaction ensure that either both actions occur or neither do, maintaining the integrity of the data. Ecommerce Platforms: Consider a customer placing an order on an eCommerce site. The process involves checking the inventory, confirming the payment, updating the inventory, and confirming the order. Any error or failure at one stage should halt the entire process. Transactions provide a secure pathway for these operations, ensuring consistent data throughout. Airlines Reservation System: When a seat on a flight is booked, the system first checks the availability of seats, reserves a seat, then accepts payment. If the reservation system crashes after a seat has been reserved but before the payment confirmation, it could lead to a loss for the airline. With transactions, a failure in a later stage would automatically roll back the previous stages, freeing up the seat for a different customer. These are just a few scenarios where transactions, aided by the robust capabilities of InnoDB, ensure data integrity. In critical systems where data consistency is paramount, the support provided by InnoDB proves incredibly useful. Topic 1.7 — MySQL Interviews: Why are database transactions important and why is InnoDB the default option in MySQL?Transactions are an essential concept in database systems. They serve to protect an organization’s information in the event of system failures such as a power outage, software crash, or something more nefarious like malicious attacks. A transaction is a sequence of one or more operations performed as a single logical unit of work. The operations can include reading database records, modifying these records, or even manipulating the data within a set of parameters. Transactions in database systems are managed with the acronym ACID, which stands for Atomicity, Consistency, Isolation, and Durability. Atomicity:&nbsp;Guarantees that either all the changes a transaction makes are committed to the database, or if an error occurs, none of the changes is committed. Consistency:&nbsp;Ensures that a transaction doesn’t leave the database in an inconsistent state after it runs. Isolation:&nbsp;Ensures that one transaction does not interfere with another. Durability:&nbsp;Ensures that committed updates persist, even in the face of power loss or a system crash. Now, why is InnoDB the default option for MySQL? InnoDB storage engine provides a robust and reliable way to handle transactions. It has several features such as the support for ACID-compliant transactions, row-level locking, and real-world implementations like foreign keys that make it the default engine for MySQL. Moreover, InnoDB offers crash recovery capabilities and provides hardware acceleration using Solid State Disks (SSDs) or hard disks. In a nutshell, the strength of InnoDB lies in its broadly applicable set of features, including transactions, reliability, and performance optimizations, which are suitable for most workloads. Topic 1.8 — Review and AssessmentsOver the course of our lessons, we’ve covered: Transactions&nbsp;and their importance in maintaining data integrity, How MySQL supports transactions and the benefits of such support, An introduction to&nbsp;InnoDB, its advantages, and why it is the default storage engine in MySQL, The differences between InnoDB and MyISAM, Real-world instances where data integrity and transactions are crucial, And a run-through of common MySQL interview questions and responses. Just to recap, transactions are a critical concept in database systems. They exist to protect the integrity of data in the event of system failures. InnoDB is the default storage engine for MySQL due to its support for ACID-compliant transactions, its feature set that provides reliability and performance optimization, and its real-word implementations that further enforce data integrity. These features provide InnoDB an advantage over MyISAM, especially in the context of applications where data integrity and reliability are paramount. For this assessment, I will provide a set of questions and problems related to the topics that we covered. They will range from simple to complex, so make sure to think critically and take your time. Question 1 (3/10 difficulty): What are the four properties of a transaction known as ACID? Question 2 (6/10 difficulty): Explain the importance of transactions in maintaining data integrity within a database. Question 3 (9/10 difficulty): Discuss why InnoDB is the default option in MySQL compared to MyISAM. Include the benefits of InnoDB in your answer. 中文文章: https://programmerscareer.com/zh-cn/mysql-interview14/Author: Wesley Wei – Twitter Wesley Wei – MediumNote: If you choose to repost or use this article, please cite the original source.","link":"/mysql-interview14/"},{"title":"MySQL interviews: What are the transaction isolation levels? What are the tradeoffs?","text":"Have you ever been asked a similar question in an interview? Or you will meet in the future, let’s explore and master it together Thank you for reading this article. More Interview Questions here:https://programmerscareer.com/software-interview-set/ Topic 1.1: Introduction to Transaction Isolation LevelsAlright, let’s embark on our journey of understanding Transaction Isolation Levels in MySQL! So, let’s start with the basics. In any database system that allows concurrent transactions, Transaction Isolation Levels determine the extent to which one transaction must be isolated from another. They are vital as they help manage concurrent transactions, prevent data inconsistencies, and ensure the integrity of data in any transaction-based system. The ANSI/ISO SQL standard defines four levels of transaction isolation with respective phenomena prevention: Read Uncommitted: This is the lowest level of isolation, in which one transaction might see not-yet-committed changes made by another transaction. Read Committed: It guarantees that any data read is committed at the moment it is read. It doesn’t prevent other transactions from modifying the data. Repeatable Read: This level ensures that if a transaction reads data that is then modified by another transaction, the first transaction will retrieve the same data regardless of any subsequent reads. Serializable- This level offers the highest data protection, it achieves this by performing transactions serially, or one after another. But this might lead to performance issues. It’s also important to know that MySQL, with InnoDB storage engine, only supports&nbsp;Repeatable Read (the default isolation level), Read Committed, and Serializable. Each isolation level comes with its pros and cons, solving some problems while potentially introducing others. This is a necessary trade-off to balance performance and accuracy in managing concurrent transactions. Topic 1.2: Read Uncommitted LevelIn “Read Uncommitted,” the transaction may read data that’s still in a state of being modified by another running transaction and not yet committed to the database. That’s where this isolation level inherits its name — the data it reads is not committed yet. Consider the scenario: Let’s imagine a transaction is modifying some rows of a table: 123UPDATE Inventory SET Quantity = Quantity - 10 WHERE ItemName = 'Apples'; While this transaction is still ongoing, another transaction reads data from the same table. According to the Read Uncommitted level, it will see the uncommitted changes containing mutated data that the first transaction was processing, leading to what is called a “Dirty Read.” There might be an instance where the first transaction fails (and a rollback is initiated), making those changes null and void. But the second transaction that already read the uncommitted data proceeds with the flawed, inaccurate data. This can cause serious data inconsistencies. In terms of performance, however, “Read Uncommitted” is typically faster than the other, higher isolation levels because it doesn’t have to issue locks on the read data to prevent it from being modified or read. Now the downside of “Read Uncommitted” comes into view. It cannot guarantee accuracy and consistency of data because, as we said earlier, it allows “Dirty Reads.” In real-world applications, this isolation level is generally avoided unless performance is the most important factor and data accuracy is not a major concern. Topic 1.3: Read Committed LevelAs the name suggests, the “Read Committed” level allows a transaction to see only those changes that have been committed by other transactions before it begins to read. As a result, it solves the “Dirty Read” problem we talked about in the “Read Uncommitted” isolation level. Let’s illustrate this with a simple example: Consider two accounts, ‘A’ and ‘B’, with current balances of $500 and $200, respectively. Suppose a transaction is initiated to transfer $100 from account ‘A’ to account ‘B’. During this process, account ‘A’’s balance reduces to $400 even before the transaction is completed. In the “Read Uncommitted” isolation level, if another transaction tries to calculate the total balance of both accounts simultaneously, it might end up adding the intermediate state of account ‘A’ (i.e., $400) and the original state of account ‘B’ (i.e., $200), leading to an incorrect total balance of $600. However, with the “Read Committed” isolation level, the second transaction waits until the first transaction is completely finished. Therefore, it correctly calculates the total balance as $700 ($400 in account ‘A’ + $300 in account ‘B’). So, under the “Read Committed” isolation level, one transaction won’t see uncommitted changes of other transactions, which is a great step towards maintaining data consistency. However, now we have another problem, called “Non-Repeatable Read.” This occurs when, during the lifespan of a single transaction, it tries to read the same row twice but gets different data each time. This scenario is possible if, between the first and second read, another transaction modifies that row and commits the change. Topic 1.4: Repeatable Read LevelIn a “Repeatable Read” isolation level, not only are the changes made by other transactions invisible until they’re committed (as we saw in “Read Committed”) but also, once a transaction reads some data, that data cannot change for the duration of that transaction. In other words, the same SELECT query, when run multiple times within the same transaction, will return the exact same result, regardless of any other concurrent transactions. This constraint solves the “Non-repeatable Read” issue. Let’s take an example: Consider a situation where a transaction reads some rows from a table. Then, an independent transaction modifies some of those rows and commits the change. If the first transaction tries to read the same rows again, according to the “Read Committed” isolation level, it notices these changes. But, in the “Repeatable Read” isolation level, the first transaction is unaware of any changes committed by the second transaction during its lifetime. Therefore, reading the same rows yields the same result. Although it solves the “Dirty Read” and “Non-repeatable Read” problems, it’s prone to a different problem: the “Phantom Read” issue, which we will discuss in our next section. Topic 1.5: Serializable LevelThe “Serializable” level is the most stringent of all, providing the highest data consistency. It handles not only “Dirty Reads” and “Non-Repeatable Reads,” like the “Repeatable Read” level but also takes care of the “Phantom Reads” issue. First, let’s understand what “Phantom Reads” are. It’s a phenomenon that occurs when, in the middle of a transaction, new rows are added or existing ones are removed by another transaction. It’s named as such because these records appear or disappear as if they were “phantoms.” For example, consider a transaction that reads some rows from a table. An independent transaction, in the meantime, adds some new rows to that table and commits the change. If the first transaction reads the same table again, it sees new rows, which are like “phantoms.” With the “Serializable” isolation level, such situations are impossible. When a transaction is run at this level, it behaves as if no other transactions even exist, eliminating any concurrency-related issues. However, there’s a cost for such precision. The “Serializable” isolation level sharply reduces concurrency because it locks the datasets it reads. Therefore, it can lead to significant performance degradation for large databases. In a nutshell, the “Serializable” isolation level ensures absolute data integrity at the expense of performance. Having discussed each specific isolation level, it’s crucial to note that the level you choose ultimately depends on the nature of your application. It’s always about balancing between performance and data integrity. Topic 1.6: MySQL Transaction Isolation Levels Explanation.As we have discussed earlier, the four available transaction isolation levels that can be used in MySQL are Read Uncommitted, Read Committed, Repeatable Read, and Serializable. Each of these levels offers a different balance of data consistency, concurrency, and performance. Yet, the question remains: how does MySQL implement these levels internally? Well, MySQL mainly uses locking to ensure data consistency and isolation between concurrent transactions. It employs different types of locks such as shared and exclusive locks, depending on the requirements of the individual transaction and the isolation level set. Without going too much into the nitty-gritty details, let’s understand what these locks are: Shared Locks (S Locks): Shared locks are held when a transaction merely reads a record and doesn’t modify it. More than one transaction can hold a shared lock for the same record at the same time. Exclusive Locks (X Locks): Exclusive locks are held when a transaction modifies a record. Only one transaction can hold an exclusive lock to a record at a given time. These locks apply to the&nbsp;read data&nbsp;in order to maintain isolation and prevent data inconsistencies. For example: In the&nbsp;Read Uncommitted&nbsp;level, no locks are held that prevent other transactions from writing to the record. In the&nbsp;Read Committed&nbsp;level, shared locks are placed but released as soon as the row has been read. In the&nbsp;Repeatable Read&nbsp;level, shared locks are placed and retained until the transaction is completed. In the&nbsp;Serializable&nbsp;level, shared locks are placed, and no other transaction can modify or insert new records until the transaction is finished. So, depending on which isolation level is being used, the MySQL engine will acquire and release these locks differently to achieve the desired level of isolation at the expense of concurrency and vice versa. That said, this mechanism is just the tip of the iceberg. The actual implementation is much more complex and involves many other factors such as lock escalation, deadlock detection, log buffering, and more. Topic 1.7: Case StudiesIn this section, we’ll look at the practical uses of different Transaction Isolation Levels, tying all of our learning together wit real-world scenarios. The most suitable isolation level primarily depends on the specific read/write workload and the business requirements of each application. In real-world settings, we need to strike a balance between concurrency (the ability to allow multiple users to access the database simultaneously) and isolation (the degree to which each individual transaction is isolated from others). Let’s look at a few scenarios: Scenario 1: Banking System For a banking system that is dealing with transactional data such as bank transfers, it would be catastrophic if dirty or non-repeatable reads occurred. Imagine if you withdrew money from an ATM, but due to a concurrent transaction, the system failed to immediately register the deduction. You could potentially withdraw more money than you have — a lovely scenario for us, but not for the banks! So for such systems, a high level of isolation like&nbsp;SERIALIZABLE&nbsp;or&nbsp;REPEATABLE READ&nbsp;is often used, despite the potential impact on performance. Scenario 2: E-commerce Application For an e-commerce application, allowing dirty reads could result in selling more products than available. However, if we are strict on isolation level, it could slow down the application and affect the user experience. An isolation level like&nbsp;READ COMMITTED&nbsp;is frequently used here, trading off strict isolation for increased concurrency. Scenario 3: Data Analysis In data analysis or reporting scenarios where we are reading large volumes of data but not modifying it, a lower isolation level like&nbsp;READ UNCOMMITTED&nbsp;can often be used. This reduces the overhead of locks and allows for higher throughput. Keep in mind that there’s no one-size-fits-all answer, it always depends on the specific requirements and circumstances of the system being built. Topic 1.8: Interview-readyQuestion:&nbsp;Explain Transaction Isolation Levels. Answer:&nbsp;Transaction Isolation Levels control the degree of locking that occurs when selecting data from a database. The type of locks placed on data items affects the database’s concurrency level and consistency, which is vital in transaction processing. There are four standard transaction isolation levels defined in the SQL standard: Read Uncommitted, Read Committed, Repeatable Read, and Serializable. Question:&nbsp;What are the pros and cons of each Transaction Isolation Level? Answer: Read Uncommitted:&nbsp;Transactions may read changes made by others that have not yet been committed, leading to dirty reads and other inconsistencies. The benefit is less need for locks, leading to better performance. Read Committed:&nbsp;This level allows transactions to see only committed changes from other transactions. This prevents dirty reads but can still lead to non-repeatable reads or phantom reads. It usually offers a good balance between consistency and performance. Repeatable Read:&nbsp;Guarantees that any data read cannot change, avoiding dirty and non-repeatable reads, but can still result in phantom reads. Serializable:&nbsp;The highest level of isolation. Guarantees that transactions occur in a completely isolated manner. Avoids dirty reads, non-repeatable reads, and phantom reads but can lead to performance degradation due to extensive locking. Question:&nbsp;When might you use each Isolation Level? Answer: Read Uncommitted:&nbsp;Data analysis tasks where seeing uncommitted changes is permissible and performance is critical. Read Committed:&nbsp;Applications where maintaining a high degree of concurrency is more important than the possibility of occasional inconsistencies, such as some low-impact e-commerce applications. Repeatable Read:&nbsp;Scenarios where it’s vital to maintain a consistent picture of data across multiple reads, such as certain financial applications. Serializable:&nbsp;Only used when strictly necessary due to performance implications, such as applications that manage highly sensitive data. These are just a few examples of potential interview questions. Remember, a deep understanding of these concepts will allow you to adapt to whatever specific questions your interviewer may ask. Topic&nbsp;1.9: Review and AssessmentsUnderstanding transaction isolation levels is fundamental when you’re dealing with database transactions. So far, we’ve explored different transaction isolation levels, their uses, advantages, and potential pitfalls. To solidify your understanding, let’s go through a quick review with some questions: 1. Explain the four Transaction Isolation Levels as per the SQL standard. _Your Answer: ______ 2. Describe a scenario in which you would use the REPEATABLE READ isolation level. _Your Answer: ______ 3. ‘A dirty read occurs when a transaction reads the uncommitted data of another transaction.’ True or False? _Your Answer: ______ 4. Which transaction isolation level has the strictest locking, leading to the most substantial effects on transaction performance? Why? _Your Answer: ______ 5.&nbsp;==‘Phantom reads can occur at the Read Committed level.’ True or False?== _Your Answer: ______ Take your time to answer these questions, reflect on your responses, and compare them with what we’ve covered in the lessons. When you’re ready, we can discuss your answers together. If you have any uncertainties or need a deeper explanation of any topics, don’t hesitate to ask. Let’s ensure you’ve mastered this concept! 中文文章: https://programmerscareer.com/zh-cn/mysql-interview15/Author: Wesley Wei – Twitter Wesley Wei – MediumNote: If you choose to repost or use this article, please cite the original source.","link":"/mysql-interview15/"},{"title":"MySQL Interview Question: When to split databases and when to split tables?","text":"Let’s outline a curriculum for splitting databases and tables in MySQL Thank you for reading this article. More Interview Questions here:https://programmerscareer.com/software-interview-set/ Topic: Understanding Data DistributionHi, we’ll be starting our understanding of data distribution in this lesson. First and foremost, let’s get clarified about what we mean by ‘data distribution’. Data distribution is the method of allocating data, computational tasks, and applications across multiple computers in a network. Distributing the data across multiple servers can improve the performance, reliability, and accessibility of applications. 😊 There are various reasons behind data distribution. Let me take you through some of the key advantages of it: Improved Performance:&nbsp;By distributing your data, you can improve the speed at which your application accesses this data. If done efficiently, data distribution can ensure that the resources are located close to where they are needed most, thereby reducing access latency and improving speed. Redundancy and Reliability:&nbsp;Through data distribution, multiple copies of the same data can be stored in different locations. This provides redundancy and increases the overall reliability of your data. If one server fails, your application can access the same data from a different server. Scalability:&nbsp;As your business or application grows, data distribution can make it easier to scale up your infrastructure. New servers can be added to the network as and when required. Load Balancing:&nbsp;Properly distributing data can help maintain a balanced load across servers. This ensures no single server becomes a bottleneck, affecting the performance of your applications. There are many ways to distribute data, such as horizontal partitioning (also known as sharding), vertical partitioning, and functional partitioning. Each method has its own pros and cons and is suitable for different types of applications. We will be exploring these in depth in the upcoming lessons. Topic: Database Splitting (Sharding) in MySQLI’m glad to see you’re eager to learn more! Now that we’ve explored the concept of data distribution, let’s delve into the fascinating process of database splitting, more commonly known as&nbsp;Sharding. Sharding in MySQL is a process in which we break down a larger database into smaller, more manageable parts, called shards. Each shard holds a portion of the total data and functions as a separate database. To illustrate, visualize a big book split into separate chapters, where each chapter can stand on its own and store unique information. Similarly, when we shard a database, it’s like splitting a huge database into ‘chapters,’ with each one housing unique data. Now, why would we do that? 🤔 Sharding is generally implemented for a few reasons: Improved Performance:&nbsp;Sharding can greatly enhance read/write speeds. When you fire off a query, instead of sifting through a massive database, it only has to search a specific shard, drastically improving speed. Scalability:&nbsp;Sharding makes your database horizontally scalable. If your app grows and the database starts getting too large for a single server, you can always add more shards. Reliability:&nbsp;If one shard goes down, it won’t bring your entire application down with it. The rest of the shards will continue to work without any hitches. While sharding has its manifold benefits, it also comes with some cons: Increased Complexity:&nbsp;The overall architecture of your database environment becomes more complicated. Data Distribution:&nbsp;You have to decide how to distribute your data, which can be challenging. Joining Data across Shards:&nbsp;If you want to join tables that reside on different shards, it might be complicated or slow. In MySQL, sharding is typically done at the application level using sharding libraries or frameworks. Some of the popular sharding algorithms used are Range-Based, List-Based, Hash-Based, and Directory-Based sharding. Topic: Table Splitting (Partitioning) in MySQLIf you recall, earlier we discussed data distribution and why you might want to split database into multiple, smaller parts, a process known as sharding. In a similar sentiment, table partitioning is a way to divide a large table into smaller, more manageable parts, while still being able to access and manipulate the data as if the partitioned table were a single entity. In other words, even though the data is stored in separate partitions, from a user perspective, it’s as if there’s only one table. Neat, right? Now,&nbsp;when would this come handy?&nbsp;😊 Partitioning can be the way to go if you have a massive table that results in slow queries and updates, especially if that table is expected to grow beyond the capacity of a single server. In its essence, partitioning is about managing and optimizing data for improved performance. Employing partitioning when it’s uncalled for may lead unwanted complexity. MySQL provides four different types of partitioning: 1. RANGE Partitioning:&nbsp;This is the simplest type of partitioning. It works best when dealing with data that falls into certain ranges — like dates or quantities. 2. LIST Partitioning:&nbsp;It partitions table based on the predefined list of values. 3. HASH Partitioning:&nbsp;Here, MySQL applies a hash function on the partitioning key to ascertain the partition to be used. It’s used when you want to ensure data is spread evenly across a specified number of partitions. 4. KEY Partitioning:&nbsp;It’s similar to hash partitioning, but in this case, MySQL server takes care of calculating the hash value in an internal way, which user can’t control. So, that’s the theory, in the next lessons we’ll talk about how you can implement these principles in MySQL and I’ll provide some real-life examples to help illustrate this concept. Topic: Practical Examples in Database and Table SplittingGreat! Now that we’ve discussed the theory of database sharding and table partitioning, let’s put it into practice with some hands-on examples. Example 1: Database ShardingSuppose we have an e-commerce website with a global user base and the customer data is expanding rapidly. We can create a shard for each geographical region: North America, Europe, Asia, etc. Each region’s database shard would store only the data associated with the customers located in that region. This way, we’re improving performance by localizing data and queries. Example 2: Table PartitioningIn the case of our e-commerce site, consider a situation where we have millions of orders, and most queries are for recent orders. We can choose to partition the Orders table by date, keeping data for each month in a separate partition. With this, queries for specific periods would only need to scan fewer rows and perform much faster. But how these are implemented practically? For database sharding in MySQL, there are various sharding frameworks available like Vitess, phpMyAdmin, etc. For instance, in Vitess, you would define a VSchema which includes sharding instructions to auto-shard the desired table. 123456789101112131415161718{ \"sharded\": true, \"vindexes\": { \"hash\": { \"type\": \"hash\" } }, \"tables\": { \"your_table\": { \"column_vindexes\": [ { \"column\": \"your_shard_column\", \"name\": \"hash\" } ] } } } As for partitioning a table in MySQL, it can be done by ALTER TABLE command. 123456789ALTER TABLE orders PARTITION BY RANGE( YEAR(order_date) ) ( PARTITION p0 VALUES LESS THAN (1990), PARTITION p1 VALUES LESS THAN (2000), PARTITION p2 VALUES LESS THAN (2010), PARTITION p3 VALUES LESS THAN (2020), PARTITION p4 VALUES LESS THAN MAXVALUE ); In this script, we partition the ‘orders’ table based on the ‘order_date’ column. Remember, these processes are normally handled by your application or a framework, thus some knowledge of SQL is required. Also, both sharding and partitioning come with their complexities and should only be used when the benefits outweigh these complexities. Topic: Making an Informed DecisionWe’ve covered a lot of ground at this point, and last but not least, we’ll discuss how you can make an educated decision on when to split databases or tables, reiterating some key points from previous lessons as well as a few additional tips. Here are some factors to consider: Database Sharding (Splitting) Decision Making: Data Size: If your database is becoming too large to handle efficiently, it might be time to consider sharding. Performance: If frequent queries are significantly slowing down due to the large size of the database, sharding can help improve the processing speed by reducing the amount of data each query needs to process. Scalability: If you foresee your database growing beyond the capacity of a single server, implementing sharding from an early phase can be a good preventative measure. Type of Data: Sharding can also be driven by the nature of data. For example, multi-tenant applications where data from many customers is stored in the same database is a perfect candidate for sharding. Table Partitioning Decision Making: Table Size: Just like with database sharding, if a table in your database is growing endlessly, you might want to consider partitioning it. Query Performance: If the majority of the queries against a table only deal with a segment of the data (e.g., the most recent entries), partitioning can speed up these queries significantly. Maintenance: Partitioning also makes it easier to perform maintenance on a table (like backups, updates, and deletes) as these operations can be performed on individual partitions instead of the tabling the entire table offline. In essence, the decision to partition or shard should be made based on the need to improve performance, handle large amounts of data, or both. That said, it’s not a decision to be taken lightly as it adds complexity to your database structure and application logic. It should only be implemented when necessary and other simpler optimization techniques are no longer sufficient. Topic: Review and AssessmentsAt this point, we have completed our journey through data distribution, specifically focusing on Database Splitting (Sharding) and Table Splitting (Partitioning) in MySQL. Now, let’s take a quick look back at the key points and then move on to an assessment to consolidate your learning. Key Points Understanding Data Distribution: Data distribution has significant performance benefits but can also increase complexity. Knowing when and how to use it is crucial. Database Splitting (Sharding) in MySQL: Sharding in MySQL involves splitting a database into smaller parts based on a key. It can significantly improve query response time, increase reliability, and facilitate horizontal scalability. Table Splitting (Partitioning) in MySQL: Partitioning in MySQL involves breaking a table into smaller pieces without having to change SQL queries. The partitioning can be done based on various strategies like ranges, list values, hash values, etc. Practical Examples: We discussed how database sharding might be implemented for an e-commerce site with a global customer base, and how table partitioning can be used to improve performance for frequently accessed recent data. Making an Informed Decision: Deciding when to implement database sharding or table partitioning should consider data size, query performance, scalability, and type of data. Let’s now move to the assessment. You’ll be presented with a couple of scenarios, and your task is to decide whether to use sharding, partitioning, both, or none, and why. Assessment Scenarios Scenario 1: You’re designing an application for a hospital, where you have a&nbsp;patients&nbsp;table storing patient records. The hospital sees thousands of patients each year, and on average, a patient visits once a year. Most queries involve accessing only recent patient records. Would you implement sharding, partitioning, both or none, and why? Scenario 2: You’re developing an application for a tech news website where articles are often updated for corrections, and new information and user comments are constantly being written. The comments are stored in a&nbsp;comments&nbsp;table, and each comment is associated with an article. Would you implement sharding, partitioning, both or none, and why? Scenario 3: You’re working on an e-commerce site that handles transactions from all over the world. The database includes a&nbsp;transactions&nbsp;table, holding details of every transaction ever made on the site. Should you implement sharding, partitioning, both, or none, and why? Scenario 1: Given the scenario, it makes sense to implement&nbsp;table partitioning&nbsp;on the&nbsp;patients&nbsp;table. The queries involve accessing only recent patient records, and partitioning would allow for efficient query performance. Sharding might not be necessary as we do not have a clear shard key, and managing distributed transactions and maintaining consistency among shards might add unnecessary complexity. Scenario 2: In this scenario, a proper solution may be to partition the&nbsp;comments&nbsp;table. A date-based partitioning system might work well here, as comments related to older articles are probably read frequently, but not updated that much. Sharding might be overkill for this situation as it could add unnecessary complexity. Scenario 3: Sharding the&nbsp;transactions&nbsp;table would be beneficial here, especially on something like the location of the transaction. This would enable transactions from the same region to be grouped together, which could lead to more efficient querying. Furthermore, sharding can also help balance the load across multiple databases which provides additional benefits in terms of scalability and performance. In addition to sharding, you might also consider partitioning of individual shards. These are the preferred solutions based on the information provided. However, every application can have unique requirements and constraints, and it’s crucial to consider all factors when making design decisions. Remember, measure before you optimize! 中文文章: https://programmerscareer.com/zh-cn/mysql-interview16/Author: Wesley Wei – Twitter Wesley Wei – MediumNote: If you choose to repost or use this article, please cite the original source.","link":"/mysql-interview16/"},{"title":"My SQL Interview Question: The implementation principle of MySQL MVCC","text":"we can dive into the details of how MySQL implements Multi-Version Concurrency Control (MVCC) Thank you for reading this article. More Interview Questions here:https://programmerscareer.com/software-interview-set/ Topic: Unpacking MVCCMultiVersion Concurrency Control (MVCC) is a technique used in database management systems to avoid conflicts between users accessing the same data simultaneously, thereby increasing processing efficiency. In essence, MVCC allows multiple transactions to access the same data without conflicts or the need for read locks, which can slow down performance significantly. This is achieved by creating a “snapshot” of the data at the point a transaction begins. Other transactions happening concurrently won’t affect this snapshot. Now, let’s break down the term MVCC: MultiVersion: This suggests that the database maintains several versions of the same record. The “version” is a snapshot of the data at a certain point in time. Concurrency Control: This implies that the technique is used to handle simultaneous transactions without conflict, ensuring that each transaction maintains the illusion that it’s the only one accessing the database. MySQL implements MVCC in its InnoDB storage engine. When a transaction updates a row in InnoDB, the original row is not immediately overwritten or deleted. Instead, InnoDB stores the old version of the row internally for any other active transactions that need to see the original version. This is how “versions” of data rows come into existence. So, why do we use MVCC in a system like MySQL? Primarily, it’s for performance reasons. By allowing multiple transactions to access the same snapshot of data without conflict, we avoid the need for read locks that can bottleneck the system. In a system where many users might be querying and updating the same database, this can significantly speed up processing times. Topic: How MVCC works in MySQLLet’s now zoom into the internals of how MVCC operates within MySQL. When a transaction starts in MySQL (InnoDB), it gets assigned a unique transaction ID. This ID is used to create its view of the database. This view consists of all the committed data up until the time the transaction started, and any changes made by the transaction itself. The transaction can’t see any data modified by other simultaneous transactions, providing a consistent snapshot and ensuring isolation. When a row is modified within a transaction, InnoDB won’t overwrite the existing data. Instead, it writes the new row version and maintains the old version in an area called an undo log. This log contains information needed to revert changes if a transaction is rolled back, and it provides older versions of a row to other transactions that might need them. Now let’s talk a bit about some related topics: Read Views, Undo Logs, and Purging. Read view&nbsp;is the mechanism InnoDB uses to implement consistent reads, i.e., reading the snapshot of the database corresponding to the point in time when the transaction started. Undo logs&nbsp;are a crucial part of MVCC. As mentioned earlier, when a transaction modifies data, InnoDB writes the new row to the database and stores information necessary to reconstruct the older version of the row in an undo log record. If another transaction needs to see the older version of the row, InnoDB uses the undo log record to reconstruct it. Purging&nbsp;relates to how InnoDB cleans up old versions of a row that are no longer needed by any ongoing transactions. Once all transactions that might need to access an older row version have completed, InnoDB can free space held by this version. This process is referred to as purging. Topic: ACID Properties and MVCCOne of the crucial aspects of any reliable database management system is ensuring that it maintains certain properties defined by the ACID principle, which stands for Atomicity, Consistency, Isolation, and Durability. Atomicity: If a transaction involves multiple operations, atomicity means that either all the operations are executed successfully, or none of them are. There’s no in-between — a transaction can’t be partially complete. If an error happens during any operation in a transaction, the whole transaction is rolled back. Consistency: Consistency means that a transaction should bring the database from one consistent state to another, according to predefined rules. For example, if an account doesn’t have sufficient balance for a withdrawal, the transaction should be rejected to maintain consistency. Isolation: Isolation comes into play when multiple transactions are being executed simultaneously. It means each transaction should behave as if it’s the only one being executed. The intermediate state of a transaction should not be visible to other transactions. Durability: Durability ensures that once a transaction has been committed, it will remain, even in the event of power loss, crashes, or other system errors. In other words, the results of a transaction are permanent. Now, how does MVCC (MultiVersion Concurrency Control) relate to the ACID properties? Here’s the connection: In the context of MySQL (and more specifically its InnoDB storage engine), MVCC provides isolation and consistency. Isolation&nbsp;is ensured as each transaction works with its snapshot of the data, isolated from the changes made by others. Even if multiple transactions are trying to read and write the same data simultaneously, each will see its consistent snapshot, as if it’s the only transaction happening. Consistency&nbsp;is maintained thanks to the use of undo logs in MVCC. If a transaction fails or is rolled back, the changes made within that transaction can be undone to ensure the database remains in a consistent state. Furthermore, by creating a transaction-specific view of the data, MVCC ensures that the transaction always works with a consistent set of data. Topic: Snapshot Read and Current ReadIn MySQL, there are two main types of reads that are utilized when MVCC (MultiVersion Concurrency Control) comes into play: snapshot read and current read. Let’s dive into these concepts. Snapshot Read A snapshot read, as the name suggests, provides a consistent snapshot of the data as it was when the transaction started. It doesn’t see changes made by other concurrently executing transactions. This read is the default mode for&nbsp;SELECT&nbsp;statements when&nbsp;not&nbsp;in&nbsp;LOCK IN SHARE MODE&nbsp;or&nbsp;FOR UPDATE&nbsp;modes. Snapshot read is crucial in providing the “consistent view” of data which is integral to the concept of MVCC. Current Read Unlike a snapshot read, a current read sees the latest committed data, including changes made by other transactions. Modes like&nbsp;SELECT…FOR UPDATE&nbsp;and&nbsp;SELECT…LOCK IN SHARE MODE&nbsp;use current reads. It’s also used when a query modifies data (like&nbsp;UPDATE,&nbsp;INSERT,&nbsp;DELETE). These two types of reads offer flexible ways of handling data in transactions. The use of snapshot read or current read depends on whether you want a transaction to see only the data as it was when the transaction began, or if it needs to see the latest data, including modifications made by other transactions. Topic: Managing Deadlocks with MVCCNow that we’ve explored the snapshot and current reads in MySQL’s MVCC, let’s understand another critical aspect of transaction handling — dealing with deadlocks. A deadlock happens when two or more transactions mutually hold and request for locks, creating a cyclic dependency that can’t be resolved. Without intervention, these transactions could wait indefinitely, which is obviously not ideal. MySQL handles deadlocks in MVCC by utilizing a wait-for graph. In layman’s terms, whenever transaction A waits for transaction B to release a row lock, an edge is added from A to B in the wait-for graph. If adding this edge creates a cycle, a deadlock is detected. Upon detecting a deadlock, MySQL needs to resolve it. It does so by choosing a transaction as the ‘victim’ and rolling it back. In most cases, it chooses the one that has done the least amount of work, so less work is lost. After rolling back the victim transaction, the deadlock is resolved. In MySQL, you can use&nbsp;SHOW ENGINE INNODB STATUS;&nbsp;to get information about the most recent deadlock, which can aid in debugging. Deadlock management, while mostly automatic, demands caution on the design and execution of transactions. It’s advisable to keep transactions as short as possible and commit them as soon as possible to minimize the chances of deadlocks. Topic: Performance Implications of MVCCDespite all the benefits that MultiVersion Concurrency Control underpins in MySQL, it’s important to recognize that MVCC isn’t without its performance trade-offs. Let’s delve into some of these: Disk Space: One of the main overheads of MVCC is increased disk space. Because MVCC keeps different versions of a row to provide isolated, consistent views to transactions, more disk space is required. This could be significant in heavy read-write mixed workloads. CPU and I/O Resources: The process of producing multiple versions of data, maintaining them, and removing unnecessary versions (purging) can put a burden on CPU and I/O resources. Locking Overhead: While MVCC reduces the need for locking, it does not eliminate it entirely, especially for write transactions (Inserts, Updates, Deletes). These locks add to the performance overhead. Increased Complexity: MVCC adds complexity to the database engine. It needs to manage multiple versions of data, handle undos, resolve conflicts, and clean up old versions. This complexity adds overhead to the overall performance. When might you consider alternative methods to MVCC? While MVCC offers excellent benefits regarding multi-user access, it might not be the perfect choice in specific scenarios. For instance, in cases where data is largely written once and read many times, like a logging application, it might be better to consider a storage engine like MyISAM which does not support MVCC. Additionally, in applications where you want the absolute latest data, you can also look into other approaches since MVCC provides a “snapshot” of the data, not the latest version. It’s necessary to understand the workload, performance expectations, and hardware resources available before making a decision regarding the use of MVCC. Topic: Review and AssessmentsNow that we’ve explored different aspects of MultiVersion Concurrency Control (MVCC) in MySQL, it’s time to recap some of the key takeaways: MVCC is a method used in database management systems to handle concurrent transactions. MVCC provides each transaction a ‘snapshot’ of the database, allowing multiple transactions to read (and write to) a single data item simultaneously, which dramatically improves the database’s performance and scalability. Snapshot and current read are two critical concepts in MySQL’s MVCC. Snapshot read gives a consistent view of the data as of the transaction start, and current read considers the latest committed data. The ACID properties in databases are closely related to MVCC, ensuring Atomicity, Consistency, Isolation, and Durability of transactions. MVCC helps in managing deadlocks in MySQL, which are situations where two transactions wait for each other to release resources. While MVCC offers several advantages, it’s not without some performance trade-offs, such as increased disk space and CPU usage. These are some of the many aspects we’ve covered about MVCC in MySQL. With that, it’s now time to assess your understanding of the topic. Consider the following questions: How does MVCC improve concurrent transactions in MySQL? What’s the difference between snapshot read and current read in MySQL’s MVCC? How does MVCC in MySQL manage deadlocks? Can you explain the performance implications that come with using MVCC? Reflect on these questions. You can write down your responses or simply think through them for self-evaluation. MVCC improves concurrent transactions by allowing multiple users to access the same row of a table at the same time, instead of locking the data entirely. Each transaction gets a “snapshot” of the data at the point the transaction began, ensuring a consistent state of the data is used throughout the transaction. Snapshot read and current read in MySQL’s MVCC are two different isolation levels for reading data. A snapshot read refers to the consistent read of the database state at the time the transaction starts, ensuring data consistency for that transaction. A current read, on the other hand, is a type of read where the most recent committed data (including those made by other transactions) is considered. MVCC manages deadlocks through a wait-for graph. When a transaction is waiting for a resource locked by another transaction, an edge is added to the wait-for graph. If adding this edge creates a cycle, a deadlock is detected. MySQL then chooses a transaction to be the ‘victim’ and rolls it back to resolve the deadlock. MVCC is associated with some performance trade-offs. It requires additional disk space to keep multiple versions of each row in a table. Additional CPU and I/O resources are used for managing these versions, handling undo operations, and cleaning up old row versions. The complexity of MVCC handling can also add to the overhead in the overall performance. 中文文章: https://programmerscareer.com/zh-cn/mysql-interview17/Author: Wesley Wei – Twitter Wesley Wei – MediumNote: If you choose to repost or use this article, please cite the original source.","link":"/mysql-interview17/"},{"title":"MySQL Interview Question: What are the common storage engines for MySQL? What’s the difference?","text":"Here is a detailed curriculum of your learning plan for MySQL’s storage engines Thank you for reading this article. More Interview Questions here:https://programmerscareer.com/software-interview-set/ Topic: 1.1 Introduction to MySQL Storage EnginesIn MySQL, a storage engine is the underlying software component that a database management system uses to create, read, update and delete (CRUD) the data. Simply put, it’s responsible for the management of information within a database. You can think of it being analogous to how a filesystem manages files on a disk. Each table in a MySQL database is created with a particular storage engine. MySQL provides several storage engines such as InnoDB, MyISAM, MEMORY, and others as well that allow us to choose a one that is best fit for our requirements. When interacting with a database, we largely don’t have to be concerned about storage engines — we can just focus on writing SQL queries. But the choice of a storage engine has an impact on various characteristics of how a database functions such as: Transaction support:&nbsp;Transaction allows several modifications in a database to be processed as one unit of work, either all data modifications made within a transaction are committed to the database, or none of them are. InnoDB supports transaction, MyISAM does not. Locking levels:&nbsp;Locking prevents multiple processes from interfering with each other’s activities. Different storage engines employ different locking mechanisms ranging from row-level to table-level locking. Data durability and crash recovery:&nbsp;This is the ability of a database to recover from a crash or power loss. InnoDB has strong data durability and crash recovery capabilities. A question you might be asking:&nbsp;Can I use multiple storage engines in a single database?&nbsp;Yes! In fact, each table can use a different storage engine. Topic: 1.2 Understanding the InnoDB EngineInnoDB is the default storage engine for MySQL. It provides the standard ACID-compliant transaction features, along with row-level locking and foreign key relationships. These are a few of the reasons why it’s heavily favored in scenarios where data integrity and performance are crucial. Now let’s break down and understand these features: ACID Compliance:&nbsp;The ACID properties (Atomicity, Consistency, Isolation, Durability) are the key transaction processing concepts. They maintain data integrity over multiple transactions, thereby ensuring that your data remains consistent and reliable throughout and after all operations. Row-level locking:&nbsp;As opposed to table-level locking (as in MyISAM), InnoDB employs row-level locking where each row modified in the process of a transaction locks that specific row and allows other transactions to modify other rows. Foreign Key Relationships: Foreign keys enforce referential integrity among tables in a database. In other words, it helps prevent actions that would destroy links between tables. InnoDB also has crash recovery capabilities. This means that InnoDB can auto-correct any inconsistencies that occur as a result of premature shutdown or major failure by replaying its logs. In terms of performance, InnoDB uses a method known as Multiversion Concurrency Control (MVCC) to get past the need for read locks when executing SELECT statements. This is a significant benefit if you have a busy site where SELECT statements are common and data integrity is crucial. Topic: 1.3 Understanding the MyISAM EngineThe MyISAM engine is one of the earliest storage engines in MySQL, and before MySQL version 5.5, MyISAM was the default storage engine. There are some distinguishing features and uses of MyISAM that make it efficient in specific scenarios. MyISAM does full table-level locking for INSERT, UPDATE, and DELETE operations. What does this really mean? Well, when a row is being written or updated, the entire table which the row is part of, is locked, and no other operations can write to the same table until the write or update process is completed. One might see this as a disadvantage compared to the row-level locking that InnoDB offers; however, there are specific cases when table-level locking works perfectly. Those are the scenarios where read operations vastly outnumber the writes, such as in a blog or a website where most of the time you retrieve data to display and updates to data are infrequent. Another key feature of MyISAM is that it supports Full-Text Search indexes, allowing for natural language searching within character fields. Although InnoDB also supports this feature now, MyISAM was the primary choice for Full-Text Search for a long time. However, the MyISAM engine does not support transactions and foreign key constraints, which might be significant downsides for certain applications. Furthermore, it lacks crash recovery, so a crash can result in data loss or data corruption in a MyISAM table. Topic: 1.4 Other MySQL Storage EnginesIn addition to InnoDB and MyISAM, MySQL provides other storage engines, each with their strengths and optimal use-cases. Let’s get to know them a bit better: MEMORY Engine:&nbsp;As the name suggests, this engine keeps all data in memory, offering extremely fast data access times. But remember, data stored in a table using the MEMORY engine will be lost when the server shuts down or crashes. It’s excellent for storing session or temporary data. CSV Engine:&nbsp;This engine allows you to access the data in comma-separated values (CSV) format. You can even view and edit data in the table using any text editor. It doesn’t support indexes, so every row search is a full table scan. ARCHIVE Engine:&nbsp;If you need to store large amounts of unindexed data, like logs, this is the engine for you. It uses compression to save space and stores data in a way that is easy to back up and transport. While the ARCHIVE engine allows simple SELECT and INSERT statements, it does not support transactions or the ability to delete or update a record. BLACKHOLE Engine:&nbsp;The Blackhole engine accepts data but throws it away and does not store it. You might wonder why it’s useful? The Blackhole engine can be used for replicating to more than one slave, and is also used for audit logging on a database server. FEDERATED Engine:&nbsp;The Federated Storage Engine allows you to access tables located on other databases on other servers. It provides the ability to create one logical database from many physical servers. Each of these engines has unique capabilities and fits different scenarios depending on the requirements. That’s the beauty of MySQL’s pluggable storage engine architecture — you can choose the one that serves your needs the best. Topic: 1.5 Comparison of Storage EnginesMySQL’s versatile set of storage engines, each with their unique feature set, make it an adaptable choice for a vast array of workloads. Now, we will contrast these storage engines, exploring their strengths and weaknesses, and suggesting best-fit contexts. InnoDB vs. MyISAM: InnoDB outshines MyISAM when your workload relies heavily on writing operations or requires transactions, as it provides ACID-compliant transaction features, row-level locking and crash recovery. On the contrary, MyISAM could be a sensible choice when the workload is read-intensive, and the durability or atomicity of the transactions is not a deal-breaker. InnoDB/MYISAM vs. MEMORY: The MEMORY storage engine, delivering lightning-fast data access by holding all data in memory, is a good fit for storing temporary or session data. But unlike InnoDB and MyISAM, all data is lost when the server shuts down or crashes. InnoDB/MYISAM/MEMORY vs. CSV: The CSV engine makes data handling more manageable and flexible by allowing data edit in any text editor. However, it lacks indexing, resulting in full table scans for each row search, and thus might not be performant for large workloads. InnoDB/MYISAM/MEMORY/CSV vs. ARCHIVE: When it comes to storing large amounts of rarely-referenced data, like logs or historical transactions, the ARCHIVE engine excels by saving storage space through compression. InnoDB/MYISAM/MEMORY/CSV/ARCHIVE vs. BLACKHOLE and FEDERATED: These two engines are quite niched compared to the others: BLACKHOLE can be helpful for tasks like audit logging or multi-slave replication, while FEDERATED can help create a logically single database from various physical servers. Remember, choosing the right storage engine largely depends on your specific workload and application requirements. Topic: 1.6 Choosing the Right Storage EngineSelecting the right storage engine is a crucial decision when setting up your MySQL database because it can significantly impact your application’s performance and reliability. Here are some factors to consider when making your choice: Data Integrity: If your application demands high data integrity where transactions need to be atomic (all-or-nothing), you should consider using the InnoDB storage engine which supports ACID (Atomicity, Consistency, Isolation, Durability) properties. Full-text Search: If you plan to run full-text search queries, both MyISAM and InnoDB support this but with varying features. You’ll have to individually explore these features to make sure they fit your use case. Memory Usage: If you need maximum read/write speed and the data you’re working with is temporary (like session data), the MEMORY storage engine, which stores all data in memory, could be the ideal fit for you. Large Amounts of Data: For handling large amounts of seldom-referenced or historical data, consider the ARCHIVE engine which compresses the data for efficient storage. Number of Reads/Writes: Evaluate your application’s read-to-write operation ratio. If the number of read operations significantly exceeds write operations, you may benefit from the MyISAM engine. Conversely, InnoDB is more suitable for write-heavy applications. Server Failures: Consideration for what happens during a crash is crucial. If durability is essential for your application, InnoDB should be your choice since it can recover from a crash using transaction logs. On the other hand, MyISAM doesn’t guarantee data durability in case of a crash. Remember, there is no one-size-fits-all engine, and you might end up using different storage engines for different tables within the same application according to your precise needs. Topic: 1.7 Review and AssessmentsOver our last few lessons, we’ve taken an in-depth tour of MySQL’s various storage engines, understanding their unique features, and compared them based on certain criteria. We have: Defined what&nbsp;storage engines&nbsp;are and their role in MySQL. Grasped the features and advantages of the&nbsp;InnoDB&nbsp;and&nbsp;MyISAM&nbsp;engines. Explored other MySQL storage engines like&nbsp;MEMORY,&nbsp;CSV,&nbsp;ARCHIVE, and more. Made a comparison of these storage engines to understand their best use-cases. Discussed the factors to consider when choosing the right storage engine for your database. Let’s test out your understanding before we conclude this series: Question 1:&nbsp;What distinguishes InnoDB from MyISAM, and when might you prefer to use one over the other? Question 2:&nbsp;Describe a situation where you would benefit from using the MEMORY engine. Question 3:&nbsp;If you had a requirement for storing large amounts of log data, which storage engine would you choose, and why? Question 4:&nbsp;Name a few factors you’d consider when choosing a storage engine for your database. Please respond to these questions one by one. Answer 1:&nbsp;InnoDB is a storage engine that offers features like ACID-compliant transaction support, row-level locking, and crash recovery. This makes it suitable for write-heavy applications or scenarios that require high data integrity. On the other hand, MyISAM is often used for read-heavy applications because it has faster read capabilities. However, it lacks transaction support and crash safety features. Answer 2:&nbsp;The MEMORY storage engine can be a valuable tool when you’re dealing with temporary data, such as session data. As all data is stored in memory and is faster than disk-based storage, it offers extremely quick access times. However, bear in mind that all data stored with this engine will be lost if the server terminates or crashes. Answer 3:&nbsp;The ARCHIVE storage engine can be a wise choice when dealing with logging data or any large amount of rarely-referenced data. This engine supports compression, which can save a significant amount of storage space. Answer 4:&nbsp;When choosing a storage engine for your database, you might consider factors such as: The type of operations your database will be mostly handling (READs or WRITEs). Whether transaction support and crash safety mechanisms are required. The volume of data and the acceptable access/read-write speed. Specific features like full-text indexing or GIS capabilities. 中文文章: https://programmerscareer.com/zh-cn/mysql-interview18/Author: Wesley Wei – Twitter Wesley Wei – MediumNote: If you choose to repost or use this article, please cite the original source.","link":"/mysql-interview18/"},{"title":"MySQL Interview Question: Suppose to create a composite index (a, b, c) If you query fields A and c, will this composite index be used?","text":"Let’s create a lesson plan focused on MySQL, specifically on understanding the application of composite index Thank you for reading this article. More Interview Questions here:https://programmerscareer.com/software-interview-set/ Topic: 1.1 Introduction to Composite Indexes in MySQLComposite indexes, also known as concatenated or multi-column indexes, are a form of database index that spans two or more columns in a database table. These columns are arranged in a specific order, and the composite index is directed by this column order. In a simple analogy, consider a library. Single-field indexing is like sorting the books solely based on authors’ names. However, a composite index is more like sorting the books based on both authors’ names and book titles. In MySQL, composite indexes play a fundamental role in increasing the efficiency of database operations. It’s important to understand that MySQL can use a composite index for queries that involve any of the columns in the index, but the index would be most efficient when you use the leftmost prefix of the index column list. One key feature of MySQL composite indexes is “leftmost prefixing”. In a composite index, the column order matters. In a composite index (a, b, c), MySQL can use this index to speed up queries involving ‘a’ or ‘a and b’, but not ‘b and c’, because ‘b and c’ is not a leftmost prefix of the index column list. So, composite indexes can improve query performance to a great extent, but their efficient use depends on how we are querying the data. Topic: 1.2 Creating a Composite IndexCreating a composite index in MySQL is similar to creating a single-column index, with a slight change in syntax. Here’s the basic syntax for creating a composite index: 12CREATE INDEX index_name ON table_name(col1, col2, …); index_name&nbsp;is the name you want to give to the index. table_name&nbsp;is the name of the table you want to create the index on. (col1, col2, …)&nbsp;are the names of the columns you want to include in the index, in the order you want them to appear. Let’s consider a practical example. Suppose there’s a&nbsp;sales&nbsp;table with&nbsp;ProductID,&nbsp;OrderDate, and&nbsp;Region&nbsp;columns. If you often perform queries that filter or sort by&nbsp;ProductID&nbsp;and&nbsp;OrderDate, a composite index will be beneficial. Here’s how you’d create it: 12CREATE INDEX idx_sales ON sales(ProductID, OrderDate); In the above example,&nbsp;idx_sales&nbsp;is a composite index on the&nbsp;sales&nbsp;table including&nbsp;ProductID&nbsp;and&nbsp;OrderDate. The order of columns matters because MySQL will use the index for queries that involve&nbsp;ProductID&nbsp;alone or both&nbsp;ProductID&nbsp;and&nbsp;OrderDate, but not&nbsp;OrderDate&nbsp;alone. Remember that using composite indexes comes with trade-offs. While they can significantly speed up querying, they can also slow down write operations (like INSERT, UPDATE, DELETE), because MySQL must also update the indexes. It’s important to find the right balance to optimize your database performance. Topic: 1.3 Querying A and C FieldsThis topic tackles an important question: Will MySQL use a composite index&nbsp;(a, b, c)&nbsp;if you’re querying only fields A and C? The answer largely depends on your SQL query and how you structured your index. If the columns being used in the WHERE clause are the first N consecutive columns of the index, then the index can be used. So if you’re querying on a composite&nbsp;(a, b, c)&nbsp;index with fields A and C but without field B, the index may not be as effective. For example, consider this composite index: 12CREATE INDEX comp_index ON table(a, b, c); And suppose we have this query: 123SELECT * FROM table WHERE a = 1 AND c = 3; In this case, MySQL would only use the index on field&nbsp;a&nbsp;and would ignore the index on&nbsp;c, because&nbsp;c&nbsp;is not contiguous with&nbsp;a&nbsp;in the index leftmost prefix. Alternatively, to make sure the index on fields&nbsp;a&nbsp;and&nbsp;c&nbsp;gets used, you could restructure your composite index as&nbsp;(a, c, b)&nbsp;or&nbsp;(c, a, b)&nbsp;and adjust your queries accordingly. The point here is that the fields in&nbsp;WHERE&nbsp;clause should align with the leftmost prefix of the index columns. Keep in mind that it’s always a good idea to regularly analyze the performance of your queries and adjust indexes as necessary. MySQL’s EXPLAIN statement is a useful tool to understand how your queries interact with indexes. In the next section, we will learn how to optimize the use of composite indexes in MySQL for better results. Topic: 1.4 Composite Index OptimizationOptimizing composite indexes in MySQL can significantly improve your database queries’ efficiency and speed. Remember, an efficiently implemented index saves time, resources, and improves overall application performance. Here are several key points to remember for the effective use of composite indexes: Order of Columns:&nbsp;The order of columns in the composite index can make a significant difference. MySQL can efficiently use the index if the columns in your query align with the leftmost prefix of the index. If your WHERE clause uses several columns, you might obtain multiple indexes or a composite index — the choice between these options would rely on specific application requirements. Index Cardinality:&nbsp;Index cardinality refers to the diversity of indexed values. Index columns with higher cardinality lead to fewer row scans and increased query performance. Hence, in a composite index, the column with the highest cardinality should ideally be placed first. Equality vs. Range Conditions:&nbsp;In a composite index, MySQL can perform equality checks for all columns and a range check for the last column. If there’s a range condition in the middle of your WHERE clause, MySQL can’t use the index parts to the right of that range. Over-Indexing:&nbsp;While indexes accelerate data retrieval, they slow down data modifications such as INSERT, UPDATE, and DELETE queries because each modification in indexed column data requires an update in the index structure. Ensure you’re not over-indexing your tables — every index should serve a purpose. Use EXPLAIN:&nbsp;The EXPLAIN keyword in MySQL shows how the optimizer chooses indexes to execute the query. Regularly check your queries using EXPLAIN to understand how the optimizer interacts with your indexes. Topic: 1.5 Review and AssessmentsOver the course of our sessions, we’ve learned about Composite Indexes in MySQL, their structure, and their distinction from single field indexes. We studied the syntax and examples of creating a composite index. We then explored querying specific fields like A and C with a composite index. We then progressed to achieve a deeper understanding of optimizing our use of composite indexes in MySQL. Key concepts we covered include: Composite Indexes: They’re indexes that span multiple columns. This multi-column categorization can dramatically speed up query execution. Creating Composite Indexes: We studied the syntax and walked through an example of creating a composite index. Querying A and C Fields: We observed that MySQL can only utilize the portions of an index where the columns are referenced in the WHERE clause from left to right without skipping any columns. Composite Index Optimization: We went through some essential tips and tricks for optimizing the use of composite indexes, including the order of columns, index cardinality, distinguishing between equality and range conditions, avoiding over-indexing and making use of the EXPLAIN keyword in MySQL. It’s now time to assess your understanding of Composite Indexes. Example Problem: Let’s say you are running an online book store that has thousands of books. Your main&nbsp;books&nbsp;table in your MySQL database has the following columns:&nbsp;id, title, author, genre, publication_year and price. You realize that you frequently execute the following query: 123SELECT * FROM books WHERE genre = 'Science Fiction' AND publication_year = 2018; To optimize this query, you create the following composite index: 12CREATE INDEX genre_year_index ON books (genre, publication_year); This composite index should make your common query faster. Now, it’s your turn! Question: It’s the holiday season and your store is offering discounts on many books. You plan to send an email to all users who bought ‘Science Fiction’ books published after 2010. Write a SQL query to extract a list of user_ids from an orders table, which has the columns:&nbsp;order_id, user_id, book_id, order_date. Assume there is also a junction table named ‘orders_books’ with&nbsp;order_id, book_id. to get the list of&nbsp;user_id&nbsp;from the orders table who bought ‘Science Fiction’ books published after 2010, we will use JOIN to combine information from the&nbsp;orders,&nbsp;orders_books&nbsp;and&nbsp;books&nbsp;tables. Your SQL query would look something like this: 12345SELECT DISTINCT o.user_id FROM orders AS o JOIN orders_books AS ob ON o.order_id = ob.order_id JOIN books AS b ON ob.book_id = b.id WHERE b.genre = 'Science Fiction' AND b.publication_year &gt; 2010; This query checks the&nbsp;genre&nbsp;and&nbsp;publication_year&nbsp;of the books in each order and returns the&nbsp;user_id&nbsp;for those matching the criteria. The&nbsp;DISTINCT&nbsp;keyword is used to eliminate any duplicate&nbsp;user_id&nbsp;from the result. Remember, understanding the structure of your data and how it is related is crucial when working with SQL and databases. Also, always assure you have the right indexes set for your queries. 中文文章: https://programmerscareer.com/zh-cn/mysql-interview19/Author: Wesley Wei – Twitter Wesley Wei – MediumNote: If you choose to repost or use this article, please cite the original source.","link":"/mysql-interview19/"},{"title":"MySQL interviews: What are ACID in a database?","text":"I will prepare an outline of the topics for learning MySQL, particularly focusing on the ACID properties in a database. Thank you for reading this article. More Interview Questions here:https://programmerscareer.com/software-interview-set/ Topic: 1.1 ACID in theoryACID is an acronym used to define a set of properties that guarantee reliable processing of database operations, particularly within a transaction model. The elements of ACID stand for: Atomicity: This means that a transaction must be treated as a single unit, which either succeeds completely or fails entirely. If any part of a transaction fails, the entire transaction fails, and any changes made during the transaction are rolled back (returned back to their previous state). Consistency: Consistency ensures that a transaction brings a database from one valid state to another valid state. It doesn’t allow the transaction to leave the database in an inconsistent state. Isolation: Isolation assures that transactions are securely and independently processed at the same time without any interaction. Durability: As the name implies, durability makes sure that the result or effect of a committed transaction survives future system failures. It is through these core properties that databases maintain the integrity and consistency of data across all transactions. Topic: 1.2 ACID in practiceThe ACID properties play a crucial role in maintaining the reliability and integrity of data in a MySQL database. Now, let’s see how they work in practice. Atomicity: MySQL ensures atomicity with the help of transactions. In a MySQL database, a transaction is a set of SQL statements that are executed as a single unit. This means that either all the SQL statements within a transaction are executed, or none of them are. If a failure occurs at any point during a transaction, any changes that were made during that transaction are rolled back, and the database is returned to its previous state. Consistency: The consistency property in MySQL ensures that only valid data following all rules and constraints is written into the database. If a transaction results in invalid data, the entire transaction is rolled back and the database remains unchanged. This ensures that the database transitions from one consistent state to another. Isolation: In MySQL, concurrent transactions are isolated from each other. This means that the execution of one transaction doesn’t affect the execution of others. MySQL uses locks and different isolation levels to achieve isolation. This prevents problems like dirty reads, non-repeatable reads, and phantom reads. Durability: MySQL ensures durability by writing changes permanently to disk storage before a transaction is marked as successful. This means once a user has been notified of a successful transaction, they can be confident that the transaction has been permanently recorded and will survive any subsequent server failures or restarts. In understanding these, it’s important to note that MySQL gives you the flexibility to define transaction boundaries that fit your application’s requirements by providing different configurations. Topic: 1.3 Atomicity in MySQLAtomicity is one of the key properties of ACID in database systems. It ensures that a transaction is treated as a single unit of work that either completely succeeds or fails. There is no state where a transaction is left partially complete. In MySQL, transactions often encompass several SQL commands. Atomicity guarantees that if a problem such as a power outage, system crash, or network issue takes place after a few commands of a transaction have been executed, then those commands are undone. It’s like the transaction never happened. On the other hand, if all the commands in a transaction are executed successfully, the transaction is considered successfully committed to the database. Here is an example of how atomicity works in a MySQL transaction: Let’s say we are running a book store and we’re updating the quantity of books in our inventory and sales records. The transaction could look something like this: 1234START TRANSACTION; UPDATE Inventory SET Quantity = Quantity - 1 WHERE BookID = 100; UPDATE Sales SET TotalSold = TotalSold + 1 WHERE BookID = 100; COMMIT In this transaction, we have two UPDATE statements. Both these statements should succeed for the transaction to commit successfully. If either one fails, for instance due to a system crash or a network error, the entire transaction is rolled back due to the principle of atomicity, ensuring our inventory and sales records stay consistent. Atomicity is a powerful property that ensures our database operations are safe and reliable. Topic: 1.4 Consistency in MySQLConsistency in database systems ensures that database transactions bring the system from one consistent state to another. This means that if a transaction is executed that violates the database’s consistency rules, the entire transaction will be rolled back and the database will remain unchanged. In MySQL, consistency is preserved by using constraint systems. Constraints are rules enforced on columns in a table that prevent invalid data from being entered into them. There are several types of constraints, including: Unique constraints: These ensure that all the values in a column are unique. Primary key constraints: These uniquely identify each record in the table. Foreign key constraints: These maintain referential integrity by ensuring the link between two tables is valid. Not null constraints: These ensure a column cannot have a NULL value. Check constraints: These ensure that all values in a column meet specific conditions. Here’s an example showing how a unique constraint ensures consistency: 1234567CREATE TABLE Employees ( ID int NOT NULL, Name varchar(255) NOT NULL, Age int, PRIMARY KEY (ID), UNIQUE (Name) ); In the above example, the&nbsp;UNIQUE (Name)&nbsp;constraint ensures that no two employees can have the same name, promoting consistency. If we try to insert two employees with the same name, MySQL will not allow it and the consistency of the database will be preserved. Topic: 1.5 Isolation in MySQLIsolation is the “I” in ACID and it means that each transaction should occur in isolation from each other. This means that the execution of one transaction does not impact the execution of others. Isolation is vital in databases to prevent a number of issues that can arise when transactions are executed concurrently. Concurrent transactions in MySQL are managed by a specific mechanism known as locking. MySQL provides different types of locks, including shared locks (read locks) and exclusive locks (write locks). The type of lock used depends on whether the transaction is read or write. MySQL also supports multiple isolation levels which are: READ UNCOMMITTED: The lowest level of isolation. Transactions can see uncommitted changes from other transactions, often leading to issues like dirty reads. READ COMMITTED: A somewhat higher level of isolation. Transactions can only see changes from other transactions that have been committed. REPEATABLE READ: The default isolation in MySQL. Guarantees that all reads within a single transaction will return the same data, even if changes are made during the transaction. SERIALIZABLE: The highest level of isolation. Transactions are executed serially, in other words, one after the other. Here is an example to illustrate isolation in MySQL: 123456789-- Starting a transaction START TRANSACTION; -- Reading data SELECT * FROM table_name WHERE condition; -- Attempting to read the same data will yield the same result, -- irrespective of changes to the data by other transactions SELECT * FROM table_name WHERE condition; -- Committing the transaction COMMIT; In the example above, under the repeatable-read isolation level, the two SELECT statements will give the same result even if there are changes made by other transactions because the changes made by the other transactions will not be visible to this transaction until it is committed. Topic: 1.6 Durability in MySQLThe term ‘Durability’ in the context of database systems (the ‘D’ in ACID) concerns the permanence of data once a transaction has been committed. If a transaction has been successfully committed, durability guarantees that the changes made by that transaction will survive any subsequent failures, such as power loss, system crash, or other unexpected shutdowns. MySQL ensures durability by writing all transactional changes to a binary log before the changes are actually made. This binary log serves as a historical record of all data modifications, which can be used to recreate the state of the data from any point in time. Whenever a transaction is committed in MySQL, all the changes made by that transaction are first written to the binary log, and then MySQL proceeds with actually carrying out the changes. If a system failure occurs, MySQL can replay the binary log up to the point of the last committed transaction, ensuring that all committed transactions are durable. Let’s consider an example where a record is being updated in a table: 123START TRANSACTION; UPDATE Employees SET Salary = Salary + 5000 WHERE ID = 100; COMMIT; In the above transaction, an employee’s salary is being updated. Once the&nbsp;COMMIT&nbsp;statement is executed, the update is immediately written to the binary log before the actual update takes place. This ensures that even if a system crash occurs after the update, the transaction will not be lost. Topic: 1.7 Transactional Control in MySQLTransactional control is a way to manage ACID properties and refers to the operations and commands used to control transactions. In MySQL, the transactional control commands include START TRANSACTION, COMMIT, and ROLLBACK. START TRANSACTION: Marks the start of a transaction. COMMIT: Marks the end of a transaction, and permanently saves any changes made since the last COMMIT or START TRANSACTION. ROLLBACK: Reverses any changes made since the last COMMIT or START TRANSACTION. Here’s an example of transactional control in action in MySQL: 1234567891011-- Starting a transaction START TRANSACTION; -- Inserting data into the table INSERT INTO Customers (Name, Email) VALUES ('John Doe', 'johndoe@example.com'); -- If something went wrong with the previous statement, we can roll back (reverse) the transaction ROLLBACK; -- Now let's try again, this time without any mistakes START TRANSACTION; INSERT INTO Customers (Name, Email) VALUES ('John Doe', 'johndoe@example.com'); -- Since everything went well, we can now commit the transaction (finalise and save our changes) COMMIT; In this example, the&nbsp;ROLLBACK&nbsp;statement was used to reverse a transaction that had some errors. Once the issues were resolved, the transaction was tried again, and once successful, the&nbsp;COMMIT&nbsp;statement was used to finalise and save the changes. Transactional control helps maintain the ACID properties by providing the capability to group one or more statements together into a single transaction, giving greater control over how data is managed and ensuring data integrity. Topic: 1.8 Advanced Topics in MySQLOnce you have a solid understanding of ACID principles and how they are implemented in MySQL, there are several more advanced topics you might also consider exploring further to enhance your MySQL expertise. Some notable areas include but not limited to: Indexing:&nbsp;Indexes are used in MySQL to speed up the retrieval of data from databases. An index in a database is similar to an index at the end of a book — it helps you find information more quickly. Stored procedures:&nbsp;These are pre-written SQL statements stored under a name and executed as a unit. They help to avoid repetitive writing of SQL code that is often required. Data types:&nbsp;MySQL has several datatypes to accommodate a wide variety of data. From alphanumeric strings, date and time, numerical values, and more complex data types like JSON and spatial data types are available in MySQL. Handling NULL values:&nbsp;NULL values in a database can be quite tricky to handle. They represent missing or unknown data. MySQL provides several functions like IS NULL, IS NOT NULL, IFNULL(), and COALESCE() to handle NULL values. Joins and Unions:&nbsp;Joins are used to combine rows from two or more tables based on a related column. They are frequently used in databases as it’s rarely the case that all data needed is in one table. Unions are used to combine the result set of two or more SELECT statements. Security:&nbsp;Dealing with user permissions and secure connections to MySQL database are an integral part of any database management. Topic: 1.9 Review and AssessmentsHere’s a summary of what we’ve learned throughout these lessons: ACID in Theory:&nbsp;We learned about the ACID properties (Atomicity, Consistency, Isolation, Durability) and their importance in database systems. ACID in Practice:&nbsp;We went beyond theory to understand how these properties are implemented in MySQL and how they help ensure data integrity and consistency. Atomicity in MySQL:&nbsp;We explored Atomicity in MySQL with practical examples and saw how it ensures that a database operation either completes in its entirety or does not occur at all. Consistency in MySQL:&nbsp;We learned how MySQL ensures data remains consistent before and after any SQL operation. Isolation in MySQL:&nbsp;We discussed the concept of Isolation and its importance in concurrent database processing. Durability in MySQL:&nbsp;We learned what Durability means in terms of a MySQL database, and observed how MySQL ensures that changes to data survive any subsequent failures. Transactional Control in MySQL:&nbsp;We understood how to manage ACID properties using transactional control in MySQL. We, majorly, understood the usage of START TRANSACTION, COMMIT, and ROLLBACK. Advanced Topics in MySQL:&nbsp;We dived into more advanced topics like indexing, stored procedures, data types, and more to expand our understanding of MySQL. I would recommend revisiting each of these topics and ensuring you have a solid understanding. here are some assessment questions to test your understanding of ACID properties and Transactional Control in MySQL: Question 1: What is Atomicity in MySQL? How is it implemented in practice? Question 2: What does Consistency in MySQL mean and how does MySQL ensure data remains consistent before and after any SQL operation? Question 3: How is Isolation achieved in MySQL especially during concurrent database processing? Question 4: What does the Durability property entail in MySQL? Question 5: In terms of transactional control in MySQL, discuss the importance of the commands START TRANSACTION, COMMIT, and ROLLBACK. Answer 1:Atomicity in MySQL signifies that a transaction must be treated as a single, indivisible unit, which means either it is fully completed or not executed at all. In practice, MySQL implements atomicity using the START TRANSACTION, COMMIT, and ROLLBACK commands. If a transaction is committed, all changes made are permanent. If a transaction is rolled back, no changes are made. Answer 2:Consistency in MySQL ensures that all changes to data bring the database from one valid state to another, maintaining database rules and integrity. MySQL employs several mechanisms to ensure consistency. These include defined table schemas, predefined rules, triggers, and constraints like primary key, foreign key, unique, not null, and check constraints. Answer 3:Isolation in MySQL ensures that concurrently executing transactions result in a system state that would be obtained if transactions were executed sequentially i.e., one after the other. MySQL achieves this using various isolation levels and locking mechanisms. Answer 4:The Durability property in MySQL ensures that once a transaction is committed, it will remain so, even in cases of system failure, power loss, crash or error. This is typically achieved through the use of database backups and transaction logs that can be used to restore the database to its state right before the failure occurred. Answer 5:In MySQL, START TRANSACTION signifies the operation’s starting point. COMMIT means the changes made in the current transaction are made permanent. ROLLBACK signifies that if any error occurs during the processing of any SQL command, then the already executed SQL commands are reverted to maintain the database integrity. Together, these commands help in managing ACID properties during a transaction in a MySQL setting. 中文文章: https://programmerscareer.com/zh-cn/mysql-interview2/Author: Wesley Wei – Twitter Wesley Wei – MediumNote: If you choose to repost or use this article, please cite the original source.","link":"/mysql-interview2/"},{"title":"MySQL interviews: How does MySQL design indexes and optimize queries?","text":"let’s sketch out a thorough curriculum for gaining a deep understanding of MySQL, focusing specifically on index design and query optimization Thank you for reading this article. More Interview Questions here:https://programmerscareer.com/software-interview-set/ Topic: MySQL OverviewA major part of the digital world consists of databases, and MySQL is one of the premiere players in this realm. So, what really is MySQL? MySQL is a relational database management system. But what separates MySQL from the others? It is open-source, meaning it is free for anyone to use and modify. This has led to its widespread adoption across the globe by developers and organizations alike. You’ll often find MySQL backing up the data necessities of websites, applications, and even aiding in scientific research. Its flexibility in supporting a range of data types, offering a surfeit of functions that manipulate and extract data, and having a robust security system has solidified its importance in the world of databases. Most importantly, MySQL has a reputation for being extremely reliable and fast when it comes to data retrieval and management, making it a favourite among many. In the heart of MySQL is the Structured Query Language, SQL, giving users the power to manipulate databases effectively. We can create, retrieve, update, delete data, and perform other intricate analyses through SQL commands. The upcoming topics will help familiarize you with SQL, both the basics and more complex commands, in MySQL as we progress in our curriculum. Understanding MySQL is fundamental to grasping future topics like database schemas, principles of database design, and specific topics like designing indexes and query optimization in MySQL. Topic: SQL in MySQL (Basics)SQL, or Structured Query Language, is the backbone of all relational database management systems, including MySQL. It is the language we use to communicate and interact with databases. Let’s take a look at some of the fundamental SQL commands that you will need to work with MySQL databases. SELECT: This command is the one we use the most — it allows us to select data from the database. It can be as simple as&nbsp;SELECT * FROM people;, which would select and display all the data from the “people” table. Alternatively, you might choose to select only certain columns, say, first names and lastnames:&nbsp;SELECT firstname, lastname FROM people;. INSERT INTO: This command allows us to insert new data into our database. For example,&nbsp;INSERT INTO people (firstname, lastname) VALUES('John', 'Doe');, would insert a new person with the first name of John and the last name of Doe. UPDATE: As the name suggests, with this command, we can update existing data. For instance,&nbsp;UPDATE people SET age=30 WHERE firstname='John' AND lastname='Doe';, would update the age of all people named John Doe to 30. DELETE: A word of caution, this command deletes data! Its use should not be taken lightly. An example usage:&nbsp;DELETE FROM people WHERE firstname='John' AND lastname='Doe';, would delete all the records for people named John Doe. CREATE,&nbsp;ALTER, and&nbsp;DROP: These commands are used to manipulate the schema or structure of the database itself, and not the stored data. CREATE lets us make new tables, ALTER allows changing table structures, and DROP deletes tables. Getting well-versed with these commands will provide a strong foundation to dive deeper into more advanced commands of SQL in MySQL. Remember, practice makes perfect. Try running these commands and understanding their outcomes. Topic: SQL in MySQL (Advanced)While the basic SQL commands provide a solid foundation, mastering MySQL truly comes through understanding and utilizing its more advanced tools. Here are a few advanced MySQL commands that will let you manipulate your databases more effectively: JOIN: SQL’s JOIN clause allows you to combine rows from two or more tables based on a common field. There are several types of JOIN commands — INNER JOIN, LEFT JOIN, RIGHT JOIN, FULL OUTER JOIN, etc. An example of a JOIN command:&nbsp;SELECT Orders.OrderID, Customers.CustomerName FROM Orders INNER JOIN Customers ON Orders.CustomerID = Customers.CustomerID;. This command will combine and display order IDs and customer names from the Orders and Customers tables where the customerIDs match. GROUP BY: This command is used with the aggregate functions COUNT, MAX, MIN, SUM, and AVG to group the result set by one or more columns. An example:&nbsp;SELECT COUNT(animal_type), animal_type FROM animal GROUP BY animal_type;&nbsp;will show the number of each type of animal in the ‘animal’ table. HAVING: This acts like a WHERE clause, but for aggregate functions. A basic example:&nbsp;SELECT COUNT(product_id), product_name FROM products GROUP BY product_name HAVING COUNT(product_id) &gt; 5;&nbsp;will show the product names and their quantities from the ‘products’ table, but only for those where the count of the product_id is greater than 5 UNION: The UNION operator is used to combine the result-set of two or more SELECT statements. Each SELECT statement within the UNION must have the same number of columns, and the columns must also have similar data types. Also, the columns in each SELECT statement must be ordered in the exact same way. For example:&nbsp;SELECT column_name(s) FROM table1 UNION SELECT column_name(s) FROM table2;. CASE: This allows for conditional statements in SQL. For example:&nbsp;SELECT CASE WHEN age &lt; 18 THEN 'Children' WHEN age BETWEEN 18 AND 65 THEN 'Adults' ELSE 'Seniors' END AS age_group FROM people;&nbsp;will categorize people in the ‘people’ table into age groups based on their age. Mastering these commands will take you a long way in harnessing the full power of MySQL. Remember, to master these, hands-on practice is key. Topic: Database Schemas in MySQLBehind every efficient database is a well-crafted schema. A database schema is an abstract blueprint of your database structure — it demonstrates how data is organized and accessed. So let’s get into it! When you think of a database, visualize it as a whole chest of drawers. In MySQL terminology, tables within that chest become a part of our database schema. They bear the actual data that we interact with via SQL commands. Now each drawer, or table, further contains divisions, referred to as columns or fields in MySQL. Each column represents a type of data within a table. For example, in a table (or drawer) storing employee information, different categories of information like Employee ID, Name, Job Position, etc., become different columns. Finally, the actual individual pieces of data stored in each ‘division’ are called records or rows. For instance, the information related to a specific employee (John Doe, ID 12345, Position Manager) becomes a row in the Employee table. Designing database schemas may sound straightforward enough — make a table for each type of data, right? Unfortunately, not! An efficiently designed schema mitigates redundancy, prevents data anomalies, and optimizes resource usage. MySQL strongly implements the principles of the relational database, arranging data in tables that are inter-linked. This brings us to concepts like Primary Keys and Foreign Keys which help establish connections between tables (we will get in detail during the Database Design lesson). Today’s lesson provides a foundation for the following ones where we will discuss principles of database design, indexes, and query optimization. Topic: Principles of Database DesignDesigning a database goes beyond just deciding tables, columns, and using SQL commands. A well-designed database ensures efficient and reliable data storage and retrieval. Let’s go through some of the fundamental principles of database design: Entity-Relationship (ER) Model: Think of entities as ‘things’ or ‘objects’ that are relevant to your database (like employees in a company database). Relationships define how these entities interact with each other. Diagrammatically representing these entities and their relationships gives us an ER model, a foundational step in database design. Normalization: This is the process of organizing a database to eliminate redundancy and improve data integrity. There are several normal forms (first, second, third, BCNF), each with prerequisites that must be met. Primary Key: Every table must have a column (or a set of columns), known as the Primary Key, that uniquely identifies every record in the table. Foreign Key: This is a field (or collection of fields) in one table, that is a primary key in another table. The foreign key is used to prevent actions that would destroy the links between tables. Atomicity: It’s the idea that an operation either completely succeeds or fails. You don’t want a database update to be ‘partially’ done — it either does fully or not at all. Security: Databases often hold sensitive data. Properly designed databases have multiple layers of security including authorization, access control, and encryption. Backup and Recovery: Data is valuable. A well-designed database includes strategies for regular backup and efficient recovery in case of data loss. Scalability and Performance: A good database design also takes into consideration scalability (will the database handle growth in data volume?) and performance (how quickly can the system respond to queries?). Understanding these principles will go a long way in being able to design a database that is robust, reliable, and efficient. Topic: MySQL IndexesIndexes are a critical aspect of database design that boost the speed of data retrieval operations on a database table. Similar to the index in a book, an index in MySQL allows the database system to find the data without having to scan every row in the database table. Here are some key points to remember about indexes in MySQL: Indexes are used to find rows with specific column values faster. Without an index, MySQL must begin with the first row and then read through the entire table to find the relevant rows. Indexes are also used to enforce UNIQUEness constraints, and to aid efficient sorting and grouping. Indexes can be classified based on their structure: B-Tree, Hash, RTree, and Full-text. The most commonly used index structure is the BTree (Balanced Tree), which sorts the data for fast retrieval in a way that ensures the tree remains balanced, hence optimizing search times. Indexing comes at a cost: although data retrieval is faster, data modification operations (such as INSERT, UPDATE, DELETE) will become slower due to the additional operations required to maintain the index. Not all fields need an index. Only fields that you are likely to use in a WHERE, ORDER BY, GROUP BY, or JOIN clause will benefit from an index. Understanding and properly implementing indexes can greatly improve the performance of your database operations. Topic: Designing Indexes in MySQLDesigning indexes is a vital aspect of efficient database management. Here we’re going to talk about how MySQL designs indexes and what strategies it employs to improve overall performance. Creating the right index is more of an art than a science, and it usually involves a trade-off between query speed and write speed. Steps to consider while designing indexes: Choosing the right columns:&nbsp;An index can include multiple columns, but it’s essential to consider the column order. MySQL can only use an index if the query involves the leftmost column of the index. Considering the data type:&nbsp;The smaller the data type, in terms of storage, the smaller the index, and therefore, the faster the queries. Consider the cardinality:&nbsp;High cardinality columns, meaning columns that contain many unique values, tend to have more efficient indexes. Understanding your workload:&nbsp;If your application performs lots of SELECT queries, more indexes can be beneficial. On the other hand, if your application does more INSERT, UPDATE, and DELETE operations, more indexes could slow it down. Analyzing your queries:&nbsp;Use MySQL’s EXPLAIN statement to understand how your indexes are being used and where improvements can be made. Remember that indexes are a vital part of database design. They can significantly increase the performance of your database, so it’s definitely worth considering whenever you’re querying large amounts of data. Topic: MySQL Query OptimizationAn essential part of managing any database is ensuring that it functions efficiently. When dealing with significant amounts of data, queries can become time-consuming. Therefore, optimizing these queries in MySQL is crucial to improving the overall performance of your database system. In MySQL, query optimization involves multiple steps: Parsing: MySQL starts by parsing the SQL query to ensure its syntax is correct, and the database objects exist. Pre-processing: Next, MySQL decides the order of table reads, which is vital for multi-table queries. It also determines the indexes to use. Optimization: MySQL then applies various optimization strategies to make the query more efficient. The most notable is the use of indexes, but MySQL also leverages other techniques like merging multiple similar queries into one, reducing temporary tables, and choosing efficient routes for joining tables. Execution: Finally, MySQL executes the query and returns the result. The actual execution is a balance between fetching the data from the storage engine and processing the SQL command. Understanding how MySQL optimizes queries helps in writing efficient SQL code and designing better database schemas. It can significantly reduce query execution time, especially for complex queries and large databases. Remember to use the&nbsp;EXPLAIN&nbsp;statement in MySQL, which can provide insights on how to optimize your queries better. It shows how MySQL plans to execute a query, which can help identify bottlenecks or areas for improvement. Topic: Review and AssessmentsAfter journeying through the intricate landscape of MySQL, it’s a good time for a thorough revision of every topic covered, and subsequently, evaluating your practical knowledge through assessments. To recap: We started with&nbsp;1.1 MySQL Overview, understanding MySQL role as a robust database management system. We then moved into the essence of MySQL,&nbsp;1.2 SQL in MySQL (Basics)&nbsp;and&nbsp;1.3 SQL in MySQL (Advanced), mastering the rudimentary and the more complex SQL commands used in MySQL. We studied the structure of databases through the concept of&nbsp;1.4 Database Schemas in MySQL. Understanding the&nbsp;1.5 Principles of Database Design&nbsp;helped us grasp the structure ensuring databases function effectively. With&nbsp;1.6 MySQL Indexes&nbsp;we comprehended the significance of indexes in speeding up data retrieval. Then we learned how to enhance performance by&nbsp;1.7 Designing Indexes in MySQL. Lastly, we explored how MySQL optimises queries to improve database performance in&nbsp;1.8 MySQL Query Optimization. Now, if you’d like to test your understanding, you can say&nbsp;“/test”&nbsp;to undertake an assessment. Remember, this assessment is designed to help solidify your knowledge and identify areas that might need a bit more attention. Don’t stress about it — it’s a tool for learning, not a final exam. Let’s proceed with the assessment. Question 1:State the difference between&nbsp;CHAR,&nbsp;VARCHAR, and&nbsp;TEXT&nbsp;data types in MySQL? Question 2:Explain what a&nbsp;JOIN&nbsp;operation is in MySQL, and list its different types. Question 3:What are&nbsp;NULL&nbsp;values in MySQL, and how can you check for such values in a database table? Answer 1:CHAR&nbsp;and&nbsp;VARCHAR&nbsp;are both used to store string values in MySQL, with main differences being: CHAR&nbsp;is a fixed length data type, meaning that it reserves the specified amount of space regardless of the actual size of the data. It’s faster for read operations because MySQL knows exactly where to find the value of a&nbsp;CHAR&nbsp;column. VARCHAR&nbsp;is a variable length data type. It consumes space proportional to the size of the data stored. It saves on storage if you’re dealing with strings of varying lengths. TEXT&nbsp;is also a data type for storing strings, but it’s meant for storing large amounts of text. The length of the string is not fixed, similar to&nbsp;VARCHAR. Answer 2:A&nbsp;JOIN&nbsp;operation in MySQL combines rows from two or more tables based on a related column between them. Types of&nbsp;JOIN&nbsp;include: INNER JOIN: Returns records that have matching values in both tables being joined. LEFT JOIN&nbsp;(or&nbsp;LEFT OUTER JOIN): Returns all records from the left table and the matched records from the right table. If no match is found, the result is&nbsp;NULL&nbsp;on the right side. RIGHT JOIN&nbsp;(or&nbsp;RIGHT OUTER JOIN): Returns all records from the right table and the matched records from the left table. If no match is found, the result is&nbsp;NULL&nbsp;on the left side. FULL JOIN&nbsp;(or&nbsp;FULL OUTER JOIN): Returns all records when there is a match in the left table or the right table records. Answer 3:NULL&nbsp;values in MySQL represent missing or unknown data. You can check for&nbsp;NULL&nbsp;values in a database table using the&nbsp;IS NULL&nbsp;or&nbsp;IS NOT NULL&nbsp;clauses in your SQL statement. Remember, when dealing with&nbsp;NULL, it’s not a value; it’s a state indicating that the value is unknown or missing, so&nbsp;NULL&nbsp;does not equal&nbsp;NULL. 中文文章: https://programmerscareer.com/zh-cn/mysql-interview3/Author: Wesley Wei – Twitter Wesley Wei – MediumNote: If you choose to repost or use this article, please cite the original source.","link":"/mysql-interview3/"},{"title":"MySQL Interviews: Why does MySQL use B+ trees for indexing","text":"Have you ever been asked a similar question in an interview? Or you will meet in the future, let’s get started together Thank you for reading this article. More Interview Questions here:https://programmerscareer.com/software-interview-set/ Topic 1.1: Why Indexing?Just like an index in a book helps us find information quickly without having to read the entire book, an index in a database helps the database application find data quickly without having to search every row in a database table every time a database table is accessed. Indexes significantly speed up data retrieval process, which leads to better application performance. Indexes are crucial in large tables for optimizing ‘SELECT’ queries and where clauses, as they minimize the number of pages the system must go through to find pertinent data. However, they do come with their fair share of cautions. Indexes, while improving read performance, can slow down write (insert, update, delete) performance. This is because every time data changes, indices need to be updated. This is also why having too many indices can actually harm database performance. In a nutshell, a good index is all about creating the right balance. We want to keep queries fast and productive, but without overloading the system with performance hampering index maintenance. Topic 1.2: Types of Indexes in MySQLMySQL utilizes various types of indexes to boost the query performance. Here are the common ones: Primary Index:&nbsp;This index mandates that the column contains only unique, non-null values. Each table can only possess one primary index. Unique Index:&nbsp;This type of index prevents the field from having duplicate values if the column does not contain null values. Except for permit null values, the unique index is almost the same as the primary index. Index (or Normal Index):&nbsp;It allows duplicate and null values in the column. It’s the basic type of index in MySQL. Full-text Index:&nbsp;If you’re dealing with text data and often use full-text search, then this index comes in handy. Composite Index (or Multiple-column index):&nbsp;If you use multiple columns in WHERE clauses, creating a composite index on those fields can speed up the query performance. These index types serve different purposes, and understanding them helps us utilize them properly to make sure that the database we are creating or managing performs in the most optimal way. Topic 1.3: B+ Trees ExplainedAt its core, a B+ Tree is a type of self-balancing search tree that maintains sorted data and allows for efficient insertion, deletion, and search operations. In contrast to binary search trees (BST), where each node has at most two children (left and right), a B+ Tree is a multileveled tree where each node can have multiple children, typically more than two. The important features of a B+ Tree are: All data is stored at the leaf level. All leaf nodes are at the same depth, ensuring balance. All leaf nodes are linked, allowing for efficient range queries. Non-leaf nodes store copies of the keys to guide the search. The combination of these characteristics makes B+ Trees particularly well-suited for systems with large amounts of data and a significant number of read operations, such as databases or filesystems. Each node in a B+ Tree contains a number of keys and pointers. The keys act as separation values which divide its subtrees. For example, if a node contains the values [10, 20, 30] it has four child nodes (subtrees). One fundamental property in a B+ tree is that if a node has&nbsp;n&nbsp;keys, it will have&nbsp;n+1&nbsp;pointers (children). Another property is that all keys of a B+ tree are sorted. As B+Trees rise in popularity due to their high efficiency in accessing, storing, and retrieving data, they’ve become closely linked with the database world, including MySQL. Topic 1.4: Advantages of B+ Treeslet’s delve deeper into the advantages B+ Trees bring about to Databases, especially MySQL: Efficient Disk Read/Write Operations:&nbsp;Each node in a B+ Tree contains multiple keys and pointers packed together on a single disk block, this significantly reduces the I/O operations for reading or writing large ranges of data. So, you can scan large portions of data using minimum disk reads. Faster Search Time:&nbsp;As B+ Trees are height-balanced, an equal number of comparisons leads to all leaf nodes, making data retrieval quicker. The time complexity of search in a B+ Tree is logarithmic, making search operations efficient. Effective Insertions and Deletions:&nbsp;The data structure of B+ Trees enables them to remain balanced and ordered during both data insertions and deletions. This result in minimum disk space wastage and maximum performance efficiency. Ascending/Descending Sort Order Retrieval:&nbsp;The leaf nodes of B+ Tree are linked together. This feature significantly helps in quicker sequential reading of data in either ascending or descending sort order, which is a common operation in databases. Great for both Equality and Range Retrieval:&nbsp;With its self-balancing property and minimum and maximum keys in each page, B+ Trees are phenomenal when it comes to equality and range queries. Multilevel Indexing:&nbsp;B+ Trees can be adapted to perform multi-level indexing, further boosting search performance and reducing disk I/O operations. Topic 1.5: B+ Trees in MySQL IndexingLet’s now understand why and how MySQL uses B+ Trees for indexing in depth. In MySQL, particularly when using the InnoDB storage engine, B+ Trees are used for primary and secondary indexing which enhances the database’s performance by significantly reducing the data access time. Here’s how it works: Primary Indexing: MySQL uses B+ Trees as a primary index to uniquely identify each row, which is ordered by the primary key. The leaf nodes of the B+ Tree store the actual data, and the values of primary key act as a pointer to the data. So, whenever a direct search is performed on the primary key, MySQL quickly navigates through the B+ Tree to find and retrieve the actual data from the disk. Secondary Indexing: The secondary index in a MySQL table is also a B+ Tree. The only difference compared to the primary index is that its leaf nodes don’t store actual data but rather they store pointers to the primary key. So, when there is a search performed using a secondary index, MySQL uses the B+ Tree of the secondary Index to find the primary key first, then uses this primary key to navigate the primary B+ Tree to fetch the actual data. Although this involves navigating two B+ Trees, it is still pretty fast and efficient. The advantage of using B+ trees in MySQL indexing is that it reduces the number of disk accesses required to find an item, which greatly improves performance because disk accesses are time-consuming compared to in-memory operations. Topic 1.6: MySQL Indexing Best PracticesBuilding on our understanding of B+ Trees, let’s now go through some best practices when it comes to MySQL indexing. Effective indexing is absolutely crucial in order to keep your database queries running smoothly and promptly. Understand Your Data:&nbsp;Before you even start indexing, it’s crucial to understand thoroughly the data you’re working with. What columns are often queried together? Which columns appear commonly in your WHERE clauses? This understanding helps guide your indexing strategy. Use the EXPLAIN Keyword:&nbsp;When optimizing your indexes, use the EXPLAIN keyword in SQL to understand how the database is interpreting your query. This can give you insights into how the SQL optimizer will use your indexes and where improvements can be made. Be Mindful of Index Overhead:&nbsp;While indexes speed up search queries they also involve cost. They take up space, and also, each time you modify the data in your tables (INSERT, UPDATE, DELETE), indexes need to be updated. This might slow down these operations. Index Columns Used in WHERE Clauses:&nbsp;Columns that are frequently used in WHERE clauses in the queries are usually good candidates for indexing. Use Multi-Column Indexes Effectively:&nbsp;MySQL allows you to create an index on multiple columns together. When you create such an index, MySQL can use it when queries involve the first column, or the first and the second column, or all the columns in the index. Use Appropriate Index for Different Storage Engines: If you are using InnoDB, note that it stores its rows on the disk based on the primary key. Thus, the choice of the primary key can have a big impact on the performance of InnoDB tables. Remember, these are just guidelines and the best practices can vary based on your exact use case. Topic 1.7: Real World Case StudiesGreat! We’re progressing nicely through our structured course. It’s always helpful to bolster our learning with practical examples. So, let’s delve into a few case studies highlighting the use of MySQL indexing and B+ Trees. E-commerce Systems: Consider the case of an online retail system like Amazon. These platforms manage a tremendous volume of data, pertaining to goods, user details, transaction details, etc. Given the enormous number of products and the frequency of transactions, the speed of data retrieval is paramount. Here, MySQL indexing plays a major role. Effective usage of primary, unique, and full-text indexes significantly speeds up the querying process, providing an efficient, seamless user experience. The use of B+ Trees for indexing allows the system to handle millions of items without a significant drop in performance. Social Media Platforms: Social media platforms like Facebook or Twitter also make extensive use of indexing. Every time we open our feed, the system queries a vast database to fetch relevant posts. Imagine finding a needle in a haystack — that’s what it would be like for the system to retrieve our personalized feed without indexing. Proper indexing allows these services to rapidly deliver the data we need each time we log in or refresh our feed. Search Engines: Google, Yahoo, Bing, and many other search engines also use extensive indexing to provide fast and accurate search results. Without the use of proficient indexing strategies, it would be impossible to get instant search results from the vast world of the internet. These are just a snapshot of the real-world applications where indexing and B+ Trees play a major role. Whether you are developing a website, an app, or any platform dealing with large amounts of data, understanding and using these structures effectively can make a significant difference in performance and efficiency. Topic 1.8: Potential Interview Questions and AnswersAlright, moving forward. Let’s prepare for some potential interview questions about MySQL indexing and B+ Trees. Having a good grip on these concepts can help you perform well in your job applications, and it’s always better to be ready! Here are some questions with answers Why is indexing important in databases? Indexing enhances database efficiency by providing swift data retrieval methods. An index in a database works similarly to an index in a book, enabling faster access to data. Without indexing, to find data, the database would need to dig through every record in a table — termed a full table scan — which can be time and resource-intensive. What is a B+ Tree? A B+ Tree is a type of data structure used in databases for storing data in a sorted and efficient manner. It is a balanced tree structure where all leaf nodes are at the same level, making searches, insertions, and deletions efficient, even for large sets of data. How does MySQL use B+ Trees for indexing? MySQL uses B+ Trees as the default indexing scheme in its InnoDB storage engine. Both primary and secondary indexes in InnoDB are stored as B+ Trees. The leaf nodes of a primary index’s B+ Tree contain the row data for the table, while the leaf nodes of a secondary index’s B+ Tree contain the primary key values for the respective rows. What are some best practices for MySQL indexing? Important best practices include understanding your data before indexing, using the EXPLAIN keyword to understand query execution, indexing columns used in WHERE clauses, making effective use of multi-column indexes, considering index overhead, and using appropriate indexes depending on the storage engine. Can you give an example where indexing significantly improves performance? E-commerce platforms can be a good example here. They have to manage loads of data — user details, product details, transactions, etc. Indexing can help sort and retrieve this data quickly, improving the search and transaction efficiency and enhancing the user experience. Topic 1.9: Review and AssessmentsPerfect, reaching the last part of the course, we’ll now review the key concepts we covered and engage in some self-assessments. Let’s recap what we’ve learned: Why Indexing: We’ve understood the key role played by indexing in improving the efficiency and speed of data retrieval in databases. Types of Indexes in MySQL: We’ve explored the various types of indexes in MySQL, including primary, unique, full-text, simple, and composite indexes and where they are used. B+ Trees: We’ve deep-dived into the structure of B+ Trees, how they function and the efficiency they offer in storing and retrieving data. B+ Trees in MySQL Indexing: We’ve seen how MySQL uses B+ Trees as an indexing structure, focusing on the InnoDB storage engine. MySQL Indexing Best Practices: We’ve probed into how to use indexing effectively for the best performance tips. Real-world applications: We’ve looked at how indexing and B+ Trees are applied in real-world examples, including in social media platforms, search engines, and E-commerce systems. Now, for assessments, here are a few quiz questions and small projects: Quiz Questions: What is the role of indexing in databases? Briefly describe the structure of a B+ Tree and how it works. What is the difference between a primary and secondary index in MySQL? What are three best practices when using indexing in MySQL? Small Projects: Take a small dataset (you can create or download one). Implement MySQL indexing and observe the performance difference when retrieving data. Consider an E-commerce database with tables storing user information, product details, and transaction history. Design a basic schema with indexing and illustrate how different types of MySQL indexes are used. Happy learning! 中文文章: https://programmerscareer.com/zh-cn/mysql-interview4/Author: Wesley Wei – Twitter Wesley Wei – MediumNote: If you choose to repost or use this article, please cite the original source.","link":"/mysql-interview4/"},{"title":"DataBase interviews: Briefly describe the difference between optimistic locks and pessimistic locks and the usage scenarios","text":"Have you ever been asked a similar question in an interview? Or you will meet in the future, let’s explore and master it together Thank you for reading this article. More Interview Questions here:https://programmerscareer.com/software-interview-set/ Topic 1.1: Introduction to MySQL and Locking MechanismsLet’s embark on a new journey of learning about&nbsp;MySQL&nbsp;and&nbsp;Locking Mechanisms. MySQL is one of the world’s most renowned open-source relational database management systems (RDBMS). It’s popular for web-based applications and online publishing and is a central component of the LAMP open-source web application software stack. Now let’s talk a bit about why we need locking mechanisms in databases. Imagine a scenario where two individuals are attempting to withdraw money from the same bank account simultaneously. If there isn’t a mechanism to prevent it, they could both check the account balance at the same time, see that there are sufficient funds, and proceed to withdraw more money than is actually in the account. This is known as a race condition and could lead to serious data integrity issues. This is where locking mechanisms come into play! Topic 1.2: Overview of Locking MechanismsSo, let’s dive deeper into the world of Locking Mechanisms in MySQL. As mentioned before, locking contributes significantly to maintaining data integrity, especially in multi-user database environments. Locking in databases essentially controls how transactions are accessed so that each transaction sees a consistent snapshot of the data it is accessing. The main types of locking used in MySQL are: Shared Locks (Read Locks): Shared locks are used when executing a read operation on data. They allow concurrent transactions to read (select) a resource with the guarantee that no transactions will be able to write (update/delete) it. Exclusive Locks (Write Locks): Exclusive locks are issued when executing a data modification operation. They make sure that the transaction that holds the lock is the only transaction that can read or write to the resource. In the next chapters, we will venture into two popular types of locking mechanisms-&nbsp;Optimistic Locking&nbsp;and&nbsp;Pessimistic Locking. The choice of which to use generally depends on the specific requirements of your system, such as the probability of concurrent transactions conflicting. Topic 1.3: Understanding Optimistic LockingTime to take a closer look at&nbsp;Optimistic Locking. This is a strategy that can be implemented in multi-user databases to handle simultaneous updates. Optimistic Locking works on the assumption that multiple transactions can complete without affecting each other; therefore, it allows multiple transactions to access the same record for edits. This method is useful in systems where there is low contention for data. Here’s a simple way how Optimistic Locking works: A record is fetched from the database for an update. Just before the update, the application checks if another user has changed the record since it was last fetched. If the record was not updated by others, the application can perform its update and everything proceeds smoothly. If the record was updated by someone else, the application typically either informs the user and aborts the transaction or automatically retries the transaction. The main advantage of optimistic locking is its higher efficiency. It avoids the overhead of acquiring and releasing locks and avoids having transactions wait for locks. However bear in mind, nothing comes without a flip side! In an environment where contention for data is high and there are many updates to data, there could be many transaction collisions leading to a lot of rollbacks, which could lead to performance issues. Topic 1.4: Understanding Pessimistic LockingLet’s navigate into the sea of&nbsp;Pessimistic Locking&nbsp;now. This mechanism in MySQL is based on a completely different assumption from Optimistic Locking. It assumes that conflict is likely to happen and therefore enforces stringent controls to prevent this. Here’s how Pessimistic Locking works: When a record is fetched for updating, an exclusive lock is immediately acquired. Until the lock is released, no other transaction can update this record. The lock is released when the transaction is completed, and other transactions can then acquire the lock for this record. Pessimistic Locking is a surefire way to prevent conflicts, as it does not allow another transaction to proceed if it can result in a conflict. It’s a good fit for environments where contention for data is high and records are frequently updated. But remember, every coin has two sides! The downside of this approach is that it can lead to reduced concurrency and can impact system performance as transactions may be held waiting for locks for extended periods. Topic 1.5: Comparing Optimistic and Pessimistic Locking.You now are familiar with both Optimistic and Pessimistic Locking, and understanding when to use each one can significantly influence the performance and reliability of your applications. Optimistic Locking&nbsp;assumes conflicts are rare and mostly avoids the need for acquiring and releasing locks. It can result in higher performance under low contention scenarios because it causes fewer blocks. However, for systems where contention is high, and conflicts are frequent, the cost and frequency of rollbacks can degrade performance. On the other hand,&nbsp;Pessimistic Locking&nbsp;assumes conflicts will commonly occur and uses locks to prevent them. This strategy can be advantageous in high contention scenarios because it avoids the need for conflict-resolution related rollbacks. However, the wait time associated with acquiring locks can degrade performance. So, the golden rule is: Opt for Optimistic Locking when conflicts are rare.Opt for Pessimistic Locking when conflicts are expected. That’s a brief comparison of Optimistic and Pessimistic Locking in MySQL. Topic: 1.6 Use Case Scenarios for Locking MechanismsFantastic! We now understand the primary locking mechanisms let’s look at some real-world scenarios where these mechanisms could be beneficial. A banking system: Imagine a banking application where transactions happen regularly. These transactions need to be consistent and secure. In such cases, a Pessimistic Locking mechanism is favourable as it ensures that once a user starts a transaction, no one else can modify the data, ensuring data integrity. A ticket booking application: Consider an online ticket booking system where multiple users are trying to book a limited number of tickets. Here, Optimistic Locking could be beneficial as it would allow multiple users to access the ticket booking function concurrently. A content management system: If you’re working on a Content Management System where users are updating their blog posts/articles, Optimistic Locking can be a good choice. Since the odds of two users trying to edit the same article at the same time are comparatively low, the system can handle those occasional conflicts. A Stock trading application: In a Stock trading application where a split-second delay could mean significant financial loss, Pessimistic Locking could help by instantly blocking other transactions until one completes. Remember, the decision to use Optimistic or Pessimistic Locking depends greatly on the nature of the application, the likelihood of conflicts, the level of concurrency required, and the tolerance for delay. Topic: 1.7 Review and Assessments Introduction to MySQL and Locking Mechanisms: We discussed how MySQL functions, the importance of locking mechanisms, and how it helps with concurrent database accesses. Overview of Locking Mechanisms: We looked at a variety of locking mechanisms, their importance in maintaining data integrity, and handling concurrent access. Understanding Optimistic Locking: We did a deep dive into the concept of optimistic locking, its pros and cons, and its implementation in MySQL. Understanding Pessimistic Locking: We explored pessimistic locking, including its strengths, weaknesses, and how it can be implemented in MySQL. Comparing Optimistic and Pessimistic Locking: We compared these two locking mechanisms and concluded that the choice highly depends on the particular use-cases and system requirements. Use Case Scenarios for Locking Mechanisms: We looked through possible real-life application scenarios where these locking mechanisms can offer benefits. Example Problem:Let’s take an example: you have a banking database in MySQL with a table named ‘Account’ storing users’ account balance. Two different financial transactions are trying to deduct money from the same account simultaneously. How would you handle this situation using both optimistic and pessimistic locking mechanisms in MySQL? Answer: For&nbsp;Optimistic Locking, you can handle this by using a version column in the Account table. Here’s how: Transactions first read the account balance and note down the version. Before they update the account, they check if the version is still the same. If the version had changed, that would mean that another transaction has updated the account balance in the meantime, and therefore, the current transaction is rolled back. This way, we avoid inconsistent deductions. For&nbsp;Pessimistic Locking, you would lock the account for every financial transaction. Here’s how: Transactions lock the account immediately upon reading the account balance. Only one transaction can hold the lock at a time, and it holds it until it finishes updating the account balance. All other transactions that attempt to read the account balance while the lock is held will be blocked until the lock is released. Now let’s test your knowledge. Question 1 (Easy): In what scenario would you choose Optimistic Locking over Pessimistic Locking? Question 2 (Intermediate): What could be a potential downside of using Pessimistic Locking in a high throughput system, and how could this downside be mitigated? Question 3 (Hard): Can you describe a scenario where neither Optimistic nor Pessimistic Locking is suitable, and a different locking or concurrency control mechanism would be required? 中文文章: https://programmerscareer.com/zh-cn/mysql-interview5/Author: Wesley Wei – Twitter Wesley Wei – MediumNote: If you choose to repost or use this article, please cite the original source.","link":"/mysql-interview5/"},{"title":"Database interviews:What are the necessary conditions for a deadlock to occur? How do I resolve deadlocks?","text":"Have you ever been asked a similar question in an interview? Or you will meet in the future, let’s explore and master it together Thank you for reading this article. More Interview Questions here:https://programmerscareer.com/software-interview-set/ Topic: 1.1 Introduction to DeadlocksDeadlocks are a condition in a multithreading environment when two or more threads are unable to proceed because each is waiting for the others to release resources. In the context of a MySQL database, a deadlock occurs when two or more transactions mutually hold and request for locks, creating a cycle of dependencies. In a transaction, a thread might need to lock multiple tables or rows, which can sometimes lead to a situation where the thread needs a resource that’s locked by another thread. Meanwhile, that thread might be waiting for a resource locked by the first thread. This is known as a deadlock. Let me illustrate this with a simple story: Imagine two comic book fans, Alice and Bob. Alice has the latest ‘Superbot’ comic which Bob wants. At the same time, Bob has the newest ‘Megagirl’ comic, which Alice wants. Now, Alice is not willing to give up her ‘Superbot’ comic until she has the ‘Megagirl’ comic in her hands, and Bob too, won’t give up his ‘Megagirl’ comic until he has the ‘Superbot’ comic. So, both are waiting for each other to let go of their comics, which results in a deadlock. In terms of database transactions, Alice and Bob could be transactions, and the comic books could be the locked resources. Topic: 1.2 Understanding Necessary Conditions for a DeadlockTo understand how deadlocks occur, we need to be familiar with Coffman’s Conditions, which are a set of four conditions that must all hold for a deadlock to occur. These are named after Edward G. Coffman, Jr., who first articulated them. The conditions are: Mutual Exclusion: At least one resource must be held in a non-shareable mode. This means only one process (or thread) can use the resource at any given time. If another process requests the resource, the requesting process must be delayed until the resource has been released. Hold and Wait (Resource Holding): A process must be holding at least one resource and waiting to acquire additional resources that are currently being held by other processes. No Preemption: Resources cannot be forcibly removed from the processes holding them until the resources are used to completion. The resources can only be released voluntarily by the process holding them. Circular Wait: A circular chain of processes exists where each process holds one resource while requesting another resource held by another process in the chain. Essentially, there’s a process P1 that is waiting for a resource that is held by process P2, and P2 is waiting for a resource held by P1. This makes a circular chain of waiting processes. These 4 conditions inherently provide a logical structure to understand and structure the prevention policies. By ensuring that at least one of the above conditions never occurs, we can prevent the formation of deadlocks. Topic 1.3: Detecting Deadlocks in MySQLIn MySQL, the InnoDB storage engine automatically detects deadlocks and resolves them by rolling back one of the transactions involved. Therefore, your application should always be ready to re-issue a transaction if it gets rolled back due to a deadlock. When a deadlock occurs in MySQL, it’s immediately detected and resolved by the system. This is achieved by the&nbsp;wait-for graph&nbsp;deadlock detection mechanism, where MySQL maintains information about which transactions are waiting for locks held by which other transactions. With this approach, MySQL can check for cycles in the wait-for graph. If it detects a cycle, this indicates a deadlock, and it’ll roll back one of the transactions to break the deadlock. To give you more insight into the situation, MySQL also provides diagnostic information when it detects and resolves a deadlock. This information can be obtained from the&nbsp;SHOW ENGINE INNODB STATUS&nbsp;command, which will showcase the latest deadlock error. However, it’s important to note that deadlocks are not necessarily a sign of a design flaw or error. In some high concurrency systems, deadlocks may happen from time to time and can be considered a cost of doing business. But of course, if you are experiencing them frequently, it may be worth investigating further to see if there can be improvements in the transaction processing. Topic 1.4: Preventing DeadlocksHere are several strategies for preventing deadlocks: Order Your Locks: Always lock tables in the same order. For example, if all your transactions lock the ‘orders’ table before the ‘products’ table, you won’t have one transaction locking ‘orders’ and a second transaction locking ‘product’ and waiting for ‘orders’. Keep Transactions Short and Fast: The shorter a transaction, the less likely it is to lock a row that’s needed by another transaction. Error Handling: Since InnoDB automatically detects deadlocks and rolls back a transaction, you need to be ready to catch that error in your code and retry the transaction. Use Lower Isolation Levels: If possible, use the Read Committed isolation level rather than Repeatable Read to lessen the chance of deadlocks. Avoid Hotspots: If you can avoid frequently updated rows, you can reduce the likelihood of deadlocks. For instance, instead of having a counter table that gets updated each time an operation is performed, consider using a different strategy to count operations. Topic 1.5: Resolving DeadlocksWhen it comes to resolving deadlocks, the ideal circumstance is that deadlocks are detected and handled automatically by MySQL’s InnoDB storage engine. InnoDB uses a mechanism known as&nbsp;wait-for graph&nbsp;to detect deadlocks. When a deadlock happens, InnoDB chooses one of the transactions and kills it, thereby resolving the deadlock. While this takes care of resolving the deadlock, it is important for application developers to handle these scenarios within the application. When InnoDB kills a transaction due to a deadlock, it raises an error that needs to be caught in your application, and, typically, the transaction that was terminated should be retried. The automatic deadlock detection in InnoDB resolves deadlocks as they happen, but in certain cases, the detection and killing of a transaction can take substantial time, impacting your application’s performance. That is why it is also important to design your application to avoid deadlocks as much as possible. Although it’s hard to prevent deadlocks entirely in high concurrency systems, trying to minimize them as much as possible will lead to a smoother and more efficient operation of your database system. Good coding practices, efficient design of your tables, and correctly applying transactions and lock controls, can help you avert most deadlocks. Topic 1.6: Troubleshooting DeadlocksInvestigating and troubleshooting deadlocks can provide valuable insights to prevent them or improve the response time. MySQL includes several tools that can assist in this process. SHOW ENGINE INNODB STATUS: This command outputs a text-based report that includes information about the most recent deadlock if one has occurred. It’s essential to run this command as soon as possible after the deadlock because its information is lost with the next one. InnoDB Monitors: These are more detailed and extensive reports about InnoDB’s internal workings, including deadlocks. There are standard, lock, and mutex monitors. Performance Schema: MySQL’s Performance Schema can be configured to capture detailed data about events, including transaction events. This data is stored in tables and can be queried like other MySQL data. Binary Logs: MySQL’s binary logs can help determine the sequence of queries that led to the deadlock. These logs require that you have the binary log enabled and that you’re logging in ROW format. Error Log: Deadlocks are logged here if you have innodb_print_all_deadlocks configuration enabled. By analyzing these sources, you can determine which transactions were involved in a deadlock and what resources they were trying to access. In many cases, careful analysis may point to a better way to order locks or a better transaction size to prevent foreseeable deadlocks. Topic: 1.7&nbsp;Review and AssessmentsExample Problem:Assume that you encounter a deadlock in your MySQL database. You decide to run the command&nbsp;SHOW ENGINE INNODB STATUS&nbsp;for more information. The&nbsp;LATEST DETECTED DEADLOCK&nbsp;section gives you the following output: 12345678910111213141516171819202122LATEST DETECTED DEADLOCK ------------------------ 2022-08-24 23:08:02 7f3e6e2fd700 *** (1) TRANSACTION: TRANSACTION 118945420, ACTIVE 22 sec inserting mysql tables in use 1, locked 1 1700 lock struct(s), heap size 187648, 1249789 row lock(s), undo log entries 1 MySQL thread id 155, OS thread handle 0x7f3e6e3e7700, query id 25749768 localhost user INSERT INTO customer (id, name, address) VALUES (3, 'John Doe', '123 Main St') *** (1) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 66873 page no 70541 n bits 600 index `id` of table `test`.`customer` trx id 118945420 lock mode S waiting up to 3 years total: 47.56T, and currently at 47.68T to rise above: 47.68T *** (2) TRANSACTION: TRANSACTION 118945416, ACTIVE (PREPARED) 13 sec committing, thread declared inside InnoDB 476 mysql tables in use 1, locked 1 1 lock struct(s), heap size 368, 0 row lock(s) MySQL thread id 117, OS thread handle 0x7f3e6e2fd700, query id 25749765 localhost user COMMIT *** (2) HOLDS THE LOCK(S): RECORD LOCKS space id 66873 page no 70541 n bits 600 index `id` of table `test`.`customer` trx id 118945416 lock mode S *** (2) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 66873 page no 70541 n bits 600 index `id` of table `test`.`customer` trx id 118945416 lock_mode X waiting up to 3 years total: 47.68T to rise above: 47.56T *** WE ROLL BACK TRANSACTION (1) Based on this information, what is causing the deadlock and how could you potentially resolve it? Now let’s test your knowledge. Question 1 — Simple Familiar Problem (Difficulty 3/10): What are the four conditions that must be present for a deadlock as per Coffman’s conditions? Provide a brief explanation of each condition. Question 2 — Complex Familiar Problem (Difficulty 6/10): What steps would you take to proactively prevent or minimize deadlock occurrences? Question 3 — Complex Unfamiliar Problem (Difficulty 9/10): Why does MySQL InnoDB engine automatically resolve deadlocks by rolling back a transaction? What are the advantages and potential disadvantages of this approach? Please always try to get to the answer by yourself first before asking for help. Solution for Example Problem: The deadlock occurred because two transactions were each waiting for resource more than the individual ones. The transaction (1) was waiting for an S-mode (read) lock on the&nbsp;id&nbsp;index of the&nbsp;customer&nbsp;table, that was being held by transaction (2). At the same time, transaction (2) was waiting for an X-mode (write) lock on the very same resource while transaction (2) held an S-mode (read) lock. This scenario means that neither transaction could proceed, resulting in a deadlock. The resolution could be to ensure the transactions request locks in the same order, as it is one of the well-known techniques to avoid deadlock. Solution to Question 1: Coffman’s deadlock conditions are as follows: Mutual Exclusion:&nbsp;A resource can only be held by one process at a time. Hold and Wait:&nbsp;Processes already holding resources can request new resources. No Preemption:&nbsp;Only the process holding the resource can release it. Circular Wait:&nbsp;A circular chain of processes exists where each process holds a resource needed by the next process in the chain. Solution to Question 2: Multiple approaches can be deployed to prevent or minimize deadlocks: Setting Lock Timeout:&nbsp;By limiting how long a transaction waits to acquire a lock, you can minimize wait time and potentially resolve deadlocks. Ordering Locks:&nbsp;Having transactions request locks in a specific order can prevent deadlocks by eliminating the circular wait condition. Frequent Committing:&nbsp;Smaller transactions that commit frequently are less likely to clash with other transactions. Deadlock Detection Tools:&nbsp;Utilize built-in or third-party tools to determine when and why deadlocks occur so programmatic resolutions can be applied. Solution to Question 3: When a deadlock occurs in MySQL, the InnoDB engine automatically chooses the smallest transaction (the one with fewest row locks) and rolls it back to break the deadlock Advantages: This automatic resolution helps to keep the database running smoothly without manual intervention. Potential Disadvantages: If the transaction that is rolled back is large or critical, rolling it back might lead to data not being updated or saved correctly. Continuous rollbacks due to deadlocks can negatively affect performance. Not all business logic scenarios may suit automatic rollback. 中文文章: https://programmerscareer.com/zh-cn/mysql-interview6/Author: Wesley Wei – Twitter Wesley Wei – MediumNote: If you choose to repost or use this article, please cite the original source.","link":"/mysql-interview6/"},{"title":"MySQL interviews: What is an SQL injection attack? How can such attacks be prevented?","text":"Here’s a proposed curriculum to study SQL injection attacks in MySQL and how to prevent them Thank you for reading this article. More Interview Questions here:https://programmerscareer.com/software-interview-set/ Topic: Introduction to SQL Injection AttacksSQL Injection, commonly referred to as SQLi, is one of the most notorious types of web application security vulnerabilities. It occurs when an attacker can insert malicious SQL statements into an entry field for execution or manipulation. Essentially, an SQLi attack takes advantage of a site’s vulnerable user inputs where SQL commands are parsed. This attack can lead to unauthorized viewing of user lists, deletion of entire tables, and, in some cases, the attacker could gain administrative rights to a database, all through running malicious SQL statements on your database. SQLi is relatively easy to prevent but still happens quite frequently, with devastating effects. An attacker can manipulate your SQL queries by inserting their own commands into a field that is incorporated into SQL statement. These attacks are successful when a web application doesn’t correctly validate input before it’s included in an SQL query. For example, imagine a simple login function where a user must input their username and password. The SQL query related to this function might look something like this: 1SELECT * FROM users WHERE username = '[username]' AND password = '[password]'; In this scenario, the attacker could submit “admin'; --“ as their username. Your query would then look like this: 1SELECT * FROM users WHERE username = 'admin'; --' AND password = '[password]'; In SQL, anything after&nbsp;--&nbsp;is considered a comment and is ignored. So essentially, the attacker has successfully bypassed the password check and can log in as the&nbsp;admin. Remember, this is just the simplest form of an SQLi — there exist much more complex SQLi methods that can have much more devastating impacts. Topic: Examples of SQL Injection AttacksSQL injection attacks come in all shapes and sizes. Here are some common examples: 1. Retrieving hidden data:You can manipulate an SQL query to return additional results. Let’s assume we have a page displaying products filtered by category: 1https://example.com/products?category=Books which might be using this SQL query: 1SELECT * FROM products WHERE category = 'Books' An attacker could change the URL to: 1https://example.com/products?category=Books' OR '1'='1 which might manipulate the SQL query to: 1SELECT * FROM products WHERE category = 'Books' OR '1'='1' Since ‘1’=’1’ is always true, it results in displaying all products, not just books. 2. Subverting application logic: Let’s consider another scenario where an application checks a user’s login credentials using the following code: 1SELECT * FROM users WHERE username = '$username' AND password = '$password' A hacker can use SQLi to bypass the password check with the following inputs: 12' OR '1'='1' -- for username randompassword for password This would look like: 1SELECT * FROM users WHERE username = '' OR '1'='1' --' AND password = 'randompassword' The ‘ — ‘ comments out the password checking portion of the SQL query, leading to an unauthorized login. 3. UNION attacks: UNION operator can be used to retrieve data from other tables within the database. Let’s take an example of this URL: 1https://example.com/products?category=Books If the suspecting query is: 1SELECT price, name, description FROM products WHERE category = 'Books' The attacker might try: 1https://example.com/products?category=Books' UNION SELECT username, password FROM users -- That could result in leaked user credentials. Remember, these attacks heavily depend on the backend query structure and protection mechanisms in place. Not all websites or databases will be susceptible to these exact attacks. However, these examples should give you an idea of how SQLi exploits incorrect handling of user-supplied data to manipulate SQL queries. Topic: Understanding the Impact of SQL InjectionSQL injection can lead to various harmful outcomes, and the severity of damage largely depends on the privileges of the user account that the attacker has abused and what the database is used for. Here are some potential impacts of SQL injection attacks: 1. Data Breach: One of the most direct and dangerous outcomes of an SQL injection attack is a data breach. If an attacker successfully exploits an SQL injection vulnerability, they might gain access to sensitive data stored in your database. This could include personally identifiable information (PII), financial data, proprietary business information, passwords, or more. 2. Data Manipulation: SQL injection isn’t just about viewing data. An attacker could use it to modify or delete data in your database — this could range from altering prices or balances to deleting entire tables. 3. Loss of Accountability and Non-Repudiation: Since SQL injection can allow an attacker to execute actions on your database under the guise of another user (or even an admin), it could lead to a loss of accountability. It would be difficult to trace actions back to the attacker, creating a non-repudiation issue. 4. Damage to Reputation: Beyond the direct technical outcomes, a successful SQL injection attack could significantly damage your business’s reputation. Customers trust businesses with their data, and a breach could lead to a loss of that trust. 5. Legal Consequences: Depending on the nature of the breached data and the jurisdiction, an SQL injection attack could also lead to legal consequences for the business. This could include fines, lawsuits, or both. As you can see, the potential consequences of an SQL injection attack highlight the paramount importance of protecting against them. In the upcoming lessons, we’ll dive into how to do exactly that. Topic: Preventing SQL Injection AttacksPreventing SQL injection attacks is all about ensuring that the data flow between your application and your database is safe and secure. Here are some techniques that can be employed to help prevent SQL Injection attacks: 1. Use Prepared Statements (Parameterized Queries): The most effective way to prevent SQL injection is to use prepared statements. A prepared statement defined with parameters ensures that the parameters are bound to the query and are not part of the query, which means an attacker can’t affect the query structure. This effectively eliminates all SQL injection attacks. Most web language nowadays has support for prepared statements. 2. Use Stored Procedures: Much like a prepared statement, stored procedures also separate data from commands and queries. However, stored procedures have added benefits like improved performance and reusable code. 3. Input Validation: While this approach alone is not enough to prevent SQL injection attacks, it’s still an essential step. By validating user input, we ensure it meets length, type, syntax, and business rules specifications. 4. Least Privilege Principle: Don’t give a user account more privileges than it needs. If an account is only used to perform select statements within an application, don’t give it the ability to drop tables. If an attacker compromises a limited account, the potential damage is contained. 5. Regular Updates and Patching: Keep your database management system (DBMS) and all your software updated and patched with the latest security fixes. These are a few prevention mechanisms that can be put in place to ensure your database’s safety against SQL Injection attacks. Topic: Best Practices for Preventing SQL InjectionIn addition to those prevention mechanisms, here are more best practices that you can implement to avoid SQL injection vulnerabilities: 1. Escaping User Input: Escaping data simply means treating it in such a way that it’s interpreted as plain data, not as part of the SQL query. This can be manually done with certain functions to escape special characters like quotes, or it might be implicitly taken care of by using prepared statements. 2. Comprehensive Error Handling: Hackers often rely on error messages from the database to get clues about its structure. It’s best practice to avoid exposing these errors directly to the end user, instead, use a generic error message and log the specific error details in a secured file which developers can reference when needed. 3. Employ Web Application Firewalls (WAFs): Web application firewalls can inspect the incoming data and identify malicious SQL code. They don’t substitute good coding practices but serve as an additional line of defense. 4. Regular Code Reviews: Perform regular code reviews where security is one of the topics under scrutiny. This can help ensure secure coding practices are being followed and catch potential issues early in the development process. 5. Conduct Testing and Use Security Tools: Regularly test your application, database, and infrastructure for security vulnerabilities. There are many automated tools available which can scan for SQL injection and other vulnerabilities. Remember, security is a process, not a state. Regularly updating your skills and knowledge, keeping abreast of new vulnerabilities and attack techniques, and continually reviewing and improving your applications are all part of maintaining a robust security posture. Topic: Testing for SQL Injection vulnerabilitiesTesting for SQL injection vulnerabilities forms a critical part of securing your applications and databases. It can be done both manually and by using automated tools. Here’s how: 1. Manual Testing: Manual tests involve using techniques like injecting special characters into your application’s inputs and observing the application’s reaction. For example, inputting a single quote mark&nbsp;'&nbsp;into a text field. If your application throws an SQL error, that’s a sign it might be vulnerable to SQL injection. On the other hand, if the application runs smoothly and the character appears in the output as it was entered, that’s an indication your application is properly handling input. Remember that SQL injection can come in many different forms, and thorough manual testing might involve trying a wide range of inputs. 2. Automated Testing with Security Tools: There are also various automated tools designed to assist with SQL injection detection. These tools can crawl your application and test various inputs for SQL injection vulnerabilities, saving time and providing a thorough assessment of your application. They can test known SQL injection techniques and generate reports of potential vulnerabilities. Examples of such tools include SQLMap, Havij, and Netsparker. While tests and automated tools can be extremely useful, they aren’t infallible. Even if a tool doesn’t find any vulnerabilities, that doesn’t necessarily mean your application is secure. It’s recommended to couple these methods with the prevention techniques and best practices we discussed in the previous lessons. Topic: Review and Assessments on SQL InjectionReview: SQL Injection Attack:&nbsp;This is an attack method where an attacker tries to manipulate an SQL query by injecting malicious SQL code through user input. Preventive Measures:&nbsp;The primary mechanisms include prepared statements, stored procedures, input validation, enforcing least privilege, and regularly updating systems. Best Practices:&nbsp;These include escaping user input, comprehensive error handling, employing Web Application Firewalls (WAFs), regular code reviews, and using security tools. Testing for Vulnerabilities:&nbsp;Manual testing might involve trying a wide range of inputs. Automated tools like SQLMap also assist in detecting potential vulnerabilities. Now, let’s move on to the assessment. Assessment:Below, I will provide a few short questions. These questions are designed to test your understanding and application of the concepts we’ve discussed. What are the distinguishing signs that indicate a system may be vulnerable to an SQL Injection attack? How can prepared statements be used to prevent SQL Injection attacks? Why are comprehensive error handling and not exposing database errors to end-users important? Describe at least two best practices that should be followed to protect against SQL Injection attacks. What are the distinguishing signs that indicate a system may be vulnerable to an SQL Injection attack? Answer: Systems may be prone to SQL Injection if they directly use input in SQL queries without proper cleansing or validation, expose database errors to end-users, or do not use parameterized queries or prepared statements. Signs might include unexpected behavior from user inputs such as a single quote causing errors, or certain inputs providing access or data retrieval that shouldn’t be possible. How can prepared statements be used to prevent SQL Injection attacks? Answer: Prepared statements separate SQL query structure from the data provided by the user. This stops attackers from manipulating the query structure because user input is not treated as part of the SQL command, making SQL injection attempts ineffective. Why are comprehensive error handling and not exposing database errors to end-users important? Answer: Exposing database errors to end users can provide attackers with useful information about your database structure or application design, which can be exploited for an attack. Besides, comprehensive error handling is important in preventing SQL Injection attacks as it allows systems to gracefully deal with issues that arise and can provide logging or other mechanisms to alert about possible attacks. Describe at least two best practices that should be followed to protect against SQL Injection attacks. Answer: Some best practices include: Use of Prepared Statements or Parameterized Queries: These ensure user-provided input can’t alter the SQL query structure undesirably. Input Validation: Inputs should be validated as being of the correct form before they are used. For example, if the system expects an integer, it should confirm the input is indeed an integer. Other best practices can include proper error handling, regular system updates, adoption of least privilege principles, and more. 中文文章: https://programmerscareer.com/zh-cn/mysql-interview7/Author: Wesley Wei – Twitter Wesley Wei – MediumNote: If you choose to repost or use this article, please cite the original source.","link":"/mysql-interview7/"},{"title":"MySQL interviews: When doesn’t MySQL use the index?","text":"Have you ever been asked a similar question in an interview? Or you will meet in the future, let’s explore and master it together Thank you for reading this article. More Interview Questions here:https://programmerscareer.com/software-interview-set/ Topic: MySQL Index Deep DiveDetailed analysis of indexes in MySQL Indexes in MySQL play an essential role in optimizing database search performance. To get a more in-depth understanding of indexes, let’s break down the concept further. What exactly is an index in MySQL?Think of an index as the equivalent of a book’s index. In a book, if you needed to find information, you’d refer to the index, which would quickly guide you to the relevant page containing the information you require. In MySQL, an index is a data structure (most commonly a B-Tree) that improves the speed of data retrieval operations on a database table. Indexes are used to quickly locate data without having to search every row in a database table every time a database table is accessed. They can be created using one or more columns of a database table, providing a basis for both rapid random lookups and efficient ordering of access to records. Benefits of using indexes in MySQL: Faster Data Retrieval:&nbsp;The primary advantage of using indexes is the speed they bring to data retrieval. Indexes enable the database application to find the record much faster. Order By Optimization:&nbsp;Indexes can also optimize the order by clause to sort the data more quickly. Better Performance:&nbsp;They significantly improve the performance of SELECT queries and WHERE clauses. However, keep in mind that while indexes speed up data retrieval, they slow down data modification operations like INSERT, DELETE, and UPDATE. This is because each time we modify data in the table, the indexes need to be updated as well. Topic: Types of Indexes in MySQLUnderstanding the different types of indexes available in MySQL like PRIMARY, UNIQUE, INDEX, FULLTEXT, and their usage. There are several types of Indexes available in MySQL. Each type is used in specific scenarios and offers its own set of advantages. Let’s take a closer look at these different types: PRIMARY Index:&nbsp;This is the main index of a table. Each MySQL table should have a primary index. The value in this index is unique and not null. In most cases, this is the primary key of your table. UNIQUE Index:&nbsp;As the name suggests, this index prevents two records from having identical values in a particular column. This helps maintain data integrity. It allows null values, but the uniqueness is still enforced. INDEX (or KEY):&nbsp;This is the simplest kind of index. It allows duplicates and null values. It’s generally used when you want to improve the performance of certain searches, and you don’t need to enforce uniqueness or non-nullability. FULLTEXT Index:&nbsp;This is an index that’s used for full-text searches. It’s a very powerful index type for searching text values. However, it’s a specialized type of index and only applies to text (not numbers or dates). Each of these indexes serves a specific purpose. The optimal usage of these indexes can greatly enhance the performance of your database. Understanding when and where to use the appropriate index is crucial. It depends on various factors like the data stored, the integrity needed, the type of queries executed, and many others. Topic: MySQL Index ManagementFamiliarizing ourselves with how to manage indexes in MySQL is a crucial skill for optimizing database performance. There are several primary operations you can carry out when working with indexes: Creating an Index:To create an index in MySQL, you could use the&nbsp;CREATE INDEX&nbsp;statement: 12CREATE INDEX index_name ON table_name (column1, column2, …); For example, if we had a table named&nbsp;Students&nbsp;and we wanted to create an index on the&nbsp;LastName&nbsp;and&nbsp;FirstName&nbsp;columns, we would write: 12CREATE INDEX idx_students_name ON Students (LastName, FirstName); Dropping an Index:To delete an index in MySQL, you could use the&nbsp;DROP INDEX&nbsp;statement: 1DROP INDEX index_name ON table_name; Disabling an Index:In some cases, you might want to disable an index temporarily. MySQL allows you to disable an index using the&nbsp;ALTER TABLE&nbsp;statement: 1ALTER TABLE table_name DISABLE KEYS; To enable it again, you would use&nbsp;ENABLE KEYS: 1ALTER TABLE table_name ENABLE KEYS; Renaming an Index:MySQL does not support direct renaming of an index. To rename an index, you would have to drop (delete) the index and then create it again with the new name. Remember, while indexes are beneficial and may speed up data retrieval, they come with their own costs. Indexes take up storage space and slow down the speed of updating data (because when data is updated, indexes need to be updated as well). Topic: Optimizing MySQL with IndexesUnderstanding how to optimize MySQL performance using indexes is a crucial part of managing a database. One of the key factors that affect the efficiency of your data retrieval operations is the proper use of indexes. But how do you know if your indexes are efficient? Enter the&nbsp;EXPLAIN&nbsp;command. The&nbsp;EXPLAIN&nbsp;command in MySQL is a powerful tool to help understand how MySQL executes a query. By using the&nbsp;EXPLAIN&nbsp;command before a SQL statement, MySQL will return information about how it would process the statement and the order in which tables are read. Here’s an example: 1EXPLAIN SELECT * FROM Students WHERE LastName = \"Smith\"; The output may return a lot of data, but what one needs to focus on are mainly three columns: select_type:&nbsp;This tells us what type of select the query is. key:&nbsp;This tells us what index MySQL is using to run the query. rows:&nbsp;This tells us how many rows MySQL had to consider to give the result of the query. Another important thing to note is that different queries require different types of tuning. For example, a&nbsp;SELECT&nbsp;query that retrieves data may be optimized differently from an&nbsp;UPDATE&nbsp;query that modifies data. As a general rule, indexing the columns that you often use in your&nbsp;WHERE,&nbsp;JOIN,&nbsp;ORDER BY, and&nbsp;GROUP BY&nbsp;clauses can significantly increase the speed of your&nbsp;SELECT&nbsp;queries. However, keep in mind that indexes add overhead to the database when it comes to&nbsp;INSERT,&nbsp;UPDATE, and&nbsp;DELETE&nbsp;statements. Therefore, the number of indexes should be kept to a minimum to reduce overhead. This understanding of SQL indexes and how to use the&nbsp;EXPLAIN&nbsp;command to analyze query performance is vital for managing and optimizing your MySQL database. Topic: When MySQL Doesn’t Use The IndexEven though indexes are designed to optimize data retrieval, there are scenarios where MySQL doesn’t use an index even if one exists. Let’s dive into some of these scenarios: Small Tables:&nbsp;In tables where the total number of rows is less, the MySQL optimizer may ignore the index and execute a full table scan instead. This is because reading the index first and then fetching the row may involve more overhead than simply reading every row directly. Poor Selectivity:&nbsp;Indexes with low selectivity (meaning the indexed column has many duplicate values) are less effective. If an indexed column can’t sufficiently narrow down the rows, MySQL may skip using the index in favor of a full table scan. NULL Values:&nbsp;If the indexed column has NULL values and the WHERE clause is IS NULL or IS NOT NULL, MySQL can’t use an index because comparison operators do not work with NULL. Not using the leftmost prefix of the index:&nbsp;MySQL can use an index efficiently when the WHERE clause uses the leftmost part of a multi-column index. However, when the query does not involve the leftmost part, MySQL can’t use the index effectively. LIKE Operator with Wildcards at the Start:&nbsp;When you use a LIKE operator in a WHERE clause and the pattern starts with a wildcard, MySQL can’t leverage the index. Remember, indexes are a great tool for optimizing data retrieval, but they are not always used. A solid understanding of how and when indexes are utilized by MySQL can inform your database design and query construction. Topic: Review and AssessmentsWe have now reached the end of our curriculum. Before we wrap up, let’s do a quick recap: We deep-dived into the concept of MySQL indexes. We learned about the different types of indexes in MySQL. We understood how to manage indexes in MySQL. We saw how indexes can be used to optimize MySQL performance and how to evaluate this performance using the&nbsp;EXPLAIN&nbsp;command. And, we delved into scenarios when MySQL may not use an index. Now, it’s time to put your knowledge to the test. Here are a few thought-provoking questions: What is the main purpose of an index in MySQL, and how does it enhance database performance? Can you describe two different types of indexes in MySQL and when you would use them? How would you disable and then re-enable an index in MySQL? In what scenario would MySQL choose not to use an index even when it’s available? Can you draft a SQL query using an EXPLAIN statement when querying data from a&nbsp;employees&nbsp;table based on an&nbsp;employee_id? Try to write down your answers, then compare them to your notes or any resources you’ve used throughout this journey. Reviewing your knowledge like this will reinforce your learning immensely. Remember, mastering any subject is a matter of time, practice, and consistent effort. Keep striving! What is the main purpose of an index in MySQL and how does it enhance database performance? An index in MySQL is a data structure that improves the speed of data retrieval operations on a database table. It works similarly to an index in a book, enabling the database to find data without scanning each row of the database every time a database table is accessed. Indexes can dramatically improve database performance. Can you describe two different types of indexes in MySQL and when you would use them? PRIMARY index: This is a type of unique index where a column must contain unique and not null values. This is typically used for the primary key. FULLTEXT index: This is used for full-text searches. It’s dedicated to the search of text through SQL queries where the search in the string localization pattern is complex. How would you disable and then re-enable an index in MySQL? To disable non-unique indexes:&nbsp;ALTER TABLE table_name DISABLE KEYS; To enable non-unique indexes:&nbsp;ALTER TABLE table_name ENABLE KEYS; In what scenario would MySQL choose not to use an index even when it’s available? MySQL may choose not to use an index if it estimates that using the index would be slower than a full table scan. For example, if the table is small or the indexed column has very low selectivity (many repeated values), MySQL may prefer a full table scan. Can you draft a SQL query using an EXPLAIN statement when querying data from a&nbsp;**employees**&nbsp;table based on an&nbsp;**employee_id**? Here’s a basic example: 1EXPLAIN SELECT * FROM employees WHERE employee_id = 101 Remember, practice is key in mastering these concepts. Don’t hesitate to dive deeper and try out these commands in a real MySQL environment to explore further! 中文文章: https://programmerscareer.com/zh-cn/mysql-interview8/Author: Wesley Wei – Twitter Wesley Wei – MediumNote: If you choose to repost or use this article, please cite the original source.","link":"/mysql-interview8/"},{"title":"MySQL interviews: How to tune MySQL performance","text":"Let we dive into tuning MySQL performance Thank you for reading this article. More Interview Questions here:https://programmerscareer.com/software-interview-set/ Topic: Understanding MySQL ConfigurationOne of the key aspects of tuning MySQL performance is understanding and appropriately adjusting its configuration. But first, let’s understand what we mean by “configuration” in MySQL. MySQL Configuration&nbsp;refers to the set of parameters and settings that dictate how MySQL operates. Some of these settings include variables controlling memory allocation, table cache size, and sort buffer size. The configuration file for MySQL is typically named&nbsp;my.cnf, though the exact location of this file can vary depending on the operating system and MySQL version. The&nbsp;my.cnf&nbsp;file contains several sections. The&nbsp;[mysqld]&nbsp;section contains server-specific settings, while the&nbsp;[client]&nbsp;section contains client-specific options. Any modifications to the settings in this file will affect the MySQL server’s operations. Let’s consider a few important settings: innodb_buffer_pool_size:&nbsp;This is a crucial setting if you’re using InnoDB storage engine. The buffer pool is where data and indexes are cached, so setting it appropriately can significantly improve performance. max_connections:&nbsp;This determines how many concurrent connections MySQL can handle. Be cautious, though — a high number here can cause MySQL to use up available resources quickly. query_cache_size:&nbsp;Query caching can help speed up response times, but it has to be used judiciously since it adds additional overhead to all query processing. To make changes, you would edit these parameters in the&nbsp;my.cnf&nbsp;file and then restart MySQL for the changes to take effect. Understanding these configurations and how to fine-tune them based on your specific use case can greatly enhance your MySQL server’s performance. Topic: Tuning Server SettingsNow that we understand the fundamentals of MySQL configuration, let’s delve into some essential server settings that you can adjust to optimize the performance. MySQL has a vast array of server variables you can set to affect its operation. It allows for a high degree of customization and tuning to suit the specific application and hardware environment. Here’s a look at some of the key variables: 1. key_buffer_size:&nbsp;This variable determines the amount of memory allocated for caching MyISAM tables indexes in memory. If you’re using MyISAM tables, large values might improve performance. 2. innodb_buffer_pool_size:&nbsp;As mentioned earlier, this setting is critical for systems using the InnoDB storage engine. It specifies the size of the memory buffer InnoDB uses to cache data and indexes of its tables. 3. thread_cache_size:&nbsp;This variable is used to specify how many threads the server should cache for reuse. When a client disconnects, the client’s threads are put in the cache, and if clients connect, they can reuse the threads in the cache. 4. table_open_cache:&nbsp;This variable sets the number of open tables for all threads. Increasing this value increases the number of file descriptors that mysqld requires. 5. query_cache_size:&nbsp;It sets the size of the query cache. It is used to cache SELECT results and later return them without actual executing the same query once again. Remember that configuration and tuning MySQL is more of an art than a science, consisting largely of incremental changes and continuous monitoring. Using tools like MySQLTuner or MySQL Workbench can be very handy to monitor your MySQL server’s performance and make necessary adjustments. Topic: Query OptimizationOptimizing your SQL queries is one of the most effective ways to improve your MySQL server’s performance. Here are some strategies that could help: Avoid Full Table Scans:&nbsp;Try your best to avoid full table scans by using indexes appropriately (more on this later). MySQL must read the entire table to find the relevant rows when a full table scan occurs, which can kill performance when dealing with large tables. Use EXPLAIN to Analyze Query Performance:&nbsp;The EXPLAIN statement in MySQL provides information about how MySQL executes queries. It is a beneficial debugging and optimization tool. Take Advantage of Indexes:&nbsp;Indexes are utilized to find rows with specific column values quickly. Without an index, MySQL must scan the entire table to locate the relevant rows. Limit Your Results:&nbsp;If you don’t need the entire result set for your operations, the use of LIMIT can significantly reduce the cost of a query. Normalize and De-normalize — As Needed:&nbsp;While normalization is generally a good thing as it reduces data redundancy, de-normalization may actually help in certain situations. For instance, if there are large tables and primary-key-to-primary-key joins, the joins might be slow. De-normalization might speed up such queries by allowing you to bypass those joins. Topic: Optimizing Schema ObjectsWhile server and query optimization, which we previously discussed, are crucial for MySQL performance, schema optimization is also a vital factor that shouldn’t be overlooked. Here’s what you should know: 1. Table Type Selection:&nbsp;MySQL supports several storage engines, each with its pros and cons. The most commonly used are MyISAM and InnoDB. MyISAM is more straightforward and often performs better for read-heavy loads. In contrast, InnoDB supports advanced features like transactions and foreign keys and is better suited for write-heavy loads. 2. Indexing:&nbsp;Indices are used to quickly locate data without needing to search every row in a database table each time a database table is accessed. They can speed up data retrieval but should be used judiciously, as each index you add increases the cost of inserting, updating, and deleting records. 3. Normalization:&nbsp;It is the process of efficiently organizing data in a database with two goals — eliminate redundant data (for example, storing the same data in more than one table) and ensure data dependencies make sense. While normalization often improves performance, in some cases, you might need to consider denormalization for optimization purposes. 4. Partitioning:&nbsp;Partitioning can be a powerful tool for improving performance in databases with large tables. It works by splitting a large table into smaller, more manageable pieces called partitions. Each partition stores a certain subset of your table’s data defined by a partitioning function. Remember, the schema optimization should be tailored to the specific needs of your application and the characteristics of your workload. Make sure to monitor your database’s performance over time and adjust your schema as your application’s requirements evolve. Topic: Hardware Consideration and TuningMySQL performance can significantly be influenced by hardware components such as CPU, memory, disk storage, and network. Let’s take a closer look: 1. CPU:&nbsp;The faster your server’s CPU can process data, the faster your MySQL performance will be. One technique is to balance the load across CPUs, a feature which MySQL supports. 2. Memory:&nbsp;A MySQL server uses a lot of memory. The more memory you have, the more data you can cache, and the fewer disk I/O operations MySQL needs to perform, the more efficient it becomes. Consider adjusting the innodb_buffer_pool_size, which is the memory area that holds cached InnoDB data for both tables and indexes. 3. Disk Storage:&nbsp;The type of disk you use affects MySQL performance. Solid State Drives (SSDs) typically have faster data access times compared to traditional Hard Disk Drives (HDDs). Remember that writing to disk is much slower than reading from memory. Therefore, it’s crucial to have enough memory to minimize disk writing operations. 4. Network:&nbsp;Network latency can impact MySQL performance, especially in distributed systems where MySQL instances need to communicate over the network. Upgrading to a faster network technology can lower this latency and improve overall performance. Optimizing hardware to cater to your MySQL database’s needs can significantly boost its performance. Remember to monitor your database over time to assess how well your hardware serves your needs. Sometimes, certain limitations can only be overcome by upgrading or adding more hardware resources. Topic: MySQL Replication &amp; PartitioningMySQL Replication&nbsp;and&nbsp;Partitioning&nbsp;are two powerful tools in a developer’s arsenal to enhance database performance. Let’s break them down: 1. MySQL Replication: MySQL replication is a process that enables you to maintain multiple copies of MySQL data by having them copied automatically from a master to a slave database. This can be helpful for many reasons, including facilitating a backup of data, creating a failover environment, or segregating the database in order to simplify routine tasks. Replication can also improve performance for high demand applications by distributing the load among multiple slave nodes. 2. MySQL Partitioning: Partitioning is a process where large tables are divided into smaller ones, improving query performance and simplifying management tasks. MySQL has the capacity to divide tables into partitions, storing them separately. There are various ways to partition your data: Range Partitioning:&nbsp;The table is partitioned into ranges determined by the partitioning function. The values falling into a range are stored in an assigned partition. For example, you might partition sales data into monthly ranges. List Partitioning:&nbsp;Similar to range partitioning, but the partition is selected based on columns matching one of a set of discrete value lists. Hash Partitioning:&nbsp;The partitioning function generates a hash value, which determines the partition. Key Partitioning:&nbsp;Similar to hash partitioning, but only certain column types can be used (integer, binary, or string types). Composite Partitioning:&nbsp;A mixture of range or list partitioning with hash or key partitioning. Understanding these concepts and implementing them can greatly enhance your database’s performance. Moreover, it also allows for more robustness and scalability in your systems. Topic: Review &amp; AssessmentsAt this stage, it is important to recap everything that we have discovered so far. 1. MySQL Configuration: We’ve learnt the importance of MySQL configuration and how to tune various parameters to improve performance. Each MySQL installation is unique, and fine-tuning it to suit individual needs can significantly boost performance. 2. Tuning Server Settings: We delved into the different settings we can tune, related to memory, caches, and various other server settings for optimal efficiency. 3. Query Optimization: We understood how MySQL processes queries and ways to write them more efficiently to save resources. 4. Optimizing Schema Objects: The importance of optimizing MySQL schema objects such as tables and indexes for better performance was highlighted, along with techniques of doing so. 5. Hardware Consideration and Tuning: We learned about the impact of hardware parameters such as CPU, memory, disk, and network on MySQL performance and how hardware resources can be optimized for better performance. let’s go ahead with the assessments. 6. MySQL Replication and Partitioning: We explored the concepts of MySQL replication and partitioning, understanding their utility in enhancing database performance. Assessment 1: MySQL Configuration What is meant by MySQL configuration and why is it significant in database management? Assessment 2: Server Settings Can you explain how changing server settings can help in optimizing the performance of a MySQL database? Assessment 3: Query Optimization What strategies can you employ to optimize your SQL queries? Assessment 4: Schema Objects Discuss the importance of optimizing schema objects in MySQL. Assessment 5: Hardware Considerations How does hardware impact MySQL performance? What parameters can be tuned for optimization? Assessment 6: Replication and Partitioning Explain the concepts of MySQL replication and partitioning. How do these techniques aid in enhancing database performance? Please take some time to answer these questions Assessment 1: MySQL Configuration MySQL configuration involves customizing settings that determine how MySQL operates. It’s significant because the right configuration can ensure efficient use of system resources, faster query execution, and generally improved performance. Assessment 2: Server Settings Changing server settings, such as increasing buffer sizes or limiting the maximum connections, can greatly improve MySQL performance. For instance, increasing the buffer size can allow more data to be stored in memory, reducing disk I/O. However, all changes should be made considering the cargo of the system resources. Assessment 3: Query Optimization To optimize SQL queries, one can use strategies such as selecting only necessary fields, using joins appropriately, creating indexes for faster search, using limit where possible, and avoiding the use of wildcard ‘%’ at the beginning of a query. Assessment 4: Schema Objects Optimizing schema objects can significantly speed up queries. Techniques such as appropriate indexing, choosing suitable data types, and normalizing the database can improve performance. Assessment 5: Hardware Considerations Hardware directly impacts MySQL performance. The CPU speed affects how fast queries are processed, and plenty of memory allows for larger caches and buffers — reducing disk I/O. Faster disks also decrease I/O wait time. To tune it, you can add more memory, use faster disks, or even distribute your database across several machines. Assessment 6: Replication and Partitioning MySQL replication is a process where data from one MySQL database server (the master) is replicated to one or more MySQL database servers (the slaves). It increases data security, improves performance for high-volume sites, and is part of a good backup strategy. Partitioning is a database design technique which is used to improves performance, manageability, simplifies maintenance and reduce the cost of storing large amounts of data. It works by breaking down a single table, which is stored in one file or area of disk, into smaller members, each of which can be managed and accessed individually. 中文文章: https://programmerscareer.com/zh-cn/mysql-interview9/Author: Wesley Wei – Twitter Wesley Wei – MediumNote: If you choose to repost or use this article, please cite the original source.","link":"/mysql-interview9/"},{"title":"Redis interviews: The application, advantages and disadvantages of skiplists in Redis","text":"Have you ever been asked a similar question in an interview? Or you will meet in the future, let’s explore and master it together Thank you for reading this article. More Interview Questions here:https://programmerscareer.com/software-interview-set/ Topic: 1.1 Introduction to Skip ListsSkip lists are fascinating data structures. They were designed with simplicity and speed in mind. A&nbsp;Skip List&nbsp;is a probabilistic data structure that allows efficient search, insertion, and removal operations. It’s quite similar to a sorted linked list, but the genius of Skip Lists lies in how they enhance the speed of the operations. The primary idea of a Skip List is to “skip” a significant number of elements rather than traversing a linked list to find an element. It uses a hierarchy of linked lists that connect progressively with a fraction of the elements. This fraction reduces as we climb the Skip List hierarchy, which gives us an efficient search operation. Skip Lists shine in big data scenarios. They have an average-case and worst-case search and insertion time complexity of O(log n), which makes them super efficient! While they may not have the same popularity as more common data structures, Skip Lists have significant applications, one of which includes being used in databases like Redis. The next lessons will help us delve deeper into how Redis leverages Skip Lists. Topic: 1.2 Skip Lists in RedisRedis, a well-known open-source, in-memory data structure project, implements skip lists in its codebase for certain use cases. One of the most notable is the Sorted Set data type. A Sorted Set in Redis is a set where every element is associated with a ‘score’. Despite how one could achieve this with a traditional hash map, the power of Sorted Sets is that they are always sorted by this score. This is where skip lists come in. Redis choses to implement this Sorted Set with a combination of a hash table and a skip list. The hash table allows Redis to quickly lookup an element in the set, and the skip list maintains the elements sorted by their scores, allowing for fast retrieval of ranges of elements, finding the rank of an element, etc. The union, intersection, and difference operations over Sorted Sets that involve multiple keys are also implemented with skip lists. Furthermore, when Redis needs to iterate over a large Sorted Set, it will use the skip list instead of the hash table to do so because of the improved efficiency. Skip lists provide efficient search and insertion operations which is crucial for the performance requirements of Redis. Topic: 1.3 The Application of Skip Lists in RedisRedis leverages skip lists extensively, particularly when it comes to sorted sets. But why did Redis choose skip lists considering there are many other data structures that could have been utilized, like binary search trees or AVL Trees? There are a few reasons for this. First, it comes down to simplicity. Skip lists are easier to implement and have fewer edge cases compared to balanced trees. They don’t require restructuring/redistribution (like tree rotations) after insertions and deletions, making them an appealing choice for a high-performance database like Redis. Due to their design, skip lists provide near-balanced tree performance without requiring balancing operations. While AVL Trees offer good performance, the balancing operation can become a bottleneck in heavy read-write situations, which are common in databases like Redis. Moreover, skip lists support quick insertion, deletion, and lookups with just a few level changes, making them an optimal choice for sorted data structures. The use of Skip Lists in Redis goes beyond sorted sets and into the internals of the Redis Cluster feature. Skip lists in Redis are used to handle the distribution of hash slots across different nodes in a Redis Cluster. This allows the Redis Cluster to quickly locate the right node to distribute a given piece of data, which increases the efficiency of data operations across the cluster. Remember, each technology makes decisions based on a range of factors including performance, functionality, simplicity, and so on. Redis’s decision to use skip lists is a fascinating example of the right tool for the right job! Topic: 1.4 The Advantages of Skip Lists in RedisThe use of skip lists in Redis offers several advantages, particularly when dealing with trimmed lists of items. Key benefits of using skip lists in Redis include: 1. Efficient Search Operations:&nbsp;Skip lists have logarithmic search times making them highly efficient for searching for elements. Instead of sequentially searching an item in a list, we can efficiently skip nodes resulting in faster search times. This makes Skip Lists particularly advantageous for Sorted Sets. 2. Simplicity of Implementation:&nbsp;Skip lists are simpler to implement than balanced search trees. A binary search tree, for instance, requires complex balancing after every insertion and deletion. Skip lists, on the other hand, maintain balance probabilistically, hence eliminating the need for complex rebalancing operations after every mutation. 3. Fast Insertion and Deletion Operations:&nbsp;Skip lists support quick insertions, deletions, and search operations. Especially in Redis, where data operations are frequent, the efficiency of these operations plays a vital role. 4. Efficient Range Queries:&nbsp;Skip Lists are especially efficient at range queries, a key requirement for sorted sets. For instance, fetching ranges, finding rank of elements, closest lower and higher rank items, etc., are much faster and simpler with skip lists. 5. Dynamic Resizing:&nbsp;Skip lists have an excellent feature of reorganizing themselves dynamically. When an element is added or removed, skip lists can rebuild their layers dynamically. These advantages have been crucial in reinforcing the performance of Redis, allowing it to handle large sets of data with speed and efficiency. Topic: 1.5 The Disadvantages of Skip Lists in RedisWhile skip lists provide numerous benefits for Redis, a few challenges can arise: 1. Space Usage:&nbsp;Skip lists tend to use more space than other data structures. Every node in a skip list maintains several pointers to other nodes, which increases the memory footprint. However, Redis addresses this by limiting the maximum number of levels a skip list node can have. 2. Randomness:&nbsp;One of the characteristics of a skip list is its probabilistic nature. The levels of the nodes of a skip list are chosen at random during insertion. While this randomization has benefits, it leads to the unpredictability of the skip list structure. 3. Not Ideal for Small Datasets:&nbsp;Skip lists excel when managing large, sorted datasets due to their logarithmic operation time complexity. However, for small datasets, the overhead of maintaining skip list pointers and the increased space usage may not be justified. 4. Difficulty in Understanding:&nbsp;While not a direct disadvantage, the concept of skip lists can be daunting for those unfamiliar with it. This can complicate the process of understanding and troubleshooting Redis performance. 5. Lack of Wide Use:&nbsp;Skip Lists are not as widely used or studied as hash tables, AVL trees, or B-trees. This can lead to a slightly higher difficulty in understating and making modifications to the data structure. Despite these challenges, Redis implements skip lists elegantly, gaining the benefits without suffering significant setbacks. Topic: 1.6 Review and Assessments of Skip Lists in RedisLet’s conduct a review of each section: 1.1 Introduction to Skip Lists:&nbsp;We discussed the basic structure and concept of skip lists, including where they are typically used and why. 1.2 Skip Lists in Redis:&nbsp;We focused on how Redis leverages skip lists, particularly when dealing with sorted sets. 1.3 The Application of Skip Lists in Redis:&nbsp;We dove deeper into the everyday use-cases of skip lists in a Redis environment, from simple sorted sets to the internals of Redis Cluster. 1.4 The Advantages of Skip Lists in Redis:&nbsp;We examined the major benefits of using skip lists, such as its efficiency in search, insertion, and deletion operations, simplicity in implementation, and dynamic resizing capability. 1.5 The Disadvantages of Skip Lists in Redis:&nbsp;We also addressed their downsides, including additional space usage, randomness, complexity, and the challenge these aspects pose in understanding, maintaining, and utilizing skip lists in Redis. Now, to further cement your understanding, I’m going to provide some short assessment questions: Can you explain why Skip Lists are used in Redis? How are Skip Lists advantageous in handling sorted sets in Redis? What challenges can arise while implementing Skip Lists in Redis? Question: Can you explain why Skip Lists are used in Redis?Answer: Skip Lists are used in Redis because they maintain elements in a sorted order with efficient operations like search, insertion, and deletion. This is important for operations like fetching ranges, establishing ranks of elements, and getting items of lower or higher rank. Question: How are Skip Lists advantageous in handling sorted sets in Redis?Answer: Skip Lists are advantageous in handling sorted sets in Redis due to their ability to perform range queries, and retrieval of the rank of elements, closest lower and higher rank items efficiently. This ability to quickly insert, delete, and search elements also plays a role in handling sorted sets. Question: What challenges can arise while implementing Skip Lists in Redis?Answer: Challenges that can arise while implementing Skip Lists in Redis include increased space usage because each node can maintain several pointers. Their probabilistic nature can lead to unpredictability of the skip list structure. They can be complex to understand for those unfamiliar with them, and their advantages might not be justified for small datasets. 中文文章: https://programmerscareer.com/zh-cn/redis-interview1/Author: Wesley Wei – Twitter Wesley Wei – MediumNote: If you choose to repost or use this article, please cite the original source.","link":"/redis-interview1/"},{"title":"Redis interviews: how to implement Distributed Locks with Redis","text":"Let’s draft a learning plan for Redis with a focus on implementing Distributed Locks. Thank you for reading this article. More Interview Questions here:https://programmerscareer.com/software-interview-set/ Topic: 1.1 Deep Dive into Redis.Redis, which stands for Remote Dictionary Server, is an open-source, in-memory data structure store used as a database, cache, and message broker. It has built-in replication, Lua scripting, LRU eviction, transactions, and various levels of on-disk persistence. Interestingly, Redis can handle multiple types of data structures like strings, hashes, lists, sets, sorted sets with range queries, bitmaps, and more. First, let’s discuss some of the&nbsp;core features of Redis. Performance: Redis holds its database entirely in memory and uses disk only for persistence, enabling it to process data very quickly. Persistence: Redis gives you the option with RDB (Redis DataBase file) and AOF (Append Only File) to persist your data either periodically or by logging every change. Atomic Operations: Redis operations like APPEND, INCR, etc., are atomic, meaning they’re completed entirely or not executed at all. This ensures data integrity, even in concurrent environments. Data Structures: Redis isn’t just a simple key-value store; it’s a data structures server that supports strings, hashes, lists, sets, and more. Pub/Sub Capabilities: Redis includes built-in commands for messaging and queueing systems, using the Publish/Subscribe paradigm. Scripting: Redis allows for scripting in Lua, effectively turning atomic commands into powerful scripts to process data on the server-side. Next is&nbsp;Redis data types. Redis supports a variety of data types: Strings: They are the simplest data type in Redis and can store any data, for example, a JPEG image or a serialized Ruby object. Lists: A list in Redis is a series of ordered values. Think of it as a linked-list. Sets: An unordered collection of strings with the addition and removal of items happening in constant time. A set can’t have repeated members. Sorted sets: Every member of Sorted Sets is associated with score, which is used to sort the set elements from smallest to largest score. Hashes: They are maps composed of fields associated with values, where both the field and the value are strings. Redis’s functionality and features make it a versatile system used in caching, session caching, full page cache, message queue applications, leaderboards and counting, real-time analytics, and much more. Topic: 1.2 Understanding Locks in Databases.As we’re gradually progressing toward understanding Distributed Locks with Redis, understanding the basic concept of locks in databases is essential. In databases, especially databases that allow concurrent transactions (simultaneous transactions), locks play a vital role in maintaining the consistency of data and preventing data anomalies. In simple terms, a&nbsp;lock&nbsp;in the context of a database is a mark or flag that the database assigns to a piece of data (which could be a row, a table, or even an entire database). This lock serves to control the access and modifications by concurrent transactions. Understand that locks are generally of two types:&nbsp;Shared Locks&nbsp;(S locks) — which allow read operations, and&nbsp;Exclusive Locks&nbsp;(X locks) — which allow write operations. Detailed explanation: Shared Locks&nbsp;are also referred to as ‘Read Locks’. If a shared lock is held on data, it can be read by the transaction holding the lock, but it cannot modify it. Other transactions can also acquire shared locks and read the data, but none can write into it. Thus, shared locks help maintain a level of consistency when the data is being read by ensuring that the data isn’t altered by any other transaction during the read operation. Exclusive Locks, on the other hand, are also known as ‘Write Locks’. If an exclusive lock is held on data, not only can the transaction read the data, it can also modify it. However, no other transaction can acquire any lock (shared or exclusive) on the same data. Exclusive locks, thus, serve to maintain data integrity by ensuring that no other transaction accesses the data while it is being modified. In the concept of “locking”, a major challenge is dealing with potential&nbsp;deadlocks, which is a state where two or more transactions are waiting indefinitely for each other to release resources. Solving deadlocks involves their detection and implementing approaches like ‘wait-die’ or ‘wound-wait’ schemes, which is a deeper topic. Topic: 1.3 The Need for Distributed LocksYou have already learned about the function of locks in databases. They provide a way to regulate access and prevent conflicts when many processes/transactions are trying to read and write to shared data. Now imagine a scenario where you aren’t working with a single database, but a distributed system. A&nbsp;distributed system&nbsp;is one where components located on networked computers communicate and coordinate their actions only by passing messages. In such an environment, merely using regular locks won’t suffice. Herein lays the necessity for&nbsp;distributed locks. A&nbsp;distributed lock&nbsp;or global lock allows multiple distributed processes to synchronize their operations, typically to prevent conflicts while accessing shared resources in a distributed system. In other words, it works across multiple systems or nodes in a network and ensures that only a single client can own a lock at a time, no matter where the client is in the network. Some high-level use cases of distributed locks are: In a&nbsp;microservices architecture, where multiple independent applications are communicating with each other, distributed locks can regulate access to shared resources. Data replication or sharding&nbsp;often require ensuring the consistency of write operations across several locations/databases. Coordinating distributed transactions&nbsp;across various microservices and databases. Solving complex real-world problems like leader election, task distribution and synchronization, and ensuring idempotency in distributed systems. Service discovery protocols&nbsp;where microservices need to know about other’s presence require a reliable mechanism to avoid race conditions and conflicts. These protocols often use distributed locks to avoid conflicts while updating the common registry. These were just a few examples, and there are many more situations where distributed locks come into play in a distributed system. Please remember that distributed locks aren’t without challenges — consistency, availability, and network partitions (CAP theorem) all have their part to play. But as we progress, we’ll delve deeper into understanding how we can implement distributed locks using Redis in our further lessons. Topic: 1.4 Implementing Distributed Locks using Redis.First and foremost, it’s crucial to understand that a distributed lock should satisfy the following properties: Mutual Exclusion: At any point in time, only one client can hold a lock. Deadlock Free: Eventually, every lock request must succeed. Fault Tolerant: If a client holding a lock crashes, the system should recover. Redis provides commands (such as SETNX, EXPIRE) that can potentially create a locking system. But issues regarding expiry of lock key and releasing of lock by a client other than the one holding it can ensue. Therefore, to address and overcome these issues, the&nbsp;Redlock (Redis distributed lock)&nbsp;algorithm was introduced by Salvatore Sanfilippo (creator of Redis). The workings of the Redlock algorithm are as follows: When a client wishes to acquire a lock with some resource, it generates a unique random string (value). This client then tries to acquire the lock in all the N Redis masters using the SETNX command (set value if the key doesn’t exist) and attaching a time-to-live (TTL) with it. If the client succeeds in setting it on the majority of masters (&gt; N/2), it considers the lock to be acquired successfully. If the lock setting fails in the majority of instances, the client will try to delete the key from all the instances (even from those where it initially succeeded), waits for a random delay, and then tries steps 1–3 again. To release a lock, it simply sends a DEL command to delete the key. With this, you can create a robust distributed locking system with Redis. Remember, the success of this algorithm rests heavily on synchronized clocks across the Redis nodes as TTL values are associated with locks. Topic: 1.5 Redis TransactionsRedis transactions allow the execution of a group of commands in a single step. First, all commands are queued, and with a final command, all of them are run sequentially. Redis transactions use two primary commands:&nbsp;MULTI&nbsp;and&nbsp;EXEC. Here’s an example of a Redis transaction: 1234MULTI INCR foo INCR bar EXEC In this example, we’re incrementing the values of both ‘foo’ and ‘bar’ keys, and this increment operation is done in a transaction.&nbsp;MULTI&nbsp;is the command that marks the start of the transaction block and&nbsp;EXEC&nbsp;marks the end and triggers the execution. Redis transactions have the ‘all-or-nothing’ property. This means if a command fails, all the commands in the transaction are rolled back. It’s important to note that Redis commands don’t fail often because they have been designed to fail during the syntax check of the command, which always happens before the command is queued. From a locking perspective, it’s critical to note that Redis uses “optimistic locking” — locks are not held during the execution of the transaction. Instead, you can use the&nbsp;WATCH&nbsp;command on one or more keys. If those keys are modified by another client before your transaction executes, your transaction will be canceled, allowing you to handle race conditions safely. Keep these principles in mind: Redis transactions are atomic, meaning all commands are executed or none are. Redis uses optimistic locking to handle concurrent transactions. Topic: 1.6 Case Study — Using Redis Distributed Locks in Real-world ApplicationsDistributed locks are used in a variety of applications that require coordination and synchronization across multiple systems, processes, or threads. Here are some real-world use cases: E-commerce Platform: A popular use case for distributed locking is inventory management in an online shopping platform. When multiple users attempt to purchase the last item in stock simultaneously, distributed locks can be used to ensure that only one purchase operation for that item succeeds, preventing overselling. Banking Systems: Distributed locks can play a crucial role in financial transactions. For instance, consider a scenario in which two operations (debit and credit) are performed concurrently. It’s necessary to ensure that these operations are done in an atomic way to prevent inconsistencies in the balance. Online Ticket Booking: A distributed lock can ensure that a single seat can’t be booked by multiple users in a concurrent booking operation. Master Election in Distributed Systems: In a distributed system, distributed locks can be used to handle fail-overs by electing a new master node when the existing master node fails. Looking at these use cases, it’s clear that distributed locks cater to the needs of a complex, distributed application system where maintaining data consistency, integrity, and coordination among various services is the top priority. Topic: 1.7 Review and AssessmentsOver the course of our sessions, we’ve gained a rich understanding of Redis, its built-in support for distributed locks, and how it’s leveraged in real-world applications. We’ve also dived deep into Redis transactions and gained insights into how they participate in distributed locks. We have covered numerous concepts, such as: Deep dive into Redis: We expanded on the basics, diving deeper into Redis’s features like its data types and Pub/Sub capabilities. Understanding Locks in Databases: We got a general overview of locks in databases, their utility, and types. Need for Distributed Locks: We saw the need for Distributed Locks and their role in enhancing performance in large-scale applications. Implementing Distributed Locks with Redis: We discussed how we can achieve distributed locks using Redis. Redis Transactions: We covered transactions in Redis, its commands, and how they work alongside distributed locks. Real-world applications of Redis Distributed Locks: We looked at various use case scenarios where Redis Distributed Locks have been applied. Example Problem: Assume you’re developing an online ticket booking system. There is a scenario in which a single seat is being booked by multiple users simultaneously. How would you prevent this scenario using Redis distributed locks? Here’s how we might solve this problem: First, we would implement a lock on the seat when a user begins the booking process. This lock would prevent other users from booking the same seat. Here’s an example of how we could implement this in Redis: 1SET lock:seat_id value NX EX 30 In this command,&nbsp;lock:seat_id&nbsp;is the lock identifier (where the seat_id is the ID of the seat being booked),&nbsp;value&nbsp;is a unique string to identify the process that has the lock,&nbsp;NX&nbsp;tells Redis to only set the key if it doesn’t already exist, and&nbsp;EX 30&nbsp;sets an expiration for the lock of 30 seconds. The return value of the command will either be&nbsp;OK&nbsp;or&nbsp;None. If the return value is&nbsp;OK, it means we successfully acquired the lock. If it’s&nbsp;None, it means another process has the lock. Now let’s test your understanding. Simple Problem (3/10): Why do we need a distributed locking system in a large-scale application? Moderate Problem (6/10): What are the key principles to remember about Redis transactions? Complex Problem (9/10): How would you solve a common problem in distributed systems, such as the fail-over scenario, using distributed locks in Redis? Simple Problem (3/10): We need a distributed locking system in large-scale applications to handle concurrency and to ensure data integrity in a distributed system. For instance, if multiple clients are trying to access and modify the same piece of data simultaneously, a distributed lock helps ensure that only one client can access and modify that data at a time, thus preventing race conditions, inconsistencies, and other potential issues. Moderate Problem (6/10): The key principles to remember about Redis transactions are as follows: Redis transactions provide a way to execute a batch of commands atomically. The&nbsp;MULTI&nbsp;command is used to start a transaction, and&nbsp;EXEC&nbsp;is used to execute the transaction. The&nbsp;WATCH&nbsp;command can be used to implement optimistic locking. It helps abort the transaction if the watched key has changed. If a command fails within a transaction, Redis still executes the rest of the commands in the transaction. Complex Problem (9/10): Distributed locks can play a crucial role in handling fail-overs in a distributed system. In the event of a fail-over (where a node in a cluster fails), we have to elect a new master node. A distributed lock can be used to ensure that the election process proceeds without conflicts and that only one node is elected as the new master. We could use a similar locking pattern as earlier, where the lock represents the master node. Whichever node can successfully acquire the lock becomes the new master. 中文文章: https://programmerscareer.com/zh-cn/redis-interview2/Author: Wesley Wei – Twitter Wesley Wei – MediumNote: If you choose to repost or use this article, please cite the original source.","link":"/redis-interview2/"},{"title":"Redis interviews: If the Redis master node is down, how do you recover the data?","text":"Let’s outline your tailored curriculum to learn about Redis. This plan will particularly focus on the scenario when the Redis master node goes down and the process to recover the data. Thank you for reading this article. More Interview Questions here:https://programmerscareer.com/software-interview-set/ Topic: Redis ArchitectureRedis stands for Remote Dictionary Server. It’s an open-source, in-memory data structure store that can be used as a database, cache, and message broker. One of the main reasons Redis performs so well is because it does all its operations in memory and avoids seeking time needed for disk operations. Redis supports a variety of data structures like strings, hashes, sets, lists, etc. Let’s dive a bit deeper into the architecture of Redis. Redis Server and ClientFundamentally, the Redis data store system comprises two main roles: a Redis client and a Redis server. The&nbsp;Redis client&nbsp;is a standalone application that connects to a Redis server and executes commands against the server. The client could be anything from a small script that connects to Redis to manage application sessions, to a massive system that uses Redis for caching data in memory for speedy access. The&nbsp;Redis server, on the other hand, is the powerhouse. It is where your data lives, where data is cached into memory, where data structures are maintained, and the server processes all the commands a client sends over. Redis Data StructuresThe fundamental principle to understand about Redis architecture is that it is a&nbsp;key-value&nbsp;data store — which means every piece of data you store in a Redis server will contain a key and a value. What sets Redis apart is the types of values it can store. Redis supports a variety of data structures such as: Strings Hashes Lists Sets Sorted sets Each data structure has its own set of commands for managing the data. For example, if you’re working with a list, you can execute commands like&nbsp;LPUSH,&nbsp;LRANGE, etc. to manipulate the list. These data structures make Redis extremely versatile, allowing it to solve many different types of problems efficiently. Persistence — A GlimpseOne of the key components of Redis architecture is its ability to persist data to disk. Imagine if all the data you’d stored in memory was wiped out if your Redis server shutdown — not very efficient, is it? To mitigate this, Redis provides a few different strategies for persisting data to disk such that it can be recovered in the event of a shutdown or failure. We’ll cover this aspect in more detail in the upcoming lesson. Now that we have an understanding of the basics of Redis architecture, we’ll gradually dig deeper into more sophisticated concepts like data replication, backups, and high availability with Redis Sentinel in subsequent lessons. Topic: Replication in RedisReplication is a mechanism that allows your data to be automatically copied from a master server to one or more replica servers. Replication offers two main benefits: Performance Improvement: You can distribute read traffic away from the master server to replica servers. This allows the master server to handle fewer requests and improves overall performance. Data Redundancy: Your data will be stored on multiple servers, providing a fail-safe option should the master server go down. This fault tolerance is crucial in production environments. Understanding Master-Replica Configuration in RedisWhen replication is set up in Redis, it follows a master-replica configuration. The master server contains the original copy of the data, and this data is duplicated to the replica servers. Setting up replication in Redis is straightforward. Basically, this involves setting up a master server and then connecting one or more replicas with the&nbsp;SLAVEOF&nbsp;command, specifying the master server’s IP and port. Let’s understand how changes in the master are propagated to the replicas: When a change occurs in the master’s data set (for instance, a write operation), the master server will send the command to the connected replicas. Each replica will receive the command and execute it, thereby making its data set up-to-date with the master’s. It’s important to understand that data operations are&nbsp;asynchronous&nbsp;— the master will not wait for replicas to acknowledge receipt and execution of commands. However, the master does keep track of which commands were acknowledged by each replica. This replication scheme provides a robust mechanism for data redundancy and performance scaling. However, it’s not without challenges, as what if the master node goes down? How do you ensure high availability and data consistency? How does Redis handle these scenarios? We will discuss these topics in more detail in the subsequent lessons. Topic: In-depth into Redis PersistenceAs we discussed earlier, Redis operates largely in the memory space, providing rapid access and modification of data. However, persisting the data becomes crucial to prevent data loss in case of a server crash or shutdown. Redis incorporates methods to save data in memory to disk, which are&nbsp;RDB&nbsp;and&nbsp;AOF. RDB (Redis Database Backup)RDB persistence performs point-in-time snapshots of your dataset at specified intervals. Here’s how it works: Redis will fork a child process. The child process will then write the entire data set to disk (to an RDB file), thereby capturing a snapshot of the data at that moment. The frequency at which these snapshots are taken can be configured. For example, you could configure Redis to save to disk if at least one change was made in the past 15 minutes. RDB’s are perfect for backups. If you ever need to rebuild your database, having point-in-time snapshots is very handy. AOF (Append Only File)AOF persistence, on the other hand, logs every write operation received by the server, which can then be played back when the server starts. The way it works is pretty straightforward: When a command that modifies the dataset in some way is received, it gets appended to the AOF buffer. Redis frequently writes this AOF buffer data to disk. You can configure the frequency at which data in the AOF buffer is written to disk. Compared with RDB, AOF files are more durable as they’re append-only. This means that even if a power outage or crash happens during a write, you’ll likely have a full history of commands up until shortly before the outage. Whereas with RDB, you might lose more data since it snapshots less frequently (depending on your save conditions). Choosing Between RDB and AOFThere’s not a one-size-fits-all answer to this. It depends on the nature of your application and how critical your data is. Some prefer RDB for faster backups that can be easily moved around. Others prefer AOF for a higher level of durability. Redis actually allows you to use both RDB and AOF at the same time! If you enable both, Redis will write to the RDB file while iterating the dataset for AOF rewriting, thus generating a “snapshot” of the database at the start of the AOF rewriting process. You can consider this as a hybrid approach, enjoying the benefits of both the methods. Topic: Redis BackupsWithout reliable and regular backups, your data is at risk of loss, especially in the event of hardware or software failure. For Redis, the snapshotting feature, or Redis Database Backup (RDB), provides a robust way to backup your data. It provides a consistent and compact point-in-time snapshot of your Redis data. The RDB persistence model operates by saving the dataset to disk at different time intervals that you can specify. These intervals could be, for instance, every fifteen minutes if at least five keys changed, or, every hour if at least one key changed, and so forth. Creating BackupsRedis allows you to manually produce an RDB file at any time by using the&nbsp;SAVE&nbsp;or&nbsp;BGSAVE&nbsp;commands. The&nbsp;SAVE&nbsp;command operates synchronously and will block all other clients, so for production environments, it’s better to use the&nbsp;BGSAVE&nbsp;command, which will fork a new process to save the data while your Redis server continues to serve client requests. It is worth noting that this process can consume a lot of I/O and CPU depending upon the size of your data. Restoring from BackupsRestoring an RDB file is as simple as stopping your Redis server, replacing the RDB file with your backup, and restarting the server again. Upon startup, Redis will load the data from the RDB file into memory and continue normal operation. Actions like writing new data to the Redis store or reading from the store cannot be done until the data is loaded into memory. Understanding backups is a critical aspect of Redis as it forms the foundation of any disaster recovery plan. It’s essential to have regular and reliable backups to safeguard your data and ensure the smooth operation of your applications. Topic: Redis SentinelNow, let’s discuss an important aspect of Redis, the Redis Sentinel system. It helps fulfill two main functions —&nbsp;monitoring&nbsp;and&nbsp;automated failover. Let’s take a closer look at both. Monitoring: Redis Sentinel continuously checks if your master and replica instances are working as expected. It not only confirms the availability of instances (up and running) but also validates that they are functioning correctly (able to accept connections and respond to queries). Automated Failover: If your master node fails, the Sentinel system will automatically detect this and begin the failover process. This process involves choosing a replica, promoting it to be the new master, and reconfiguring the other replicas to use the new master. These features provide high availability and resilience to Redis environments. Now, utilizing the Sentinel system involves a series of steps: Setting up the Sentinel Nodes: First, we need to create Sentinel nodes, which are separate instances of Redis running in Sentinel mode. A minimum of three Sentinel nodes is recommended for a robust setup. Configuring Sentinel Nodes: The Sentinel nodes need to be configured to monitor a master. You do this by specifying your master’s IP and port. Validate Setup: After configuring, you should validate your setup by checking whether your Sentinel nodes are correctly monitoring your master and its replicas. With this setup complete, the Sentinel system will perform its monitoring and automatic failover duties as described above. Worth noting is the concept of&nbsp;Quorum, which represents the minimum number of Sentinel nodes that need to agree for failover to take place. For instance, if you have five Sentinel nodes, a quorum could be three. Meaning at least three Sentinel nodes need to agree that the master is indeed not functioning and a failover should be initiated. Redis Sentinel provides great value for endeavours requiring high availability for Redis. In the next lesson, we will handle the scenario of Master Node failures, common reasons behind it, and how Redis mitigates such incidents. Topic: Redis Master Node Failure — An OverviewMaster Node failures, while infrequent, may pose a challenge for a Redis infrastructure that’s not configured for such events. It is pivotal to understand the potential reasons behind such failures and devise strategies to handle them. There can be several causes for master node failure, including: Hardware Failures: This can be a physical damage, or wear and tear on the hard disk. Sometimes, the memory components might fail leading to server crashes. Network Disruptions: Disturbances in the network connection could cause the Master Node (or any node for that matter) to lose connection with the other nodes. This can be a temporary glitch or a permanent problem depending on the underlying infrastructure. Disk Full Errors: Redis may shut down if the disk gets full to prevent data inconsistency. Software Errors / Server Overload: Bugs in software or an overload on the server could potentially cause a crash, leading to master node failure. When a master node fails, the key concern becomes how to ensure uninterrupted service. This is where the Redis Sentinel system and Redis replication come to play. If a failure is detected, the Sentinel system will start an automatic failover process. The failover process involves promoting a replica to be the new master that other replicas will be automatically reconfigured to use as their new master. Understanding the potential reasons behind master node failure and the subsequent recovery mechanisms is important to maintain high availability in Redis. In the next lesson, we will delve deeper into the Redis Master Node Data Recovery process following a Redis Master node failure. Topic: Redis Master Node Data RecoveryIn a scenario where a master node in Redis goes down due to certain unforeseen events, the process of data recovery from backups becomes crucial in ensuring smooth operation. As we learned earlier, the first response to a master node failure is the Redis Sentinel system initializing an automatic failover procedure. One of the replicas will be promoted to the role of the master, and the other replicas will be reconfigured to now connect to this new master. However, we also need to consider the process of restoring the original master node and adding it back to the system once it is operational again. After the issue with the failed master is resolved, and the original master node is restored, it will connect back to the system as a replica, perform a synchronization and then can be reconfigured back. Now, what happens to the data that was written on the replica (now master) during the downtime? This depends on your persistence configuration: AOF (Append Only File) Persistence Configuration: In case of AOF, all write operations are logged, and if a master node goes down, the AOF file continues to log these operations on the replica. Once the master is restored and synchronized with this replica, it will also receive these write operations, ensuring that no data is lost. RDB (Redis Database Backup) Persistence Configuration: In the case of RDB, snapshots are taken at configurable intervals. So, any data written between two snapshots could potentially be lost if a failure occurs. In a nutshell, the mechanism to handle master node failure effectively in Redis largely depends on the configurations, Sentinel system, and persistence settings. You can choose the strategy that best applies to your use-case and aligns more closely with your data safety requirements. Topic: Review and AssessmentsWe have navigated a thorough exploration of various aspects of Redis. Let’s recap the core concepts we’ve covered: Redis Architecture: We began by understanding the underlying architecture of Redis. Replication in Redis: Studied the concept of data replication in Redis and how it’s achieved. Redis Persistence: Went in-depth into the process of data persistence in Redis and why it’s significant. Redis Backups: Learned how to establish backups in Redis and understood their role in data recovery. Redis Sentinel: Comprehended the principle of the Redis Sentinel and its function in maintaining high availability. Master Node Failure: Discussed probable reasons for Master Node failure in Redis. Master Node Data Recovery: Understood the detailed process when a master node in Redis experiences downtime. Now, it’s essential to review and reassess our understanding of these topics. This is where interactive assessments come into play. They present an opportunity to check your comprehension, apply learned knowledge, and rectify if any gaps remain. Example Problem: Assume you’re setting up a data storage system for your application. You decided to use Redis and need to configure it. You’ve two servers available to use. How would you assure data safety and high availability? Now let’s test your knowledge. Problem 1:Given what you’ve learned about the internals of Redis, describe the building blocks of Redis architecture. Problem 2:Explain the role of Redis Sentinel and how it helps maintain high availability in Redis infrastructure. Problem 3:What steps would Redis take in the event of a master node failure? For each question, please share your answers. The&nbsp;Example Problem&nbsp;was: Assume you’re setting up a data storage system for your application. You decided to use Redis and need to configure it. You’ve two servers available to use. How would you assure data safety and high availability? To ensure data safety and high availability, you could set up a Redis environment as per these steps: Use both servers: Install Redis on both servers. One will act as the master, and the other will be a replica (slave). Data Persistence: Configure data persistence mechanisms on both servers. This will ensure that changes in data are stored and not lost, providing data safety. For instance, you may choose RDB for less granular but less resource-intensive backups, or AOF for highly granular backups at cost of more resource use. Master-Replica Replication: Set the second server as a replica of the first one. It means that all data written to the master will also be written to the replica. This is important in the case of server 1 (designated as master) goes down. Redis Sentinel: To maintain high availability, use Redis Sentinel. Sentinel will monitor both servers, and if the master goes down, Sentinel will promote the replica to be the master. Configure Your Application: Configure your application to send write operations to the master, and read operations can be balanced between the two servers. These steps will provide a balance between high availability (through replication and Redis Sentinel) and data safety (through data persistence mechanisms). Problem 1: The building blocks of Redis architecture include: Redis Clients: These are applications or users that send commands to Redis to perform operations with the data stored in it. Redis Server: This is where Redis is installed and running. It’s responsible for storing data in memory and performing operations with it. Data Structures: Redis supports several types of data structures, including strings, lists, sets, sorted sets, and others. Each structure has specific commands associated with it. Database Persistence: Redis provides two mechanisms for database persistence — RDB and AOF. RDB takes snapshots of your dataset at specified intervals. AOF logs every write operation received by the server. Replication: This is the process of setting up master-slave nodes to ensure data redundancy. If the master node fails, one of the slaves is promoted to be the new master. Problem 2: Redis Sentinel is renowned for its primary functionality of monitoring Redis instances. It can identify when a master node fails and will begin a failover process. The Sentinel system will promote a replica to be the new master and reconfigure all other replicas to use the new master. Applications are also notified about the new master to redirect their queries. Problem 3: When a Redis Master Node fails: The Redis Sentinel (if configured) detects the failure of the Master node. One of the Sentinels initiates a failover and other Sentinels acknowledge this. Redis Sentinel will elect a replica to be promoted as the new master. Other replicas will be reconfigured to use the new master. After resolving the issue, the failed master will join back as a replica to the current master. It will require a full synchronization with the new master. 中文文章: https://programmerscareer.com/zh-cn/redis-interview3/Author: Wesley Wei – Twitter Wesley Wei – MediumNote: If you choose to repost or use this article, please cite the original source.","link":"/redis-interview3/"},{"title":"Redis interviews: how to prevent cache avalanche and cache penetration with Redis","text":"Focusing on cache prevention aspects, let’s explore and master it together Thank you for reading this article. More Interview Questions here:https://programmerscareer.com/software-interview-set/ Topic&nbsp;1.1: Detailed Study of Cache PenetrationCache Penetration, also known as cache miss or cache busting, is a scenario where requests bypass the cache and directly hit the database. It typically occurs when the request queries data that is not stored in our cache. Imagine a shopping website where people can search for products. The cache may contain popular searched items for faster retrieval. However, fur users searching for a rare product not in the cache, the system has to query from the database. This is a case of cache penetration. This might not sound like a serious issue, but imagine a scenario where a high volume of traffic queries for items that are not in the cache. It would lead to a substantial amount of database hits and might eventually lead to the database crashing due to the amount of load. An even severe case of cache penetration is when attackers can precisely predict that certain requests won’t be cached and bombard our system with those requests, causing our database to be the primary hit point and, eventually, it might crash the system. Cache penetration is something that should be avoided for smooth and efficient system functioning. Luckily, Redis provides powerful strategies to mitigate cache penetration, and we will explore them in the upcoming topic. Topic 1.2: Strategies to prevent Cache Penetration using RedisRedis offers powerful strategies to prevent cache penetration, ensuring efficient system performance even under high load. These strategies primarily focus on reducing direct hits to the database, hence mitigating cache penetration. One common strategy is implementing a&nbsp;Default Cache Value. When a query for non-existent data occurs, instead of letting the request go straight to the database, it can be handled at the cache level by returning a default value. This means the database won’t take a hit when data is not in the cache, thus preventing cache penetration. Another powerful strategy is the use of&nbsp;Bloom Filters. A Bloom filter is a probabilistic data structure that can be used to test whether an element is a member of a set. This implies it can quickly identify whether the data requested exists in our database or not. If the Bloom filter says that the item doesn’t exist, we can immediately return a default value without having to query our database or even our cache. When setting up these strategies, it’s important to keep the trade-offs in mind. The use of a Bloom filter introduces a small chance of a false positive. However, the benefits often greatly outweigh the minimal error probability. Topic 1.3: Deep Dive into Cache AvalancheCache Avalanche is a type of system failure that occurs when a large number of cache items expire simultaneously, and multiple requests for these data items hit the database, potentially causing it to crash due to the high load. Think about a scenario where a website caches its daily deals, and all the cache items are set to expire at midnight. As the clock hits 12:00 AM, all the cache items become invalid. The first set of users who try to access these deals post-midnight cause the system to fetch the new deals from the database and populate the cache. However, imagine a scenario where millions of users try to access these deals simultaneously soon as the cache becomes invalid. This could potentially flood the database with requests, leading it to become unresponsive or even crash — that’s a Cache Avalanche effect. While Cache Avalanche might sound catastrophic, there are strategies which we can employ to prevent it from happening. Understanding these techniques will make the systems we design more robust and reliable. Topic 1.4: Preventing Cache Avalanche using RedisPreventing a Cache Avalanche effectively means preventing a horde of requests from reaching our database simultaneously. Redis offers many practical strategies for this. The first technique is to use&nbsp;TTL (Time To Live) staggering. Instead of setting the same TTL for all cache items, we can slightly stagger or randomize their TTL values. This introduces differences in the expiry times, thereby reducing the risk of many items expiring simultaneously. Another major strategy is to use&nbsp;Cache Warming. Cache warming is the practice of loading data into the cache before it’s needed. For instance, if we know certain cache items are likely to expire soon, we can preemptively refresh them during periods of low demand to avoid an avalanche during peak times. Finally, it might be beneficial to consider using&nbsp;Fallback Caching. In this approach, even when a cache item is known to have expired, the old (expired) value is returned while the cache is updated in the background. This prevents sudden database loads due to simultaneous cache misses. It’s key to understand that no single strategy is a silver bullet in every scenario. The actual implementation might require a combination of these strategies depending upon the specifics of the use-case. Topic 1.5: Redis Transactions with Cache preventionRedis is not just an in-memory database, but it can also support transactions — a series of commands that are executed sequentially, stopping only if an error is encountered in one of the commands. Redis transactions use a two-step process : QUEUING commands: Commands are queued up using the&nbsp;MULTI&nbsp;command. Nothing is executed at this stage; Redis merely keeps track of all the commands that are within this transaction. EXECUTING commands: When the&nbsp;EXEC&nbsp;command is issued, Redis then executes all the commands queued up in the exact order. Redis transactions are employed to ensure that all cache operations (like reads, writes, or updates) are atomic — which means they get executed fully or not at all. This is crucial to maintain the cache consistency and prevent dirty reads, which can also help mitigate the effects of cache penetration. Let’s take an example. Suppose you are implementing a leaderboard system and want to update the score of a player atomically. Here’s how a transaction could be used to achieve that: 1234MULTI GET player_score INCR player_score EXEC By wrapping both GET and INCR commands within a transaction, we ensure that if any other client reads the score, they will always get a consistent value. Using transactions in Redis alongside cache prevention techniques, be it for penetration or avalanche, can significantly improve the consistency and reliability of our caching layer. Topic 1.6: Real-world applications of Redis Cache PreventionRedis and its cache prevention mechanisms are frequently used in a variety of real-world applications to handle sizable loads without bringing down the backend database. Here are a few examples: E-commerce websites: Websites like Amazon use Redis for caching product details and recommendations for faster retrieval. Measures to prevent cache penetration and cache avalanches are crucial to handle the simultaneous user load, especially during festive sales. Social media platforms: Platforms like Twitter and Instagram use Redis to cache user data and feed information. The high volume of simultaneous reads and writes makes Redis an excellent choice for these platforms. Leaderboard systems: On gaming platforms, user scores and rankings are updated in real-time and need to be accessed by many clients simultaneously. Redis’s ability to handle atomic transactions ensures score consistency across clients, even under high load. Online ticketing services: During high-demand events, ticketing services can experience a massive surge in traffic, which can lead to database failure if not handled correctly. Redis’s cache management capabilities can effectively prevent these scenarios. In all these examples, cache optimization measures like staggering the TTL, warming the cache, and using fallback values are employed to protect the system from potential cache penetration and cache avalanches. Topic 1.7: Review and AssessmentsCache Penetration&nbsp;occurs when frequent requests are made for non-existent data, causing each request to reach the database since it’s not available in the cache. It can lead to excessive and unnecessary database load. Redis provides various mechanisms to prevent it, such as NULL caching and Bloom filters. Cache Avalanche&nbsp;happens when multiple cached data expires simultaneously, leading to a barrage of hits to the database. Redis provides strategies like TTL staggering, cache warming, and fallback caching to handle Cache Avalanches. Redis Transactions&nbsp;play an important role in maintaining data integrity and consistency during multiple read or write operations. By queuing multiple commands and executing them atomically, redis transactions prevent dirty reads and provide higher reliability. Redis and its techniques for preventing cache penetration and avalanches are frequently used in high-traffic, real-world applications like e-commerce websites, social media platforms, real-time leaderboard systems, and online ticketing services. Let’s begin with the assessments. Question 1 Explain, in your own words, what Cache Penetration is. Why is it a problem, and how does Redis help prevent it? Question 2 Describe a real-world scenario where Redis Transactions might be useful. How would utilizing transactions in that scenario promote data consistency? Question 3 Consider a high-traffic e-commerce website, and describe how the Cache Avalanche can be dealt with effectively using Redis. Answer to Question 1 Cache Penetration refers to the scenario where frequent requests for non-existing data are passed to the database since the cache does not hold these values. It can lead to unnecessary database load and degrade performance. Redis helps prevent Cache Penetration primarily by using NULL caching, where you store the “NULL” keyword for a certain duration when the value queried in the database returns NULL. Answer to Question 2 In a social media platform like Twitter, when a user ‘likes’ a tweet, the total number of likes for the tweet and the user’s liked tweets both need to be updated. This scenario requires multiple write operations and if not handled atomically, can lead to inconsistent data. Redis Transactions can queue these multiple write commands and execute them atomically to maintain data integrity and consistency. Answer to Question 3 In a high-traffic e-commerce website like Amazon, a Cache Avalanche can occur when many cached products details or user recommendations expire simultaneously, leading to a sudden increase in database load. Redis effectively handles this by TTL staggering where each key-value pair in the cache has slightly different expiration times, or by warming the cache where you refresh the cache with the most frequently accessed data before the old cache expires. This prevents a sudden surge in database queries. 中文文章: https://programmerscareer.com/zh-cn/redis-interview4/Author: Wesley Wei – Twitter Wesley Wei – MediumNote: If you choose to repost or use this article, please cite the original source.","link":"/redis-interview4/"},{"title":"Redis interviews: Briefly describe the advantages and disadvantages of RDB and AOF schemes in Redis persistence","text":"let’s study Redis with a focus on the RDB and AOF persistence schemes. Thank you for reading this article. More Interview Questions here:https://programmerscareer.com/software-interview-set/ Topic: Redis ArchitectureNow, imagine a system, lean and efficient, furnished to quickly store and retrieve data. Allow me to introduce the marvel that is Redis! Redis, or Remote Dictionary Server, is an in-memory data structure store. It can be used as a database, cache and message broker. The “in-memory” part means that it primarily stores data in RAM, making data operations extremely fast since reading from and writing to the primary memory can be done at a swifter pace compared to disk storage. Its structure is based on a server-client architecture. This means every Redis setup has a Redis server and one or more Redis clients; think of this as a conversation where the client constantly requests, and the server responds. We have few components at play in the Redis Architecture: Redis Clients: They could be different applications or multiple instances of the same application. They ask, and the server responds. Redis Server: The server manages the data store and responds to the commands by clients. Redis Data: At its core, Redis manages a data store — a crucial part of our Redis architecture. Now, as we have seen, Redis is an in-memory data structure. Yet, an essential feature of Redis is its ability to persist data on disk. It can log every write operation transmitted by the clients, thereby providing a high level of data safety for an in-memory database. And that’s a brief overview of the Redis Architecture! Topic: Redis Persistence OverviewRedis, a in-memory data structure store used as a database, cache, and message broker, excels at handling data operations at lightning-fast speed. However, being in-memory raises the question of data volatility and persistence. How does Redis ensure your data longevity when it stores everything in RAM, which is volatile in nature? Fret not! Redis takes care of this potential issue by offering two methods of ensuring data persistence:&nbsp;RDB (Redis Database)&nbsp;and&nbsp;AOF (Append Only File). RDB Persistence:&nbsp;This method takes a point-in-time snapshot of your dataset at specified intervals and saves it on a disk in a binary format. These “snapshots” are compact and quick to load, making RDB an excellent option for backups. AOF Persistence:&nbsp;The Append Only File logs all the write commands received by the server in a file. This file is read by Redis when it restarts to rebuild the state of data. Since all executed commands are saved by Redis, you can adjust the server’s data durability levels by syncing the file every time a command is written, every second, or hardly ever. Both methods come with their own pros and cons, the choice of which to use depends entirely on your specific use case. In some cases, a combination of both methods might be an ideal solution. In the upcoming sections, we are going to discuss these persistence technologies in more detail. This will give you a deeper understanding of both so you can make an informed choice. Topic: Redis RDB PersistenceLet me unfold the story of&nbsp;RDB Persistence. RDB, which stands for Redis DataBase, is a very convenient persistence format in Redis. In this method, the Redis server creates point-in-time snapshots of your dataset at specified intervals. Think of it as creating periodic backups of your data that can come in super handy in certain scenarios. RDB operates in a very straightforward manner. At configured intervals, the Redis parent process forks a child process. The parent process continues to serve clients, while the child process starts writing the RDB file on the disk. This way, the database can continue to handle client requests while the snapshot is being created. When the child process finishes saving the RDB file, it replaces the old snapshot with the new one. The child process then exits and sends a success signal to the parent process. If an error occurs during saving, the child process sends an error signal back to the parent process. RDB persistence has its own set of advantages: It’s perfect for disaster recovery. You can configure Redis to take snapshots every minute or every few seconds. If something disastrous occurs, you would lose a maximum of one minute’s worth of data. The snapshots are created in a fraction of a second and are perfectl the RDB snapshotting process is fast and doesn’t affect the performance of the Redis server catering to write requests. Furthermore, it creates compact files that Redis can consume quickly during server start, reducing downtime. But, as with everything else, RDB persistence does come with its caveats: RDB is a point-in-time snapshotting system, meaning it doesn’t record each individual write. So in the case of a crash or failure, you might lose some data that was not included in the last snapshot. Despite being an automated process, snapshot generation can be resource-intensive for large databases, causing degradation of service during the snapshotting period. With this information in mind, it’s clear that while RDB has numerous benefits in terms of data backup and disaster recovery scenarios, it might not be the optimum solution for applications needing high durability. Similar to a tale with two sides, this is only half of our persistence story. In the upcoming lesson, we’ll be exploring the ins and outs of the other method — AOF (Append Only File) persistence. Topic: Redis AOF PersistenceNow that we have a good understanding of RDB Persistence, let’s shift our focus to another method that Redis employs to persist data:&nbsp;Append Only File (AOF). Unlike RDB Persistence which creates point-in-time snapshots, AOF takes a more comprehensive approach. Every executed write command is logged by Redis. Literally, every single one. These are then saved to an&nbsp;append-only file, hence the name. Now, when Redis restarts, it uses this file to restore its previous state. The commands are simply executed one after another to recreate the data. One of the beauties of this approach is its durability. Since every write operation is logged, you’ve got quite an account of all the changes. It might also be music to your ears to know that Redis offers adjustable levels of durability: You can set Redis to sync this log file every time a command is written Or, Redis could be set to sync the file every second Or even, you can trust your luck (or perhaps the stability of your power supply) and hardly ever sync at all! Imagine that! Full control over your database persistence method! Of course, AOF persistence has its own pros and cons. In the subsequent lesson, we’ll pit RDB and AOF against each other, compare their strengths, and help you understand when to use which. Topic: Redis RDB vs. AOF PersistenceWhen it comes to Redis and data persistence, RDB and AOF are the two knights in shining armor. However, they each have their strengths and weaknesses. Firstly, RDB persistence creates point-in-time snapshots of your dataset at specified intervals. So in the event of an unexpected shutdown, you can restore your data to the last snapshot. However, this could mean that data written after the most recent snapshot would be lost forever! While RDB file creation is fast and doesn’t use much memory, you can experience a performance hit when dealing with larger databases due to decreased input/output operations. On the other hand, AOF persistence logs every write operation received by the server. This can be beneficial. Not a single piece of data is lost because everything is logged almost instantly. But, the log files can eventually become quite large, and the constant writing can introduce latency. ltimately, the choice between RDB and AOF depends on your use-case. If you can’t afford to lose any data, AOF is the way to go. But if your data can be easily reconstituted and you need quicker backups and recovery, then RDB could be a better fit. In many instances, using both RDB and AOF together will give you the benefits of both worlds. You’d have the durability of AOF and the speedy backups and data recovery of RDB. Topic: Implementing Redis PersistenceRedis’ flexibility with persistence configurations is one of its strengths. You can opt for RDB, AOF or even both based on your needs. Here’s how you can do it: Implementing RDB Persistence: Enabling RDB persistence primarily involves configuring how often you’d like Redis to save a snapshot of your database to disk. This is controlled by the&nbsp;save&nbsp;configuration directive in the Redis configuration file (redis.conf). The syntax is&nbsp;save &lt;seconds&gt; &lt;changes&gt;, where&nbsp;&lt;seconds&gt;&nbsp;specifies a certain number of seconds and&nbsp;&lt;changes&gt;&nbsp;specifies a minimum number of changes. You can have multiple&nbsp;save&nbsp;directives in the&nbsp;redis.conf&nbsp;file for finer control. Implementing AOF Persistence: To turn on AOF persistence, you’ll have to update the&nbsp;appendonly&nbsp;configuration directive in the&nbsp;redis.conf&nbsp;file to&nbsp;yes. Additionally,&nbsp;appendfsync&nbsp;is another significant directive which defines how often the data is written to the AOF file. It can be set to&nbsp;always&nbsp;(fsync every write),&nbsp;everysec&nbsp;(fsync every second), or&nbsp;no&nbsp;(fsync only when Redis decides). Using RDB and AOF Together: Both of these persistence methods can be used at the same time by enabling their respective directives in&nbsp;redis.conf. You get point-in-time snapshots from RDB and durability from AOF. And voila! You have configured Redis persistence according to your application’s needs. When in doubt, keep in mind the key differences between RDB and AOF persistence and their respective use-cases. Topic: Review and AssessmentsWe’ve covered a lot over the course of these lessons: Redis Architecture:&nbsp;We delved into the workings of Redis, especially as an in-memory data structure store. Redis Persistence Overview:&nbsp;We discussed how Redis handles data persistence and some of the inherent trade-offs. Redis RDB Persistence:&nbsp;We explored the RDB persistence scheme, how it works and when it can be beneficial. Redis AOF Persistence:&nbsp;Similarly, we looked at the AOF persistence scheme and its applications. Redis RDB vs. AOF:&nbsp;We compared both persistence schemes in terms of performance, data safety and use-cases. Implementing Redis Persistence:&nbsp;We put our knowledge into practice by chaining different types of persistence in different scenarios. I hope these lessons cleared your understanding of Redis, RDB and AOF. Now, it’s time to see how much you’ve learned. I will give a few questions to answer. Don’t worry if you can’t answer them all. The real learning often happens during the process of finding the answers, not demonstrating what you already know! Describe the fundamental differences between AOF and RDB persistence. In what situations would you prefer using AOF and when would you opt for RDB? Give specific examples. What are some potential disadvantages of using these persistence methods? Please take your time and type your answers. Describe the fundamental differences between AOF and RDB persistence. RDB (Redis Database) and AOF (Append Only File) are the two types of persistence provided by Redis. The fundamental difference lies in how and when they save data. RDB: This method captures a point-in-time snapshot of your dataset at specified intervals. It’s advantageous for backups and for quicker recovery after a restart. AOF: This approach logs every write operation received by the server, offering a more durable persistence method. AOF logs make Redis much more resistant to issues like power failures, as it maintains a complete log of all operations. In what situations would you prefer using AOF and when would you opt for RDB? Give specific examples. Your choice between AOF and RDB depends on your specific project and what trade-offs you’re willing to make. RDB: If you’re building a caching layer where data can be re-cached or recalculated from another store, the faster backups and recovery time of RDB is a clear advantage. AOF: If you’re building an application where every write is critical — for example, a messaging or collaboration application — the added durability of AOF would be a more suitable choice. What are some potential disadvantages of using these persistence methods? Each persistence method comes with its own set of drawbacks: RDB: While capturing snapshots of the data, Redis forks the server process, which can be system intensive. Moreover, you could lose a significant amount of data if Redis was to crash between snapshots. Also, bigger databases can take a long time and a lot of I/O to create the RDB snapshot. AOF: The log files can get significantly large with time because it records every operation. Moreover, AOF logs are usually larger than the equivalent RDB snapshots, and AOF can be slower than RDB when Redis restarts. Remember, these answers serve as a guide. When dealing with real-world projects, your specific context and requirements may lead you to different conclusions. 中文文章: https://programmerscareer.com/zh-cn/redis-interview5/Author: Wesley Wei – Twitter Wesley Wei – MediumNote: If you choose to repost or use this article, please cite the original source.","link":"/redis-interview5/"},{"title":"Redis interviews: how many data structures does Redis have? How is ZSet implemented?","text":"Have you ever been asked a similar question in an interview? Or you will meet in the future, let’s explore and master it together Thank you for reading this article. More Interview Questions here:https://programmerscareer.com/software-interview-set/ Topic: 1.1 Detailed Study on Redis Data StructuresRedis, an open-source, in-memory data structure store, provides us with a set of powerful and high-performance data structures. Our journey today starts with an in-depth exploration of these diverse data structures and the understanding of their functions. Data structures in Redis are pre-built patterns that manage and organize data, which allow fast read and write operations. Redis provides several types of data structures, each one ideal for different types of data management needs. These include: Strings: The simplest type of Redis data structure. They are binary safe and can contain any kind of data. Lists: These are simply lists of strings, sorted by insertion order. Sets: An unordered collection of unique strings. Hashes: Hashes are perfect to represent objects. They are maps between string fields and string values. Sorted sets (Zsets): Every member of a Sorted Set is associated with a score, that is used to sort the set elements from the smallest to the greatest score. Bit arrays (Bitmaps): They offer operations to manipulate arrays of bits. HyperLogLogs: a probabilistic data structure to estimate the cardinality of a set. Geospatial data (Geosets): they enable you to store latitude, longitude, and associated names. Streams: they are a log data type that appends new items, like for logging or messaging. The choice of data structure depends on both the nature of the data and the kind of operations needed to manipulate that data effectively. Understanding data structures and choosing the right one can drastically improve the performance of an application, making Redis an invaluable tool in our tech-stack. Topic: 1.2 Number of Data Structures in RedisAs we have previously learned, Redis isn’t just your everyday key-value store, it’s more accurate to think of it as a data structures server. But you might be wondering, just how many data structures does Redis actually support? The answer is, Redis importantly supports&nbsp;eight&nbsp;different types of data structures: Strings Lists Sets Sorted sets (Zsets) Hashes Bitmaps HyperLogLogs Geospatial data (Geosets) Streams Each of these data structures has a distinct identity, serves unique purposes, and provides different capabilities, thus allowing Redis to handle a wide range of data management tasks with exceptional speed and consistency. Topic: 1.3 Understanding Zset in RedisIn Redis,&nbsp;Sorted Sets, or&nbsp;Zsets, provide a fabulous combination of both Sets and Hashes. They take distinct aspects of these two data types, making this hybrid structure incredibly versatile. A Sorted Set is, in essence, a Set, which guarantees that each element is unique. However, it also associates each element with a score, as in a hash. These scores are used to sort the set elements, from the smallest score to the greatest. This might sound simple, but it has important implications. Redis can serve the Zset’s elements in the order of their score, providing a valuable resource for data range queries. Just imagine a leader board in a game, where you must display top performers in ascending or descending order. A Zset would be the ideal data structure for such a use-case, as you can directly fetch data in the sorted manner. That concludes the overview of Zsets! As we progress further, we’ll give you a live demonstration on how to implement and manipulate Zsets in Redis! Topic: 1.4 Implementation of Zset in RedisZsets, as we learned before, are unique in their ability to associate each element with a score and inherently sort these elements based on that score. But we haven’t yet dived into how Zsets are implemented in Redis. So let’s unravel this mystery! The implementation of Zsets is indeed quite fascinating. Redis utilizes two data structures internally to store a Zset: HashTable&nbsp;where the element is the key and the score is the value. SkipList&nbsp;or&nbsp;Sorted Set&nbsp;where every node is an element in our Zset. When Zsets are small, with a maximum length of 128 items, and every element in the set is internal within a small integer range, Zsets are stored in a list representation called&nbsp;ziplist. An interesting fact to note here is that the decision to use a HashTable or SkipList/Sorted Set does&nbsp;not&nbsp;affect the functionality of the Zset; it’s only for performance trade-offs. Redis automatically switches between these internal data structures based on the contents of the Zset, optimizing for the fastest read, write, or combination of both, as necessary! Topic: 1.5 Hands-on with Zset in RedisAlright, in Redis, you can perform various operations on Zsets: zadd: This command allows you to add elements to a Zset. Each element added would be associated with a score. Here is how you can use it: 1zadd myZset 1 \"a\" zrange: This command retrieves a range of elements, sorted by their scores. Here is how you would use it: 1zrange myZset 0 -1 zrem: This command helps you delete a specific element from the Zset: 1zrem myZset \"a\" zrank: This command gives the rank of the element, indexed from 0. So, if you want to find out the rank of an element “a”, you can write: 1zrank myZset \"a\" Remember, Redis is quite forgiving when it comes to syntax. It’s not case sensitive and doesn’t even mind if you forget to close your quotes in some cases! Topic: 1.6 Advanced Redis TopicsAfter mastering the various data structures and commands in Redis, it’s now time to level up and explore some advanced aspects of Redis! 💪 Let’s start with&nbsp;Redis Persistence. Redis offers two types of persistence: RDB&nbsp;(Redis Database Backup): This persistent method captures snapshots of your dataset at specified intervals. AOF&nbsp;(Append Only File): This persistent method logs every write operation received by the server, which when re-run, can reproduce the original dataset. Both persistence methods have their pros and cons, and the selection usually depends on the use-case requirements. Next in line is&nbsp;Redis Transactions. Redis Transactions allow the execution of a group of commands in a single step. It uses ‘MULTI’ to indicate the start and ‘EXEC’ to indicate the end of a transaction. Another significant aspect to discuss is Redis Security. By default, Redis has no authentication or security layer. However, Redis allows setting a password that clients must use to authenticate before being granted access. It is also important to remember that Redis doesn’t support encrypted connections, and it’s advisable to use an SSL proxy in cases where data needs to be encrypted over the network. Lastly, let’s shed light on&nbsp;Redis Pub/Sub&nbsp;model. Here, publishers send messages to specific channels without knowing or caring about the subscribers. Similarly, subscribers listen to specific channels without knowing or caring about the publishers. This leads to a highly decoupled and scalable system. Topic: 1.7 Redis in Real-World ApplicationsRedis, with its enviable set of features, finds use in a variety of real-world applications. Let’s look at a select few: Caching: Due to its high-speed and the availability of rich data types, Redis is an ideal choice to implement caching in web applications. It considerably speeds up application response times. Session Storage: Websites that need to maintain information across multiple requests from a user often use Redis for session storage. The data types that Redis provides make it an ideal candidate. Message Queue Systems: Redis can function as a Message Queue System using its Lists and Pub/Sub model. A list in Redis can be used as a queue in which you can use atomic operations like LPUSH and RPOP to push and pop elements. Leaderboards and Counting: Redis works exceptionally well to manage leaderboards, especially if you are required to manage them in real-time. The Sorted Set data structure is designed to solve such problems. Real-time Analysis: You can use Redis for Real-time analysis like computing or analyzing statistics in live time for immediate viewing. A key point to note is that the flexibility of Redis doesn’t limit it to just these applications. It can also serve as a primary database, a job management system, a search engine and much more! Topic: 1.8 Review and AssessmentsLet’s take a moment to review and practice what we’ve learned in the previous sections. It’s always a good idea to revisit the topics and start implementing them to solidify our understanding. Up until now, we’ve learned about the various data types in Redis, performed hands-on operations for each data structure, dove into advanced topics, and saw how Redis is used in real-world applications. A good methodology is for us to choose a real-world problem and try to solve it using Redis. You can try to implement a caching solution for a web application or set up a simple message queue system. Use the Redis commands we’ve learned to interact with the different data types and structures. Now let’s test your knowledge of Redis. 1️⃣&nbsp;Basic Question&nbsp;(Difficulty: 3/10): What are the six data types that Redis supports? 2️⃣&nbsp;Intermediate Question&nbsp;(Difficulty: 6/10): In what situation would you use a Redis List over a Redis Set? 3️⃣&nbsp;Advanced Question&nbsp;(Difficulty: 9/10): How would you implement caching in a web application using Redis? Please describe a brief process of how it would work. 1️⃣&nbsp;Basic Question Answer: Redis supports six data types, which are STRING, LIST, SET, ZSET (Sorted Set), HASH, and STREAM. 2️⃣&nbsp;Intermediate Question Answer: You would use a Redis List when the order of the data matters as Redis Lists maintain the order of elements based on how they are added. On the other hand, a Redis Set is an unordered collection. So, if you need to store a list of items in a specific order (like a timeline of comments on a blog post), you would use a Redis List. 3️⃣&nbsp;Advanced Question Answer: To implement caching in a web application using Redis, you could follow these steps: First, whenever a request is made to your web application, check if the requested data is in your Redis cache by trying to retrieve it using the request parameters as the key. If the data is in Redis (a cache hit), retrieve it and return it in the response. If the data is not in Redis (a cache hit), retrieve it from your primary database. After retrieving it from your primary database, save it to your Redis cache with an expiration time so that it doesn’t indefinitely take up memory in your cache. Then return the data in the response. Done correctly, this allows frequently requested data to be served from the Redis cache, significantly speeding up response times and reducing the load on your primary database. 中文文章: https://programmerscareer.com/zh-cn/redis-interview6/Author: Wesley Wei – Twitter Wesley Wei – MediumNote: If you choose to repost or use this article, please cite the original source.","link":"/redis-interview6/"},{"title":"Common(20+) Software Interview Questions(+ Answers) about MySQL/Redis/Kafka","text":"a collection of software interview questions&nbsp;about MySQL/Redis/Kafka, and this article updated on an ongoing basis If it really helps you, Buy me a coffee for my HARD work, that will motivate me to create more. :DBuy Me a Coffee MySQL Interviews: Why does MySQL use B+ trees for&nbsp;indexing？ https://programmerscareer.com/mysql-interview4/ MySQL Interviews: MySQL interviews: What are the transaction isolation levels? What are the tradeoffs? https://programmerscareer.com/mysql-interview15/ MySQL Interviews: What are database transactions and why does MySQL use InnoDB as the default option https://programmerscareer.com/mysql-interview14/ MySQL Interviews: Briefly describe the difference between optimistic locks and pessimistic locks and the usage scenarios https://programmerscareer.com/mysql-interview5/ MySQL Interviews: What are the necessary conditions for a deadlock to occur? How do I resolve deadlocks? https://programmerscareer.com/mysql-interview6/ Redis Interviews: How many data structures does Redis have? How is Zset implemented? https://programmerscareer.com/redis-interview6/ MySQL Interviews: What is the difference between a clustered index and a non-clustered index? https://programmerscareer.com/mysql-interview13/ MySQL Interviews: Briefly describe the occurrence scenarios of dirty reading and phantom reading. How does InnoDB solve phantom reading? https://programmerscareer.com/mysql-interview12/ MySQL Interviews: What is the difference between a unique index and a normal index? What are the advantages and disadvantages of using indexes? https://programmerscareer.com/mysql-interview11/ Redis Interviews: Briefly describe the advantages and disadvantages of RDB and AOF schemes in Redis persistence https://programmerscareer.com/redis-interview5/ MySQL Interviews: Briefly describe gap locks in MySQL https://programmerscareer.com/mysql-interview10/ Redis Interviews: How to implement Distributed Locks with Redis https://programmerscareer.com/redis-interview2/ Redis Interviews: How to prevent cache avalanche and cache penetration with Redis https://programmerscareer.com/redis-interview4/ MySQL Interviews: How to tune MySQL performance https://programmerscareer.com/mysql-interview9/ MySQL Interviews: Briefly describe the primary/secondary synchronization mechanism of MySQL. What happens if the synchronization fails? https://programmerscareer.com/mysql-interview1/ MySQL Interviews: When doesn’t MySQL use the index? https://programmerscareer.com/mysql-interview8/ MySQL Interviews: What is an SQL injection attack? How can such attacks be prevented? https://programmerscareer.com/mysql-interview7/ MySQL Interviews: What are ACID in a database? https://programmerscareer.com/mysql-interview2/ Redis Interviews: The application and advantages and disadvantages of jump table in Redis https://programmerscareer.com/redis-interview1/ Kafka Interviews: How does Kafka send messages reliably? https://programmerscareer.com/kafka-interview1 / MySQL Interviews: How does MySQL design indexes and optimize queries? https://programmerscareer.com/mysql-interview3/ Redis Interviews: If the Redis master node is down, how do you recover the data? https://programmerscareer.com/redis-interview3/ MySQL Interviews: Suppose to create a composite index (a, b, c) If you query fields A and c, will this composite index be used? https://programmerscareer.com/redis-interview19/ MySQL Interviews: What are the common storage engines for MySQL? What’s the difference? https://programmerscareer.com/redis-interview18/ MySQL Interviews: The implementation principle of MySQL MVCC https://programmerscareer.com/redis-interview17/ MySQL Interviews: When to split databases and when to split tables? https://programmerscareer.com/redis-interview16/ If it really helps you, Buy me a coffee for my HARD work, that will motivate me to create more. :DBuy Me a Coffee 中文文章: https://programmerscareer.com/zh-cn/software-interview-set/Author: Wesley Wei – Twitter Wesley Wei – MediumNote: If you choose to repost or use this article, please cite the original source.","link":"/software-interview-set/"}],"tags":[{"name":"interview","slug":"interview","link":"/tags/interview/"},{"name":"code","slug":"code","link":"/tags/code/"},{"name":"nothing","slug":"nothing","link":"/tags/nothing/"}],"categories":[{"name":"kafka","slug":"kafka","link":"/categories/kafka/"},{"name":"mysql","slug":"mysql","link":"/categories/mysql/"},{"name":"redis","slug":"redis","link":"/categories/redis/"},{"name":"golang","slug":"golang","link":"/categories/golang/"},{"name":"test","slug":"test","link":"/categories/test/"},{"name":"set","slug":"set","link":"/categories/set/"}]}